{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOflCXiNdbfj"
      },
      "source": [
        "# Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_jmWZwoBGJ_",
        "outputId": "817dfa71-76f9-4022-ca9c-f7e8c7710ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting antlr4-python3-runtime\n",
            "  Downloading antlr4-python3-runtime-4.10.tar.gz (116 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▉                             | 10 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 30 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 40 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 81 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 116 kB 4.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.10-py3-none-any.whl size=144171 sha256=8dc794825cb5984544c9bc25a9855d6080ee82c102bcc15b22465ea901d3d6c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/3d/4d/b5f9dab12f8e1e752959553da73f44289afa5c2dd47fa377d7\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime\n",
            "Successfully installed antlr4-python3-runtime-4.10\n"
          ]
        }
      ],
      "source": [
        "pip install antlr4-python3-runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSl9dxYAXSXJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKW4FqB8ys5p",
        "outputId": "f8d61502-de73-4d51-c775-2c3222ff04f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/ArshadPeoject')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4UJaLqq0Kw3"
      },
      "outputs": [],
      "source": [
        "from antlr4 import *\n",
        "from antlr4.tree.Tree import TerminalNodeImpl\n",
        "from my_parser.antlr.query.QueryLexer import QueryLexer\n",
        "from my_parser.antlr.query.QueryParser import QueryParser\n",
        "from my_parser.antlr.query.QueryVisitor import QueryVisitor\n",
        "\n",
        "from my_parser.antlr.path.PathLexer import PathLexer\n",
        "from my_parser.antlr.path.PathParser import PathParser\n",
        "from my_parser.antlr.path.PathVisitor import PathVisitor\n",
        "\n",
        "import re\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95oiJBDtWFt1",
        "outputId": "c8eb9622-eff2-48ef-aa20-d4de33b1e650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqx33-HMaQEZ"
      },
      "outputs": [],
      "source": [
        "# class MyParser(HttpGrammarParser):\n",
        "#   def accept(self, visitor:ParseTreeVisitor):\n",
        "#     print(\"a - \" , visitor.getRuleIndex())\n",
        "#     return super().accept(visitor)\n",
        "\n",
        "# class MyVisitor(HttpGrammarVisitor):\n",
        "#   def decompose(self, root):\n",
        "#     pass\n",
        "\n",
        "#   def visit(self, tree):\n",
        "#     print(\"v -\" , tree.getRuleIndex())\n",
        "#     return super().visit(tree)\n",
        "\n",
        " \n",
        "          \n",
        "#   def visitChildren(self, ctx):\n",
        "#     # print(\"ss\")\n",
        "#     # ctx.getLeaf()\n",
        "#     print(\"child -\" , ctx.getRuleIndex())\n",
        "#     print(\"c -\" ,type(ctx))\n",
        "\n",
        "#     ss = super().visitChildren(ctx)\n",
        "#     # print(ss)\n",
        "#     return ss\n",
        "  \n",
        "#     # def cus_visit(node,  root , S):\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRxPsnq-diMa"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, name, parent=None):\n",
        "        self.name = name\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        # ...\n",
        "\n",
        "        if parent:\n",
        "            self.parent.children.append(self)\n",
        "\n",
        "def nodize(tree, p=None):\n",
        "  \n",
        "  if not isinstance(tree, TerminalNodeImpl):\n",
        "    root = Node(\"<\" + str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\") + \">\", p)\n",
        "    for i in tree.getChildren():\n",
        "      nodize(i,root)\n",
        "  else:\n",
        "    root = Node(\"'\" +str(tree)+ \"'\", p)\n",
        "  return root\n",
        "\n",
        "\n",
        "def print_tree(current_node, indent=\"\", last='updown'):\n",
        "\n",
        "    nb_children = lambda node: sum(nb_children(child) for child in node.children) + 1\n",
        "    size_branch = {child: nb_children(child) for child in current_node.children}\n",
        "\n",
        "    \"\"\" Creation of balanced lists for \"up\" branch and \"down\" branch. \"\"\"\n",
        "    up = current_node.children\n",
        "    down = []\n",
        "    while up and sum(size_branch[node] for node in down) < sum(size_branch[node] for node in up):\n",
        "        down.append(up.pop())\n",
        "\n",
        "    \"\"\" Printing of \"up\" branch. \"\"\"\n",
        "    for child in up:     \n",
        "        next_last = 'up' if up.index(child) is 0 else ''\n",
        "        next_indent = '{0}{1}{2}'.format(indent, ' ' if 'up' in last else '│', \" \" * len(current_node.name))\n",
        "        print_tree(child, indent=next_indent, last=next_last)\n",
        "\n",
        "    \"\"\" Printing of current node. \"\"\"\n",
        "    if last == 'up': start_shape = '┌'\n",
        "    elif last == 'down': start_shape = '└'\n",
        "    elif last == 'updown': start_shape = ' '\n",
        "    else: start_shape = '├'\n",
        "\n",
        "    if up: end_shape = '┤'\n",
        "    elif down: end_shape = '┐'\n",
        "    else: end_shape = ''\n",
        "\n",
        "    print ('{0}{1}{2}{3}'.format(indent, start_shape, current_node.name, end_shape))\n",
        "\n",
        "    \"\"\" Printing of \"down\" branch. \"\"\"\n",
        "    for child in down:\n",
        "        next_last = 'down' if down.index(child) is len(down) - 1 else ''\n",
        "        next_indent = '{0}{1}{2}'.format(indent, ' ' if 'down' in last else '│', \" \" * len(current_node.name))\n",
        "        print_tree(child, indent=next_indent, last=next_last)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHUrGtI_g3FH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4E7l7Xag4iL"
      },
      "outputs": [],
      "source": [
        "def getLeaf(root,prev):\n",
        "  leaf=set()\n",
        "  if not isinstance(root, TerminalNodeImpl):\n",
        "    for i in root.getChildren():\n",
        "      leaf.update(getLeaf(i,root))\n",
        "  else:\n",
        "    leaf.add(str(prev).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\"))\n",
        "  return leaf\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGoO5T6reV3"
      },
      "source": [
        "vector is : [path_vector].concat([query_Vector])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdyfhvLcrGsf"
      },
      "outputs": [],
      "source": [
        "def decomposeTree(root):\n",
        "  S=set()\n",
        "  children=root.getChildren()\n",
        "  for child in children:\n",
        "    visit(child,root,S)\n",
        "  return S\n",
        "\n",
        "def visit(node,root,S):\n",
        "  rootLeafs= getLeaf(root)\n",
        "  childLeafs = getLeaf(node)\n",
        "  diff=rootLeafs.difference(childLeafs)\n",
        "  extr=False\n",
        "  if not isinstance(node, TerminalNodeImpl):\n",
        "    children=node.getChildren()\n",
        "    for child in children:\n",
        "      extr=visit(child,node,S)\n",
        "  if len(diff)>0 and not extr:\n",
        "    S.add(\"<\" + str(node).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\") + \">\")\n",
        "    extr=True\n",
        "  return extr\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4azhS4CKgvO"
      },
      "outputs": [],
      "source": [
        "def getStates0(tree, root, S):\n",
        "   rootLeafs= getLeaf(root,None)\n",
        "  #  print(rootLeafs)\n",
        "   childLeafs = getLeaf(tree,None)\n",
        "  #  print(childLeafs)\n",
        "   diff=rootLeafs.difference(childLeafs)\n",
        "  #  print(diff)\n",
        "  #  print(len(diff))\n",
        "   extr=False\n",
        "   if not isinstance(tree, TerminalNodeImpl): \n",
        "     state = str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\")      #  print(state)\n",
        "     if len(diff)>0 and not extr:\n",
        "      # print(state , \" | \", tree.getText())\n",
        "      txt =tree.getText();\n",
        "      nsc=len(re.sub('[\\w]+' ,'', txt))\n",
        "      lav=len(txt)\n",
        "      rml=4096\n",
        "      t=S[1][int(state)]*S[0][int(state)]\n",
        "      S[0][int(state)]+=1\n",
        "      S[1][int(state)]=( t + int((1+(nsc/lav))*rml) + lav)/S[0][int(state)]\n",
        "      \n",
        "      extr=True\n",
        "     ch=tree.getChildren()\n",
        "     prev=tree\n",
        "     for c in ch:\n",
        "       getStates0(c,prev, S)\n",
        "       prev=c  \n",
        "\n",
        "# def getStates(dfa_states, s):\n",
        "#   for k in dfa_states:\n",
        "#     if k!=None and not k.stateNumber in s:\n",
        "#       s.add(k.stateNumber)\n",
        "#       if k.edges!=None:\n",
        "#           getStates(k.edges,s)\n",
        "  \n",
        "def getStates(tree, S):\n",
        "  #  S.add(\"<\" + str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\") + \">\")\n",
        "   if not isinstance(tree, TerminalNodeImpl): \n",
        "     state = str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\")\n",
        "    #  if state!='':\n",
        "    #   #  print(state)\n",
        "    #    S[int(state)]+=1\n",
        "     ch=tree.getChildren()\n",
        "     prev=tree\n",
        "     for c in ch:\n",
        "       getStates0(c,prev, S)\n",
        "       prev=c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEcsif87WzpR",
        "outputId": "7ea6f8c5-8749-42ca-bcd4-8361b47e7c89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "x = \"asdfkls2df#$&rwefe^ef#wef@!\"\n",
        "new = len(re.sub('[\\w]+' ,'', x))\n",
        "print(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3aXE6kkKYss"
      },
      "outputs": [],
      "source": [
        "# def getStates0(tree, root, S):\n",
        "#   #  rootLeafs= getLeaf(root)\n",
        "#   #  childLeafs = getLeaf(tree)\n",
        "#   #  diff=rootLeafs.difference(childLeafs)\n",
        "#    extr=False\n",
        "#    if not isinstance(tree, TerminalNodeImpl): \n",
        "#      state = str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\")\n",
        "#       #  print(state)\n",
        "#      ch=tree.getChildren()\n",
        "#      prev=tree\n",
        "#      for c in ch:\n",
        "#        extr=getStates0(c,prev, S)\n",
        "#        prev=c  \n",
        "#     #  if len(diff)>0 and not extr:\n",
        "#     #  print(state)\n",
        "#      print(state , \" | \", tree.getText())\n",
        "#      S[int(state)]+=1\n",
        "#       #  extr=True\n",
        "#    return extr\n",
        "\n",
        "\n",
        "# # def getStates(dfa_states, s):\n",
        "# #   for k in dfa_states:\n",
        "# #     if k!=None and not k.stateNumber in s:\n",
        "# #       s.add(k.stateNumber)\n",
        "# #       if k.edges!=None:\n",
        "# #           getStates(k.edges,s)\n",
        "  \n",
        "# def getStates(tree, S):\n",
        "#   #  S.add(\"<\" + str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\") + \">\")\n",
        "#    if not isinstance(tree, TerminalNodeImpl): \n",
        "\n",
        "#      state = str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\")\n",
        "#      print(tree.getText())\n",
        "#      if state!='':\n",
        "#       #  print(state)\n",
        "#        print(state , \" | \", tree.getText())\n",
        "#        S[int(state)]+=1\n",
        "#      ch=tree.getChildren()\n",
        "#      for c in ch:\n",
        "#        getStates0(c,tree, S)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utt5GAD7-ZRP"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "tZu_ouwkBR54",
        "outputId": "9af2e4b8-cae9-441c-a4e1-fcf71c1f6990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tienda1/imagenes/1.gif\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-173964657260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath_end\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mq_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'InputStream' is not defined"
          ]
        }
      ],
      "source": [
        "# \n",
        "url = \"http://localhost:8080/tienda1/imagenes/1.gif\".lower()\n",
        "\n",
        "path_start=url.index(\"/\", url.index(\"/\")+2)\n",
        "try:\n",
        "  path_end=url.index(\"?\")\n",
        "except:\n",
        "  path_end=len(url)\n",
        "\n",
        "basic = url[:path_start]\n",
        "path = url[path_start+1:path_end]\n",
        "query = url[path_end+1:]\n",
        "print(path)\n",
        "q_data = InputStream(query)\n",
        "p_data = InputStream(path)\n",
        "\n",
        "q_lexer = QueryLexer(q_data)\n",
        "q_stream = CommonTokenStream(q_lexer)\n",
        "q_parser =QueryParser(q_stream)\n",
        "q_tree = q_parser.query()\n",
        "print_tree(nodize(q_tree))\n",
        "p_lexer = PathLexer(p_data)\n",
        "p_stream = CommonTokenStream(p_lexer)\n",
        "p_parser =PathParser(p_stream)\n",
        "p_tree = p_parser.path()\n",
        "print_tree(nodize(p_tree))\n",
        "\n",
        "\n",
        "query_vector =[0]*65\n",
        "print(query_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuhB8UBzdebn"
      },
      "outputs": [],
      "source": [
        "def parse(url):\n",
        "  \n",
        "  url=url.lower()\n",
        "  path_start=url.index(\"/\", url.index(\"//\")+1)\n",
        "  try:\n",
        "    path_end=url.index(\"?\")\n",
        "  except:\n",
        "    path_end=len(url)\n",
        "\n",
        "  basic = url[:path_start]\n",
        "  path = url[path_start+1:path_end]\n",
        "  query = url[path_end+1:]\n",
        "\n",
        "  q_vec=np.zeros((2,285))\n",
        "  p_vec=np.zeros((2,235))\n",
        "  if query == '': \n",
        "    query=None\n",
        "  if path=='':\n",
        "    path=None\n",
        "  if query!=None:\n",
        "    q_data = InputStream(query)\n",
        "    q_lexer = QueryLexer(q_data)\n",
        "    q_stream = CommonTokenStream(q_lexer)\n",
        "    q_parser =QueryParser(q_stream)\n",
        "    q_tree = q_parser.query()\n",
        "    getStates(q_tree, q_vec)\n",
        "    visitor = QueryVisitor()\n",
        "    output =visitor.visit(q_tree)\n",
        "  # print(\"===========================================\")\n",
        "  if path!=None:\n",
        "    p_data = InputStream(path)\n",
        "    p_lexer = PathLexer(p_data)\n",
        "    p_stream = CommonTokenStream(p_lexer)\n",
        "    p_parser =PathParser(p_stream)\n",
        "    p_tree = p_parser.path()\n",
        "    getStates(p_tree, p_vec)\n",
        "    visitor = PathVisitor()\n",
        "    output =visitor.visit(p_tree)\n",
        "  # print(len(p_vec[1]))\n",
        "  # print(\"==================\")\n",
        "  # print(p_vec[0])\n",
        "  # print(\"  --  \")\n",
        "  # print(p_vec[1])\n",
        "  # print(\"==================\")\n",
        "  # print(q_vec[1])\n",
        "  # return np.concatenate((p_vec[1], q_vec[1]))\n",
        "  return q_vec,q_tree, p_vec,p_tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wny4a87vRozs"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxYEhSg3j-kw"
      },
      "outputs": [],
      "source": [
        "\n",
        "url= \"http://localhost:8080/tienda1/miembros/editar.jsp?te=select+*+from+tb_ss+where+1=1&modo=registro&login=olson&password=conFesuR%CDA&nombre=Aberardo&apellidos=Segovia&email=raya%40acumulador.com.iq&dni=06076324T&direccion=Calle+Atarfe%F1o%2C+1%2B15%2C+&ciudad=Marug%E1n&cp=15589&provincia=Lleida&ntc=0903154438828489&B1=Registrar\".lower()\n",
        "# path_start=url.index(\"/\", url.index(\"/\")+2)\n",
        "# try:\n",
        "#   path_end=url.index(\"?\")\n",
        "# except:\n",
        "#   path_end=len(url)\n",
        "\n",
        "# basic = url[:path_start]\n",
        "# path = url[path_start+1:path_end]\n",
        "# query = url[path_end+1:]\n",
        "# print(path)\n",
        "# q_data = InputStream(query)\n",
        "# p_data = InputStream(path)\n",
        "\n",
        "# q_lexer = QueryLexer(q_data)\n",
        "# q_stream = CommonTokenStream(q_lexer)\n",
        "# q_parser =QueryParser(q_stream)\n",
        "# q_tree = q_parser.query()\n",
        "# print_tree(nodize(q_tree))\n",
        "# p_lexer = PathLexer(p_data)\n",
        "# p_stream = CommonTokenStream(p_lexer)\n",
        "# p_parser =PathParser(p_stream)\n",
        "# p_tree = p_parser.path()\n",
        "# print_tree(nodize(p_tree))\n",
        "\n",
        "\n",
        "# query_vector =[0]*65\n",
        "# print(query_vector)\n",
        "q, q_t, p, p_t = parse(url)\n",
        "print(q[0])\n",
        "# print(q[1])\n",
        "\n",
        "print(\"=====\")\n",
        "print_tree(nodize(q_t))\n",
        "# print(p[0])\n",
        "# print(p[1])\n",
        "\n",
        "\n",
        "\n",
        "# # print(url[406:])\n",
        "# # url= \"get http:/dpxd.er.ui:8080/p2/ppath2/path_47-8rt?q=w&q=re&e=sd-4de-6\"\n",
        "# # url = \"get https:/colab.research.google.com/drive/sel/se-d_m/sed34-ws/sdw_er4-w/a33sdr-t15iiki-_v/ty5-der_sdw?p=qwe&qw23w=25,w=sdw-s dd96,r=ade,edef4,rfrf4_de-&buery=select * from tb_test where 1=1\".lower()\n",
        "# v =parse(\"https://stackoverflow.com/questions/42905267/antlr-error-no-viable-alternative-at-input-eof\")\n",
        "\n",
        "# print(v)\n",
        "# query_vector =[0]*65\n",
        "# path_vector= [0]*65"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxeVSd-vDB_0"
      },
      "outputs": [],
      "source": [
        "# for i in range(0,len(v)):\n",
        "#   if v[i] >0:\n",
        "#     print(i,\"=\",v[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTDsmAdHAklo"
      },
      "outputs": [],
      "source": [
        "# for i in range(0,len(v)):\n",
        "#   if v[i] >0:\n",
        "#     print(i,\"=\",v[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW26lbxUDdz5"
      },
      "outputs": [],
      "source": [
        "# for i in range(0,len(v)):\n",
        "#   if v[i] >0:\n",
        "#     print(i,\"=\",v[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejjR_h0QgP_2"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def analyse(p):\n",
        "  \n",
        "  tmp= p.split('\\n')\n",
        "  if(p.startswith(\"GET\")):\n",
        "    tm = tmp[0].split(' ')\n",
        "    return tm[0]+' '+tm[1]\n",
        "  tm = tmp[0].split(' ')\n",
        "  # print(tmp)\n",
        "  return tm[0]+' '+tm[1]+ '?' +tmp[len(tmp)-3]\n",
        "  \n",
        "def readLangs(lang1, lang2, path, reverse=False):\n",
        "    c=[\"col\"]*520\n",
        "    print(c)\n",
        "    for i in range(0,520):\n",
        "      c[i]=\"col-\"+str(i)\n",
        "    f = open('/content/gdrive/MyDrive/ArshadPeoject/data/anomalousTrafficTest.csv', \"w\")\n",
        "    cw = csv.writer(f)\n",
        "    cw.writerow(c)\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    filedata = open(path, encoding='utf-8').\\\n",
        "        read().strip()\n",
        "    filedata = re.sub(r\"GET\", r\"@GET\", filedata)\n",
        "    filedata = re.sub(r\"POST\", r\"@POST\", filedata)\n",
        "    lines= filedata.split(\"@\")\n",
        "    lines= lines[1:]\n",
        "    # print(lines[20982])\n",
        "    # Split every line into pairs and normalize\n",
        "    # pairs = [[normalizeString(s) for s in [l,l]] for l in lines]\n",
        "    for l in lines:\n",
        "      cw.writerow(parse(analyse(l)))\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pLDfCUlEY-a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def init_test_validation_data():\n",
        "  anomal_file = open('/content/gdrive/MyDrive/ArshadPeoject/data/anomalousTrafficTest.csv', \"r\")\n",
        "  normal_file = open('/content/gdrive/MyDrive/ArshadPeoject/data/normalTrafficTest.csv', \"r\")\n",
        "\n",
        "  normal_valid = open('/content/gdrive/MyDrive/ArshadPeoject/data/normal_valid.csv', \"w\")\n",
        "  normall_test = open('/content/gdrive/MyDrive/ArshadPeoject/data/normal_test.csv', \"w\")\n",
        "\n",
        "  valid_write =csv.writer(normal_valid)\n",
        "  test_write  =csv.writer(normall_test)\n",
        "  c=[\"col\"]*520\n",
        "  for i in range(0,520):\n",
        "    c[i]=\"col-\"+str(i)\n",
        "\n",
        "\n",
        "  reader = csv.reader(normal_file)\n",
        "  a=list()\n",
        "  isFirst=False\n",
        "  valid_write.writerow(c)\n",
        "  test_write.writerow(c)\n",
        "  for row in reader:\n",
        "    if not isFirst:\n",
        "      isFirst=True\n",
        "    else:\n",
        "      a.append(row)\n",
        "  random.shuffle(a)\n",
        "  count =int(0.2*len(a))\n",
        "  for i in range(0,len(a)):\n",
        "    if i <count:\n",
        "      valid_write.writerow(a[i])\n",
        "    else:\n",
        "      test_write.writerow(a[i])\n",
        "\n",
        "  normal_valid.close()\n",
        "  normall_test.close()\n",
        "  #===============================================\n",
        "  anomal_valid = open('/content/gdrive/MyDrive/ArshadPeoject/data/anomalous_valid.csv', \"w\")\n",
        "  anomal_test = open('/content/gdrive/MyDrive/ArshadPeoject/data/anomalous_test.csv', \"w\")\n",
        "  valid_write =csv.writer(anomal_valid)\n",
        "  test_write  =csv.writer(anomal_test)\n",
        "\n",
        "  reader = csv.reader(anomal_file)\n",
        "  a=list()\n",
        "  isFirst=False\n",
        "  valid_write.writerow(c)\n",
        "  test_write.writerow(c)\n",
        "  for row in reader:\n",
        "    if not isFirst:\n",
        "      isFirst=True\n",
        "    else:\n",
        "      a.append(row)\n",
        "  random.shuffle(a)\n",
        "  count =int(0.2*len(a))\n",
        "  for i in range(0,len(a)):\n",
        "    if i <count:\n",
        "      valid_write.writerow(a[i])\n",
        "    else:\n",
        "      test_write.writerow(a[i])\n",
        "\n",
        "  anomal_valid.close()\n",
        "  anomal_test.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4zFmu0zzEpP"
      },
      "outputs": [],
      "source": [
        "# anomal_file = open('/content/gdrive/MyDrive/ArshadPeoject/data/normalTrafficTest.csv', \"r\")\n",
        "# rrr = csv.reader(anomal_file)\n",
        "# r=0\n",
        "# for row in rrr:\n",
        "#   print(row)\n",
        "#   print(len(row))\n",
        "#   r+=1\n",
        "#   if r==5: \n",
        "#     pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vgQ1IFJgOpe"
      },
      "outputs": [],
      "source": [
        "# anorm_train.close()\n",
        "# anorm_test.close()\n",
        "\n",
        "# readLangs('eng', 'fra', '/content/gdrive/MyDrive/ArshadPeoject/data/raw_data/anomalousTrafficTest.txt', False)\n",
        "# print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C21N-z-bzIjy"
      },
      "outputs": [],
      "source": [
        "# init_test_validation_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6WWkw8VeW3U"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import click\n",
        "import torch\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from abc import ABC, abstractmethod\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywpx0nfPexBl"
      },
      "source": [
        "# Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3Ex4wEkfhfo"
      },
      "outputs": [],
      "source": [
        "class BaseADDataset(ABC):\n",
        "    \"\"\"Anomaly detection dataset base class.\"\"\"\n",
        "\n",
        "    def __init__(self, root: str):\n",
        "        super().__init__()\n",
        "        self.root = root  # root path to data\n",
        "\n",
        "        self.n_classes = 2  # 0: normal, 1: outlier\n",
        "        self.normal_classes = None  # tuple with original class labels that define the normal class\n",
        "        self.outlier_classes = None  # tuple with original class labels that define the outlier class\n",
        "\n",
        "        self.train_set = None  # must be of type torch.utils.data.Dataset\n",
        "        self.test_set = None  # must be of type torch.utils.data.Dataset\n",
        "        self.validation_set = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def loaders(self, batch_size: int, shuffle_train=True, shuffle_test=False, num_workers: int = 0) -> (\n",
        "            DataLoader, DataLoader):\n",
        "        \"\"\"Implement data loaders of type torch.utils.data.DataLoader for train_set and test_set.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ebZEcowfi1x"
      },
      "outputs": [],
      "source": [
        "class BaseNet(nn.Module):\n",
        "    \"\"\"Base class for all neural networks.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.rep_dim = None  # representation dimensionality, i.e. dim of the last layer\n",
        "\n",
        "    def forward(self, *input):\n",
        "        \"\"\"\n",
        "        Forward pass logic\n",
        "        :return: Network output\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"Network summary.\"\"\"\n",
        "        net_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
        "        params = sum([np.prod(p.size()) for p in net_parameters])\n",
        "        print('Trainable parameters: {}'.format(params))\n",
        "        print(self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdD6k2WtfoSU"
      },
      "outputs": [],
      "source": [
        "class BaseTrainer(ABC):\n",
        "    \"\"\"Trainer base class.\"\"\"\n",
        "\n",
        "    def __init__(self, optimizer_name: str, lr: float, n_epochs: int, lr_milestones: tuple, batch_size: int,\n",
        "                 weight_decay: float, device: str, n_jobs_dataloader: int):\n",
        "        super().__init__()\n",
        "        self.optimizer_name = optimizer_name\n",
        "        self.lr = lr\n",
        "        self.n_epochs = n_epochs\n",
        "        self.lr_milestones = lr_milestones\n",
        "        self.batch_size = batch_size\n",
        "        self.weight_decay = weight_decay\n",
        "        self.device = device\n",
        "        self.n_jobs_dataloader = n_jobs_dataloader\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self, dataset: BaseADDataset, net: BaseNet) -> BaseNet:\n",
        "        \"\"\"\n",
        "        Implement train method that trains the given network using the train_set of dataset.\n",
        "        :return: Trained net\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def test(self, dataset: BaseADDataset, net: BaseNet):\n",
        "        \"\"\"\n",
        "        Implement test method that evaluates the test_set of dataset on the given network.\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBW5FxHDfvLk"
      },
      "outputs": [],
      "source": [
        "class TorchvisionDataset(BaseADDataset):\n",
        "    \"\"\"TorchvisionDataset class for datasets already implemented in torchvision.datasets.\"\"\"\n",
        "\n",
        "    def __init__(self, root: str):\n",
        "        super().__init__(root)\n",
        "\n",
        "    def loaders(self, batch_size: int, shuffle_train=True, shuffle_test=False, num_workers: int = 0) -> (\n",
        "            DataLoader, DataLoader):\n",
        "        train_loader = DataLoader(dataset=self.train_set, batch_size=batch_size, shuffle=shuffle_train,\n",
        "                                  num_workers=num_workers)\n",
        "        test_loader = DataLoader(dataset=self.test_set, batch_size=batch_size, shuffle=shuffle_test,\n",
        "                                 num_workers=num_workers)\n",
        "        # validation_loader =  DataLoader(dataset=self.validation_set, batch_size=batch_size, shuffle=shuffle_test,\n",
        "        #                          num_workers=num_workers)\n",
        "        return train_loader, test_loader\n",
        "        # , validation_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exfkHVG9eIJp"
      },
      "source": [
        "# Optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AV2pPmAeRtX"
      },
      "outputs": [],
      "source": [
        "class AETrainer(BaseTrainer):\n",
        "\n",
        "    def __init__(self, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 150, lr_milestones: tuple = (),\n",
        "                 batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda', n_jobs_dataloader: int = 0):\n",
        "        super().__init__(optimizer_name, lr, n_epochs, lr_milestones, batch_size, weight_decay, device,\n",
        "                         n_jobs_dataloader)\n",
        "        self.validation = {\n",
        "            'loss': list(),\n",
        "            'valid':list()\n",
        "        }\n",
        "\n",
        "\n",
        "    def store_tmp(self,c_epoch,opt,schedul,ae):\n",
        "      torch.save({'optim_nameR': self.optimizer_name ,\n",
        "                    'lr': self.lr,\n",
        "                    'n_epochs': self.n_epochs,\n",
        "                    'lr_milestones': self.lr_milestones,\n",
        "                    'batch_size': self.batch_size,\n",
        "                    'weight_decay': self.weight_decay,\n",
        "                    'device': self.device,\n",
        "                    'n_jobs_dataloader': self.n_jobs_dataloader,\n",
        "                    'current_epoch':c_epoch,\n",
        "                    'validations':self.validation,\n",
        "                    'optimaizer': opt.state_dict(),\n",
        "                    'sched': schedul.state_dict(), \n",
        "                    'ae': ae.state_dict()}, '/content/gdrive/MyDrive/ArshadPeoject/model/tmp/ae.dict')\n",
        "\n",
        "\n",
        "    def train(self, dataset: BaseADDataset, valid_dataset: BaseADDataset, ae_net: BaseNet):\n",
        "        logger = logging.getLogger()\n",
        "\n",
        "        # Set device for network\n",
        "        ae_net = ae_net.to(self.device)\n",
        "\n",
        "        # Get train data loader\n",
        "        # Set optimizer (Adam optimizer for now)\n",
        "        optimizer = optim.Adam(ae_net.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n",
        "                               amsgrad=self.optimizer_name == 'amsgrad')\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.lr_milestones, gamma=0.1)\n",
        "\n",
        "        # Training\n",
        "        print('Starting pretraining...')\n",
        "        start_time = time.time()\n",
        "        return self.train_worker(ae_net,optimizer,scheduler,0,dataset,valid_dataset)\n",
        "       \n",
        "    def train_worker(self, ae_net,optimizer,scheduler,epoch_start, dataset, valid_dataset):\n",
        "        train_loader, _ = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "        _,valid_loader =valid_dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "        \n",
        "        ae_net.train()\n",
        "\n",
        "        for epoch in range(epoch_start, self.n_epochs):\n",
        "\n",
        "            scheduler.step()\n",
        "            if epoch in self.lr_milestones:\n",
        "                print('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n",
        "\n",
        "            loss_epoch = 0.0\n",
        "            n_batches = 0\n",
        "            epoch_start_time = time.time()\n",
        "            for data in train_loader:\n",
        "                inputs, _, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                # Zero the network parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Update network parameters via backpropagation: forward + backward + optimize\n",
        "                outputs = ae_net(inputs)\n",
        "                # print(outputs)\n",
        "                scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n",
        "                loss = torch.mean(scores)\n",
        "\n",
        "                # print(loss)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                loss_epoch += loss.item()\n",
        "                n_batches += 1\n",
        "\n",
        "            # log epoch statistics\n",
        "            if epoch % 10 ==0:\n",
        "              loss_eval=0.0\n",
        "              n_eval_batches=0\n",
        "              self.validation['loss'].append(loss_epoch / n_batches)\n",
        "              ae_net.eval()\n",
        "              with torch.no_grad():\n",
        "                  for data in valid_loader:\n",
        "                      inputs, labels, idx = data\n",
        "                      e_inputs = inputs.to(self.device)\n",
        "                      e_outputs = ae_net(e_inputs)\n",
        "                      e_scores= torch.sum((e_outputs - e_inputs) ** 2, dim=tuple(range(1, e_outputs.dim())))\n",
        "                      eval_loss = torch.mean(e_scores)\n",
        "\n",
        "                      loss_eval += eval_loss.item()\n",
        "                      n_eval_batches += 1\n",
        "\n",
        "              self.validation['valid'].append(loss_eval / n_eval_batches)\n",
        "              ae_net.train()\n",
        "            \n",
        "            self.store_tmp(epoch+1,optimizer,scheduler,ae_net);\n",
        "            epoch_train_time = time.time() - epoch_start_time\n",
        "            print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f} {:.8f} {:.8f}'\n",
        "                        .format(epoch + 1, self.n_epochs, epoch_train_time, loss_epoch / n_batches, float(scheduler.get_lr()[0]), float(scheduler.get_last_lr()[0])))\n",
        "\n",
        "        # pretrain_time = time.time() - start_time\n",
        "        # print('Pretraining time: %.3f' % pretrain_time)\n",
        "        print('Finished pretraining.')\n",
        "\n",
        "        return ae_net, self.validation\n",
        "\n",
        "    \n",
        "    def test(self, dataset: BaseADDataset, ae_net: BaseNet):\n",
        "        logger = logging.getLogger()\n",
        "\n",
        "        # Set device for network\n",
        "        ae_net = ae_net.to(self.device)\n",
        "\n",
        "        # Get test data loader\n",
        "        _, test_loader = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "        # Testing\n",
        "        print('Testing autoencoder...')\n",
        "        loss_epoch = 0.0\n",
        "        n_batches = 0\n",
        "        start_time = time.time()\n",
        "        idx_label_score = []\n",
        "        ae_net.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                inputs, labels, idx = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                outputs = ae_net(inputs)\n",
        "                scores= torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n",
        "                loss = torch.mean(scores)\n",
        "\n",
        "                # Save triple of (idx, label, score) in a list\n",
        "    \n",
        "                idx_label_score += list(zip(idx.cpu().data.numpy().tolist(),\n",
        "                                            labels.cpu().data.numpy().tolist(),\n",
        "                                            scores.cpu().data.numpy().tolist()))\n",
        "\n",
        "                loss_epoch += loss.item()\n",
        "                n_batches += 1\n",
        "\n",
        "        print('Test set Loss: {:.8f}'.format(loss_epoch / n_batches))\n",
        "\n",
        "        _, labels, scores = zip(*idx_label_score)\n",
        "        labels = np.array(labels)\n",
        "        scores = np.array(scores)\n",
        "\n",
        "        # auc = roc_auc_score(labels, scores)\n",
        "        # print('Test set AUC: {:.2f}%'.format(100. * auc))\n",
        "\n",
        "        test_time = time.time() - start_time\n",
        "        print('Autoencoder testing time: %.3f' % test_time)\n",
        "        print('Finished testing autoencoder.')\n",
        "\n",
        "\n",
        "    def load_and_continue(self,dataset, valid_dataset, ae):\n",
        "      data =  torch.load('/content/gdrive/MyDrive/ArshadPeoject/model/tmp/ae.dict',map_location=torch.device(self.device))\n",
        "\n",
        "      self.optimizer_name = data['optim_nameR']\n",
        "      self.lr =  data['lr']\n",
        "      # self.n_epochs =  data['n_epochs']\n",
        "      # self.lr_milestones =  data['lr_milestones']\n",
        "      self.batch_size =  data['batch_size']\n",
        "      self.weight_decay =  data['weight_decay']\n",
        "      # self.device =  data['device']\n",
        "      self.n_jobs_dataloader =  data['n_jobs_dataloader']\n",
        "      self.validation = data['validations']\n",
        "      # print(device)\n",
        "      ae = ae.to(self.device)\n",
        "\n",
        "      # data['sched']['milestones']=[118,200]\n",
        "      # data['sched']['last_epoch']=0\n",
        "      print(data['sched'])\n",
        "      # print(data['optimaizer'])\n",
        "\n",
        "      curr_epoch =data['current_epoch']\n",
        "      print(curr_epoch)\n",
        "      ae.load_state_dict(data['ae'])\n",
        "\n",
        "\n",
        "      opt=optim.Adam(ae.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n",
        "                               amsgrad=self.optimizer_name == 'amsgrad')\n",
        "\n",
        "      sche= optim.lr_scheduler.MultiStepLR(opt, milestones=self.lr_milestones, gamma=0.1)\n",
        "    \n",
        "      opt.load_state_dict(data['optimaizer'])\n",
        "\n",
        "      sche.load_state_dict(data['sched'])\n",
        "      print(sche.milestones)\n",
        "\n",
        "      return self.train_worker(ae,opt,sche,curr_epoch, dataset, valid_dataset)\n",
        "     \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkRA1Ve0wUHg"
      },
      "outputs": [],
      "source": [
        "a = np.array([[1, 2],[5, 5],[6,6]])\n",
        "b = np.array([[1, 1 ], [2, 2]])\n",
        "\n",
        "\n",
        "# print(torch.tensor(c))\n",
        "\n",
        "# c= torch.sum(torch.tensor(c), dim=0).cpu()\n",
        "\n",
        "# print(c)\n",
        "\n",
        "# # c= \n",
        "\n",
        "# print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPArwLzofPT_"
      },
      "outputs": [],
      "source": [
        "class DeepSVDDTrainer(BaseTrainer):\n",
        "\n",
        "    def __init__(self, objective, R, c, nu: float, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 150,\n",
        "                 lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "                 n_jobs_dataloader: int = 0):\n",
        "        super().__init__(optimizer_name, lr, n_epochs, lr_milestones, batch_size, weight_decay, device,\n",
        "                         n_jobs_dataloader)\n",
        "        self.validation = {\n",
        "            'loss': list(),\n",
        "            'valid':list()\n",
        "        }\n",
        "        assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n",
        "        self.objective = objective\n",
        "\n",
        "        # Deep SVDD parameters\n",
        "        self.R = torch.tensor(R, device=self.device)  # radius R initialized with 0 by default.\n",
        "        self.c = torch.tensor(c, device=self.device) if c is not None else None\n",
        "        self.nu = nu\n",
        "\n",
        "        # Optimization parameters\n",
        "        self.warm_up_n_epochs = 10  # number of training epochs for soft-boundary Deep SVDD before radius R gets updated\n",
        "\n",
        "        # Results\n",
        "        self.train_time = None\n",
        "        self.test_auc = None\n",
        "        self.test_time = None\n",
        "        self.test_scores = None\n",
        "\n",
        "    def train(self, dataset: BaseADDataset, valid_dataset: BaseADDataset, net: BaseNet):\n",
        "        logger = logging.getLogger()\n",
        "\n",
        "        # Set device for network\n",
        "        net = net.to(self.device)\n",
        "\n",
        "        \n",
        "        # Set optimizer (Adam optimizer for now)\n",
        "        optimizer = optim.Adam(net.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n",
        "                               amsgrad=self.optimizer_name == 'amsgrad')\n",
        "        \n",
        "        train_loader, _ = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.lr_milestones, gamma=0.1)\n",
        "\n",
        "        # Initialize hypersphere center c (if c not loaded)\n",
        "        if self.c is None:\n",
        "            print('Initializing center c...')\n",
        "            self.c = self.init_center_c(train_loader, net)\n",
        "            print('Center c initialized.')\n",
        "\n",
        "        # Training\n",
        "        print('Starting training...')\n",
        "        start_time = time.time()\n",
        "        return self.train_worker(net,optimizer,scheduler,0,dataset,valid_dataset,train_loader)\n",
        "        \n",
        "    def train_worker(self,  net,optimizer,scheduler,epoch_start, dataset, valid_dataset, train_loader):\n",
        "      # Get train data loader\n",
        "      _,valid_loader =valid_dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "      net.train()\n",
        "      for epoch in range(epoch_start,self.n_epochs):\n",
        "\n",
        "          scheduler.step()\n",
        "          if epoch in self.lr_milestones:\n",
        "              print('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n",
        "\n",
        "          loss_epoch = 0.0\n",
        "          n_batches = 0\n",
        "          epoch_start_time = time.time()\n",
        "          for data in train_loader:\n",
        "              inputs, _, _ = data\n",
        "              inputs = inputs.to(self.device)\n",
        "\n",
        "              # Zero the network parameter gradients\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              # Update network parameters via backpropagation: forward + backward + optimize\n",
        "              outputs = net(inputs)\n",
        "              dist = torch.sum((outputs - self.c) ** 2, dim=1)\n",
        "              if self.objective == 'soft-boundary':\n",
        "                  scores = dist - self.R ** 2\n",
        "                  loss = self.R ** 2 + (1 / self.nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n",
        "              else:\n",
        "                  loss = torch.mean(dist)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              # Update hypersphere radius R on mini-batch distances\n",
        "              if (self.objective == 'soft-boundary') and (epoch >= self.warm_up_n_epochs):\n",
        "                  self.R.data = torch.tensor(get_radius(dist, self.nu), device=self.device)\n",
        "\n",
        "              loss_epoch += loss.item()\n",
        "              n_batches += 1\n",
        "\n",
        "          # log epoch statistics\n",
        "          if epoch % 10 ==0:\n",
        "            loss_eval=0.0\n",
        "            n_eval_batches=0\n",
        "            self.validation['loss'].append(loss_epoch / n_batches)\n",
        "            net.eval()\n",
        "            with torch.no_grad():\n",
        "                for data in valid_loader:\n",
        "                    inputs, labels, idx = data\n",
        "                    e_inputs = inputs.to(self.device)\n",
        "                    e_outputs = net(e_inputs)\n",
        "                    dist = torch.sum((outputs - self.c) ** 2, dim=1)\n",
        "                    if self.objective == 'soft-boundary':\n",
        "                        e_scores = dist - self.R ** 2\n",
        "                    else:\n",
        "                        e_scores = dist                      \n",
        "                    \n",
        "                    eval_loss = torch.mean(e_scores)\n",
        "\n",
        "                    loss_eval += eval_loss.item()\n",
        "                    n_eval_batches += 1\n",
        "\n",
        "            self.validation['valid'].append(loss_eval / n_eval_batches)\n",
        "            net.train()\n",
        "          epoch_train_time = time.time() - epoch_start_time\n",
        "          print('  Epoch {}/{}\\t Time: {:.3f}\\t Loss: {:.8f}'\n",
        "                      .format(epoch + 1, self.n_epochs, epoch_train_time, loss_epoch / n_batches))\n",
        "          self.store_tmp(epoch+1,optimizer,scheduler,net)\n",
        "\n",
        "      # self.train_time = time.time() - start_time\n",
        "      # print('Training time: %.3f' % self.train_time)\n",
        "\n",
        "      print('Finished training.')\n",
        "\n",
        "      return net,self.validation\n",
        "\n",
        "    def test(self, dataset: BaseADDataset, net: BaseNet):\n",
        "        logger = logging.getLogger()\n",
        "\n",
        "        # Set device for network\n",
        "        net = net.to(self.device)\n",
        "\n",
        "        # Get test data loader\n",
        "        _, test_loader = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "        # Testing\n",
        "        print('Starting testing...')\n",
        "        start_time = time.time()\n",
        "        idx_label_score = []\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                inputs, labels, idx = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                outputs = net(inputs)\n",
        "                dist = torch.sum((outputs - self.c) ** 2, dim=1)\n",
        "                if self.objective == 'soft-boundary':\n",
        "                    scores = dist - self.R ** 2\n",
        "                else:\n",
        "                    scores = dist\n",
        "\n",
        "                # Save triples of (idx, label, score) in a list\n",
        "                idx_label_score += list(zip(idx.cpu().data.numpy().tolist(),\n",
        "                                            labels.cpu().data.numpy().tolist(),\n",
        "                                            scores.cpu().data.numpy().tolist(),outputs))\n",
        "\n",
        "        self.test_time = time.time() - start_time\n",
        "        print('Testing time: %.3f' % self.test_time)\n",
        "\n",
        "        self.test_scores = idx_label_score\n",
        "\n",
        "        # Compute AUC\n",
        "        _, labels, scores,_ = zip(*idx_label_score)\n",
        "        labels = np.array(labels)\n",
        "        scores = np.array(scores)\n",
        "\n",
        "        self.test_auc = roc_auc_score(labels, scores)\n",
        "        print('Test set AUC: {:.2f}%'.format(100. * self.test_auc))\n",
        "\n",
        "        print('Finished testing.')\n",
        "\n",
        "    def init_center_c(self, train_loader: DataLoader, net: BaseNet, eps=0.1):\n",
        "        \"\"\"Initialize hypersphere center c as the mean from an initial forward pass on the data.\"\"\"\n",
        "\n",
        "        outs=None\n",
        "        net.eval()\n",
        "        isFirst=True\n",
        "        with torch.no_grad():\n",
        "            for data in train_loader:\n",
        "                inputs, labels, idx = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                if isFirst:\n",
        "                  outs = net(inputs).cpu()\n",
        "                  isFirst=False\n",
        "                else:\n",
        "                  outs = np.append(outs,net(inputs).cpu(),axis=0)\n",
        "\n",
        "        \n",
        "\n",
        "        from sklearn.cluster import KMeans\n",
        "        import math\n",
        "        kmeans = KMeans(n_clusters=3)\n",
        "        kmeans.fit(outs)\n",
        "        c=kmeans.cluster_centers_\n",
        "        return torch.tensor(c,device=self.device)\n",
        "\n",
        "    def store_tmp(self,c_epoch,opt,schedul,ae):\n",
        "      torch.save({'optim_nameR': self.optimizer_name ,\n",
        "                    'lr': self.lr,\n",
        "                    'n_epochs': self.n_epochs,\n",
        "                    'lr_milestones': self.lr_milestones,\n",
        "                    'batch_size': self.batch_size,\n",
        "                    'weight_decay': self.weight_decay,\n",
        "                    'device': self.device,\n",
        "                    'n_jobs_dataloader': self.n_jobs_dataloader,\n",
        "                    'current_epoch':c_epoch,\n",
        "                    'validations':self.validation,\n",
        "                    'optimaizer': opt.state_dict(),\n",
        "                    'sched': schedul.state_dict(),\n",
        "                    'R': self.R,\n",
        "                    'c': self.c, \n",
        "                    'ae': ae.state_dict()}, '/content/gdrive/MyDrive/ArshadPeoject/model/tmp/ae-clf.dict')\n",
        "\n",
        "    def load_and_continue(self,dataset, valid_dataset, ae):\n",
        "          data =  torch.load('/content/gdrive/MyDrive/ArshadPeoject/model/tmp/ae-clf.dict',map_location=torch.device(self.device))\n",
        "          train_loader, _ = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "          self.optimizer_name = data['optim_nameR']\n",
        "          self.lr =  data['lr']\n",
        "          # self.n_epochs =  data['n_epochs']\n",
        "          # self.lr_milestones =  data['lr_milestones']\n",
        "          self.batch_size =  data['batch_size']\n",
        "          self.weight_decay =  data['weight_decay']\n",
        "          # self.device =  data['device']\n",
        "          self.n_jobs_dataloader =  data['n_jobs_dataloader']\n",
        "          self.validation = data['validations']\n",
        "\n",
        "          self.R= data['R']\n",
        "          self.c= data['c']\n",
        "\n",
        "          # print(device)\n",
        "          ae = ae.to(self.device)\n",
        "\n",
        "          # data['sched']['milestones']=[118,200]\n",
        "          # data['sched']['last_epoch']=0\n",
        "          print(data['sched'])\n",
        "          # print(data['optimaizer'])\n",
        "\n",
        "          curr_epoch =data['current_epoch']\n",
        "          print(curr_epoch)\n",
        "          ae.load_state_dict(data['ae'])\n",
        "\n",
        "\n",
        "          opt=optim.Adam(ae.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n",
        "                                  amsgrad=self.optimizer_name == 'amsgrad')\n",
        "\n",
        "          sche= optim.lr_scheduler.MultiStepLR(opt, milestones=self.lr_milestones, gamma=0.1)\n",
        "        \n",
        "          opt.load_state_dict(data['optimaizer'])\n",
        "\n",
        "          sche.load_state_dict(data['sched'])\n",
        "          print(sche.milestones)\n",
        "\n",
        "          return self.train_worker(ae,opt,sche,curr_epoch, dataset, valid_dataset,train_loader)\n",
        "\n",
        "def get_radius(dist: torch.Tensor, nu: float):\n",
        "    \"\"\"Optimally solve for radius R via the (1-nu)-quantile of distances.\"\"\"\n",
        "    return np.quantile(np.sqrt(dist.clone().data.cpu().numpy()), 1 - nu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSsJgijKgdHL"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXRlkRKzgh4E"
      },
      "outputs": [],
      "source": [
        "def read(base_path):\n",
        "  n_exps = 3\n",
        "  n_seeds = 3\n",
        "\n",
        "  exps = range(n_exps)\n",
        "  seeds = range(1, n_seeds)\n",
        "\n",
        "  for exp in exps:\n",
        "\n",
        "      exp_folder = str(exp) + 'vsall'\n",
        "      aucs = np.zeros(n_seeds, dtype=np.float32)\n",
        "\n",
        "      for seed in seeds:\n",
        "\n",
        "          seed_folder = 'seed_' + str(seed)\n",
        "          file_name = 'results.json'\n",
        "          file_path = base_path + '/' + exp_folder + '/' + seed_folder + '/' + file_name\n",
        "\n",
        "          with open(file_path, 'r') as fp:\n",
        "              results = json.load(fp)\n",
        "\n",
        "          aucs[seed - 1] = results['test_auc']\n",
        "\n",
        "      mean = np.mean(aucs[aucs > 0])\n",
        "      std = np.std(aucs[aucs > 0])\n",
        "\n",
        "      # Write results\n",
        "      log_file = '{}/result.txt'.format(base_path)\n",
        "      log = open(log_file, 'a')\n",
        "      log.write('Experiment: {}\\n'.format(exp_folder))\n",
        "      log.write('Test Set AUC [mean]: {} %\\n'.format(round(float(mean * 100), 4)))\n",
        "      log.write('Test Set AUC [std]: {} %\\n'.format(round(float(std * 100), 4)))\n",
        "      log.write('\\n')\n",
        "\n",
        "  log.write('\\n')\n",
        "  log.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuud9CRcgnB1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    \"\"\"Base class for experimental setting/configuration.\"\"\"\n",
        "\n",
        "    def __init__(self, settings):\n",
        "        self.settings = settings\n",
        "\n",
        "    def load_config(self, import_json):\n",
        "        \"\"\"Load settings dict from import_json (path/filename.json) JSON-file.\"\"\"\n",
        "\n",
        "        with open(import_json, 'r') as fp:\n",
        "            settings = json.load(fp)\n",
        "\n",
        "        for key, value in settings.items():\n",
        "            self.settings[key] = value\n",
        "\n",
        "    def save_config(self, export_json):\n",
        "        \"\"\"Save settings dict to export_json (path/filename.json) JSON-file.\"\"\"\n",
        "\n",
        "        with open(export_json, 'w') as fp:\n",
        "            json.dump(self.settings, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSiaYH1jiRS8"
      },
      "outputs": [],
      "source": [
        "def plot_images_grid(x: torch.tensor, export_img, title: str = '', nrow=8, padding=2, normalize=True, pad_value=0):\n",
        "    \"\"\"Plot 4D Tensor of images of shape (B x C x H x W) as a grid.\"\"\"\n",
        "\n",
        "    grid = make_grid(x, nrow=nrow, padding=padding, normalize=normalize, pad_value=pad_value)\n",
        "    npgrid = grid.cpu().numpy()\n",
        "\n",
        "    plt.imshow(np.transpose(npgrid, (1, 2, 0)), interpolation='nearest')\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.xaxis.set_visible(True)\n",
        "    ax.yaxis.set_visible(True)\n",
        "\n",
        "    if not (title == ''):\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.savefig(export_img, bbox_inches='tight', pad_inches=1)\n",
        "    plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l"
      ],
      "metadata": {
        "id": "EgkDJv7Wilqe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "c36fc589-c8e0-43f5-c5f7-e35aa8223c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-cde25b5e10ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'l' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxxF5BTlieyp"
      },
      "source": [
        "# Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3BShsTAik1e"
      },
      "outputs": [],
      "source": [
        "class My_LeNet(BaseNet):\n",
        "\n",
        "    def __init__(self):\n",
        "      \n",
        "        super().__init__()\n",
        "        self.rep_dim = 8\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.pool5n5 = nn.MaxPool2d(5, 5)\n",
        "        # self.fc1 = nn.Linear(520, 512, bias=False)\n",
        "        # self.bn1d0 = nn.BatchNorm1d(512, eps=1e-04, affine=False)\n",
        "        self.conv1 = nn.Conv2d(1,2,3, bias=False, padding=1)\n",
        "        self.bn2d1 = nn.BatchNorm2d(2, eps=1e-04, affine=False)\n",
        "        self.conv2 = nn.Conv2d(2,4,3, bias=False, padding=1)\n",
        "        self.bn2d2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.conv3 = nn.Conv2d(4,8,3, bias=False, padding=1)\n",
        "        self.bn2d3 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.conv4 = nn.Conv2d(8,16,3, bias=False, padding=1)\n",
        "        self.bn2d4 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "        self.fc0 = nn.Linear(16*13*13, self.rep_dim, bias=False)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # x=x.unsqueeze(1)\n",
        "        # print(x)\n",
        "        # x = self.bn1d0(self.fc1(x))\n",
        "        # x\n",
        "        # x = x.view(x.size(0), 1,16, 32)\n",
        "        # print(x.size())\n",
        "        # print(x)\n",
        "        # q=2/0\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
        "        x = self.conv4(x)\n",
        "        x = self.pool5n5(F.leaky_relu(self.bn2d4(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x= self.fc0(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class My_LeNet_Autoencoder(BaseNet):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rep_dim = 8\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.pool5n5 = nn.MaxPool2d(5, 5)\n",
        "\n",
        "        # self.fc1 = nn.Linear(520, 512, bias=False)\n",
        "        # self.bn1d0 = nn.BatchNorm1d(512, eps=1e-04, affine=False)\n",
        "        self.conv1 = nn.Conv2d(1,2,3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d1 = nn.BatchNorm2d(2, eps=1e-04, affine=False)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(2,4,3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(4,8,3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d3 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(8,16,3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d4 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "        # self.conv4 = nn.Conv2d(8,16,3, bias=False, padding=1)\n",
        "        # nn.init.xavier_uniform_(self.conv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        # self.bn2d4 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "\n",
        "        self.fc0 = nn.Linear(16*13*13, self.rep_dim, bias=False)\n",
        "\n",
        "        self.bn1d1 = nn.BatchNorm1d(self.rep_dim, eps=1e-04, affine=False)\n",
        "        # Encoder (must match the Deep SVDD network above\n",
        "    \n",
        "\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(self.rep_dim, 16*13*13, bias=False)\n",
        "\n",
        "        self.bn1d2 = nn.BatchNorm1d(16*13*13, eps=1e-04, affine=False)\n",
        "\n",
        "        self.deconv0 = nn.ConvTranspose2d(16, 8, 3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.deconv0.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d5 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(8, 4, 3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.deconv1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d6 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "\n",
        "        self.deconv2 = nn.ConvTranspose2d(4, 2, 3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.deconv2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d7 = nn.BatchNorm2d(2, eps=1e-04, affine=False)\n",
        "\n",
        "        self.deconv3 = nn.ConvTranspose2d(2, 1, 3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.deconv3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d8 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "\n",
        "        # self.deconv4 = nn.ConvTranspose2d(8, 1, 3, bias=False, padding=1)\n",
        "        # nn.init.xavier_uniform_(self.deconv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        # self.bn2d9 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "        # self.fc2 = nn.Linear(512, 520, bias=False)\n",
        "\n",
        "        # self.deconv5 = nn.ConvTranspose2d(8, 1, 3, bias=False, padding=1)\n",
        "        # nn.init.xavier_uniform_(self.deconv5.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        # self.bn2d9 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "        # self.fc2 = nn.Linear(256, 200, bias=False)\n",
        "\n",
        "\n",
        "          # print(x.size())\n",
        "        # x=x.unsqueeze(1)\n",
        "        # print(x.size())\n",
        "    def forward(self, x):\n",
        "        # print(x.size())\n",
        "        # print(x.size())\n",
        "        # x = self.bn1d0(self.fc1(x))\n",
        "        # print(x.size())\n",
        "        # x = x.view(x.size(0), 1,16, 32)\n",
        "        # print(x)\n",
        "        # print(x.size())\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
        "        # print(x.size())\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
        "        # print(x.size())\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.pool5n5(F.leaky_relu(self.bn2d4(x)))\n",
        "        # print(x.size())\n",
        "        # print(x.size())\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # print(x.size())\n",
        "        # print(x.size())\n",
        "        x= self.bn1d1(self.fc0(x))\n",
        "        # print(x.size())\n",
        "        x= self.bn1d2(self.fc3(x))\n",
        "        # print(x.size())\n",
        "        x = x.view(x.size(0), 16,13,13)\n",
        "\n",
        "        x = self.deconv0(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn2d5(x)), scale_factor=5)\n",
        "        # print(x.size())\n",
        "        x = self.deconv1(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn2d6(x)), scale_factor=2)\n",
        "        # print(x.size())\n",
        "        x = self.deconv2(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn2d7(x)), scale_factor=2)\n",
        "        # print(x.size())\n",
        "        x = self.deconv3(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn2d8(x)), scale_factor=2)\n",
        "        # print(x.size())\n",
        "        # x = self.deconv4(x)\n",
        "        # x = F.interpolate(F.leaky_relu(self.bn2d9(x)), scale_factor=2)\n",
        "        # # x.squeeze(1)\n",
        "        # print(x.size())\n",
        "\n",
        "        # x = x.view(x.size(0), -1)\n",
        "        # print(x.size())\n",
        "        # x =self.fc2(x)\n",
        "\n",
        "        # print(x.size())\n",
        "        # x = torch.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBGw7i3MPkbO"
      },
      "outputs": [],
      "source": [
        "# class My_LeNet(BaseNet):\n",
        "\n",
        "#     def __init__(self):\n",
        "      \n",
        "#         super().__init__()\n",
        "#         self.rep_dim = 50\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "#         self.fc1 = nn.Linear(200, 256, bias=False)\n",
        "#         self.bn1d1 = nn.BatchNorm1d(240, eps=1e-04, affine=False)\n",
        "#         self.conv1 = nn.Conv2d(1,8,3, bias=False, padding=1)\n",
        "#         self.bn2d1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "#         self.conv2 = nn.Conv2d(8,16,3, bias=False, padding=1)\n",
        "#         self.bn2d2 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "#         self.conv3 = nn.Conv2d(16,32,3, bias=False, padding=1)\n",
        "#         self.bn2d3 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n",
        "#         self.conv4 = nn.Conv2d(32,64,3, bias=False, padding=1)\n",
        "#         self.bn2d4 = nn.BatchNorm2d(64, eps=1e-04, affine=False)\n",
        "#         self.fc0 = nn.Linear(64, self.rep_dim, bias=False)\n",
        "\n",
        "        \n",
        "#     def forward(self, x):\n",
        "        \n",
        "#         x=x.unsqueeze(1)\n",
        "#         # print(x)\n",
        "#         x = self.fc1(x)\n",
        "#         x = x.view(x.size(0), 1,16, 16)\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
        "#         x = self.conv3(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
        "#         x = self.conv4(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d4(x)))\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x= self.fc0(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class My_LeNet_Autoencoder(BaseNet):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.rep_dim = 50\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "#         self.fc1 = nn.Linear(200, 256, bias=False)\n",
        "#         self.bn1d1 = nn.BatchNorm1d(240, eps=1e-04, affine=False)\n",
        "#         self.conv1 = nn.Conv2d(1,8,3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.conv1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.conv2 = nn.Conv2d(8,16,3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.conv2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d2 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.conv3 = nn.Conv2d(16,32,3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.conv3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d3 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.conv4 = nn.Conv2d(32,64,3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.conv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d4 = nn.BatchNorm2d(64, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.fc0 = nn.Linear(64, self.rep_dim, bias=False)\n",
        "\n",
        "#         self.bn1d = nn.BatchNorm1d(self.rep_dim, eps=1e-04, affine=False)\n",
        "#         # Encoder (must match the Deep SVDD network above\n",
        "    \n",
        "\n",
        "#         # Decoder\n",
        "#         self.deconv0 = nn.ConvTranspose2d(50, 32, 3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.deconv0.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d5 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.deconv1 = nn.ConvTranspose2d(32, 16, 3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.deconv1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d6 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.deconv2 = nn.ConvTranspose2d(16, 8, 3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.deconv2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d7 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.deconv3 = nn.ConvTranspose2d(8, 1, 3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.deconv3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d8 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "\n",
        "#         # self.deconv4 = nn.ConvTranspose2d(8, 1, 3, bias=False, padding=1)\n",
        "#         # nn.init.xavier_uniform_(self.deconv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         # self.bn2d9 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "#         self.fc2 = nn.Linear(256, 200, bias=False)\n",
        "\n",
        "#         # self.deconv5 = nn.ConvTranspose2d(8, 1, 3, bias=False, padding=1)\n",
        "#         # nn.init.xavier_uniform_(self.deconv5.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         # self.bn2d9 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "#         # self.fc2 = nn.Linear(256, 200, bias=False)\n",
        "\n",
        "\n",
        "      \n",
        "#     def forward(self, x):\n",
        "#         x=x.unsqueeze(1)\n",
        "#         # print(x.size())\n",
        "#         x = self.fc1(x)\n",
        "#         # print(x.size())\n",
        "#         x = x.view(x.size(0), 1,16, 16)\n",
        "#         # print(x.size())\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
        "#         # print(x.size())\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
        "#         # print(x.size())\n",
        "#         x = self.conv3(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
        "#         # print(x.size())\n",
        "#         x = self.conv4(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d4(x)))\n",
        "#         # print(x.size())\n",
        "#         # print(x.size())\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         # print(x.size())\n",
        "#         x= self.bn1d(self.fc0(x))\n",
        "#         # print(x.size())\n",
        "#         x = x.view(x.size(0), 50,1,1)\n",
        "#         # print(x.size())\n",
        "#         x = F.leaky_relu(x)\n",
        "#         x = self.deconv0(x)\n",
        "#         x = F.interpolate(F.leaky_relu(self.bn2d5(x)), scale_factor=2)\n",
        "#         # print(x.size())\n",
        "#         x = self.deconv1(x)\n",
        "#         x = F.interpolate(F.leaky_relu(self.bn2d6(x)), scale_factor=2)\n",
        "#         # print(x.size())\n",
        "#         x = self.deconv2(x)\n",
        "#         x = F.interpolate(F.leaky_relu(self.bn2d7(x)), scale_factor=2)\n",
        "#         # print(x.size())\n",
        "#         x = self.deconv3(x)\n",
        "#         x = F.interpolate(F.leaky_relu(self.bn2d8(x)), scale_factor=2)\n",
        "#         # print(x.size())\n",
        "#         # x = self.deconv4(x)\n",
        "#         # x = F.interpolate(F.leaky_relu(self.bn2d9(x)), scale_factor=2)\n",
        "#         # # x.squeeze(1)\n",
        "#         # print(x.size())\n",
        "\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         # print(x.size())\n",
        "#         x =self.fc2(x)\n",
        "\n",
        "#         # print(x.size())\n",
        "#         # x = torch.sigmoid(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCA8TyOWfc9u",
        "outputId": "289bbcb1-dada-4a8a-9e33-3edc560ee6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.483314773547883\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.53452248,  0.26726124,  0.80178373])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "x=np.array([-4,2,6])\n",
        "print(np.linalg.norm(x))\n",
        "x/np.linalg.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX75D3oaQbwx"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlZlll3Y63bM"
      },
      "outputs": [],
      "source": [
        "# train_set_path=open(\"/content/gdrive/MyDrive/ArshadPeoject/data/normalTrafficTraining.csv\",\"r\")\n",
        "# reader = csv.reader(train_set_path)\n",
        "# c=0\n",
        "# for row in reader:\n",
        "#    print(row)\n",
        "#    c+=1\n",
        "#    if c==11:\n",
        "#       break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEqbHlnkxOc7"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QVY2sudQgm3"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "class HttpDataset(Dataset):\n",
        "    def __init__(self, train, validate=False, useAnomals=True):\n",
        "        self.cols=520\n",
        "        train_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/normalTrafficTraining.csv\"\n",
        "\n",
        "        test_normal_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/normal_test.csv\"\n",
        "        validation_normal_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/normal_valid.csv\"\n",
        "\n",
        "        test_anormal_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/anomalous_test.csv\"\n",
        "        validation_anormal_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/anomalous_valid.csv\"\n",
        "\n",
        "        self.idf = self.idf_counter(train_set_path,self.cols)\n",
        "        self.transform =transforms.ToTensor();\n",
        "        self.target_transform =  transforms.Lambda(lambda x: int(x in self.outlier_classes))\n",
        "        if not train:\n",
        "        \n",
        "          self.anomal_data_test = pd.read_csv(test_anormal_set_path)\n",
        "          lbldf = pd.DataFrame([1]*len(self.anomal_data_test),columns=['label'])\n",
        "          self.anomal_data_test=pd.concat([self.anomal_data_test,lbldf], axis=1)\n",
        "          self.anomal_data_test=self.anomal_data_test.apply(pd.to_numeric,downcast=\"float\")\n",
        "\n",
        "\n",
        "          self.anomal_data_valid = pd.read_csv(validation_anormal_set_path)\n",
        "          lbldf = pd.DataFrame([1]*len(self.anomal_data_valid),columns=['label'])\n",
        "          self.anomal_data_valid=pd.concat([self.anomal_data_valid,lbldf], axis=1)\n",
        "          self.anomal_data_valid=self.anomal_data_valid.apply(pd.to_numeric,downcast=\"float\")\n",
        "\n",
        "          self.normal_data_test = pd.read_csv(test_normal_set_path)\n",
        "          lbldf = pd.DataFrame([0]*len(self.normal_data_test),columns=['label'])\n",
        "          self.normal_data_test=pd.concat([ self.normal_data_test,lbldf], axis=1)\n",
        "          self.normal_data_test=self.normal_data_test.apply(pd.to_numeric,downcast=\"float\")\n",
        "\n",
        "          self.normal_data_valid = pd.read_csv(validation_normal_set_path)\n",
        "          lbldf = pd.DataFrame([0]*len(self.normal_data_valid),columns=['label'])\n",
        "          self.normal_data_valid=pd.concat([ self.normal_data_valid,lbldf], axis=1)\n",
        "          self.normal_data_valid=self.normal_data_valid.apply(pd.to_numeric,downcast=\"float\")\n",
        "\n",
        "\n",
        "        self.useAnomals=useAnomals\n",
        "        self.validate=validate\n",
        "        self.train=train\n",
        "        self.data = pd.read_csv(train_set_path)\n",
        "        lbldf = pd.DataFrame([0]*len(self.data),columns=['label'])\n",
        "        self.data=pd.concat([self.data,lbldf], axis=1)\n",
        "        self.data =self.data.apply(pd.to_numeric,downcast=\"float\")\n",
        "\n",
        "\n",
        "        self.filtered_data = self.data\n",
        "\n",
        "    def __len__(self):\n",
        "      if self.train :\n",
        "        return len(self.filtered_data)\n",
        "      else:\n",
        "        if self.validate:\n",
        "          if(self.useAnomals):\n",
        "            return len(self.normal_data_valid)+len(self.anomal_data_valid)\n",
        "          else:\n",
        "            return len(self.normal_data_valid)\n",
        "        else:\n",
        "          if(self.useAnomals):\n",
        "            return len(self.normal_data_test)+len(self.anomal_data_test)\n",
        "          else:\n",
        "            return len(self.normal_data_test)\n",
        "\n",
        "    def idf_counter(self , path, col):\n",
        "\n",
        "      normal_file = open(path, \"r\")\n",
        "      reader = csv.reader(normal_file)\n",
        "      c=0\n",
        "      occInDoc = np.array([0.0]*col)\n",
        "      isFirst=False\n",
        "      for row in reader:\n",
        "        if not isFirst:\n",
        "          isFirst=True\n",
        "        else:\n",
        "          c+=1\n",
        "          for i in range(0,col):\n",
        "            if int(row[i])>0 :\n",
        "              occInDoc[i]+=1;\n",
        "\n",
        "      for i in range(0,col):\n",
        "        if occInDoc[i]==0:\n",
        "          occInDoc[i]=math.log(c);\n",
        "        else:\n",
        "          if(occInDoc[i]==c):\n",
        "            occInDoc[i]-=1\n",
        "          occInDoc[i] = math.log(c/occInDoc[i])\n",
        "      return occInDoc\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(\"idx=\",idx)\n",
        "        label=0\n",
        "        if self.train:\n",
        "          data = self.filtered_data.iloc[idx,:self.cols].to_numpy()\n",
        "        else:\n",
        "          if self.validate:\n",
        "              len_d=len(self.normal_data_valid)\n",
        "              if self.useAnomals:\n",
        "                if(idx >= len_d):\n",
        "                  i=idx-len_d\n",
        "                  # print(\"i=\",str(i),\"  len=\",str(len(anomal_data)))\n",
        "                  data = self.anomal_data_valid.iloc[i,:self.cols].to_numpy()\n",
        "                  label=1\n",
        "                else:\n",
        "                  data = self.normal_data_valid.iloc[idx,:self.cols].to_numpy()\n",
        "              else:\n",
        "                data = self.normal_data_valid.iloc[idx,:self.cols].to_numpy()\n",
        "          else:\n",
        "            if self.useAnomals:\n",
        "              len_d=len(self.normal_data_test)\n",
        "              if(idx >= len_d):\n",
        "                i=idx-len_d\n",
        "                # print(\"i=\",str(i),\"  len=\",str(len(anomal_data)))\n",
        "                data = self.anomal_data_test.iloc[i,:self.cols].to_numpy()\n",
        "                label=1\n",
        "              else:\n",
        "                data = self.normal_data_test.iloc[idx,:self.cols].to_numpy() \n",
        "            else:\n",
        "              data = self.normal_data_test.iloc[idx,:self.cols].to_numpy()\n",
        "           \n",
        "        \n",
        "        \n",
        "\n",
        "        # print(\"ortg, \", data)\n",
        "        # data =(data/np.sum(data))*self.idf\n",
        "        # data= np.float32(data)       \n",
        "        # print(data)\n",
        "        d = np.array([data])\n",
        "        d_t= d.transpose()\n",
        "        f_d=np.dot(d_t,d)/2\n",
        "        np.fill_diagonal(f_d,0)\n",
        "        # print(f_d.shape)\n",
        "\n",
        "        min = np.min(f_d)\n",
        "        max = np.max(f_d)\n",
        "        r= max - min\n",
        "        f_d= (f_d-min)/r\n",
        "        # print(idx, avg , \", \",data)\n",
        "        # c_data = \n",
        "\n",
        "        return  np.array([f_d]), label,idx\n",
        "    \n",
        "    def filter_data(self, indexes):\n",
        "      self.filtered_data= self.data.iloc[indexes]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4K_2mPbbL-y"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QAplBeFXQAr",
        "outputId": "a9c10bdc-ebdf-4d53-ed97-fa08edeaee96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np \n",
        "\n",
        "t=np.array([[1,2,3,4]])\n",
        "t_trans = t.transpose()\n",
        "# print(t_trans)\n",
        "# print(t)\n",
        "r = np.dot(t_trans,t)\n",
        "np.fill_diagonal(r,0)\n",
        "# print(r)r\n",
        "\n",
        "# print(type(np.array(r)))\n",
        "print(r.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YwBhDLW7OsW"
      },
      "outputs": [],
      "source": [
        "class My_Dataset(TorchvisionDataset):\n",
        "\n",
        "    def __init__(self, root: str, normal_class=5):\n",
        "        super().__init__(root)\n",
        "\n",
        "        self.n_classes = 1  # 0: normal, 1: outlier\n",
        "        # self.normal_classes = tuple([normal_class])\n",
        "        # self.outlier_classes = list(range(0, 10))\n",
        "        # self.outlier_classes.remove(normal_class)\n",
        "\n",
        "        # # Pre-computed min and max values (after applying GCN) from train data per class\n",
        "        # min_max = [(-28.94083453598571, 13.802961825439636),\n",
        "        #            (-6.681770233365245, 9.158067708230273),\n",
        "        #            (-34.924463588638204, 14.419298165027628),\n",
        "        #            (-10.599172931391799, 11.093187820377565),\n",
        "        #            (-11.945022995801637, 10.628045447867583),\n",
        "        #            (-9.691969487694928, 8.948326776180823),\n",
        "        #            (-9.174940012342555, 13.847014686472365),\n",
        "        #            (-6.876682005899029, 12.282371383343161),\n",
        "        #            (-15.603507135507172, 15.2464923804279),\n",
        "        #            (-6.132882973622672, 8.046098172351265)]\n",
        "\n",
        "        # # CIFAR-10 preprocessing: GCN (with L1 norm) and min-max feature scaling to [0,1]\n",
        "        # transform = transforms.Compose([transforms.ToTensor(),\n",
        "        #                                 transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1')),\n",
        "        #                                 transforms.Normalize([min_max[normal_class][0]] * 3,\n",
        "        #                                                      [min_max[normal_class][1] - min_max[normal_class][0]] * 3)])\n",
        "\n",
        "        # target_transform = transforms.Lambda(lambda x: int(x in self.outlier_classes))\n",
        "\n",
        "        # train_set = \n",
        "        # # Subset train set to normal class\n",
        "        # train_idx_normal = get_target_label_idx(train_set.train_labels, self.normal_classes)\n",
        "        self.train_set = HttpDataset(train=True)\n",
        "        self.test_set = HttpDataset(train=False,validate=True)\n",
        "\n",
        "    def filter_data(self,idx):\n",
        "      self.train_set.filter_data(idx)\n",
        "\n",
        "\n",
        "class My_Dataset_cross(TorchvisionDataset):\n",
        "\n",
        "    def __init__(self, root: str, normal_class=5):\n",
        "        super().__init__(root)\n",
        "\n",
        "        self.n_classes = 1  # 0: normal, 1: outlier\n",
        "        # self.normal_classes = tuple([normal_class])\n",
        "        # self.outlier_classes = list(range(0, 10))\n",
        "        # self.outlier_classes.remove(normal_class)\n",
        "\n",
        "        # # Pre-computed min and max values (after applying GCN) from train data per class\n",
        "        # min_max = [(-28.94083453598571, 13.802961825439636),\n",
        "        #            (-6.681770233365245, 9.158067708230273),\n",
        "        #            (-34.924463588638204, 14.419298165027628),\n",
        "        #            (-10.599172931391799, 11.093187820377565),\n",
        "        #            (-11.945022995801637, 10.628045447867583),\n",
        "        #            (-9.691969487694928, 8.948326776180823),\n",
        "        #            (-9.174940012342555, 13.847014686472365),\n",
        "        #            (-6.876682005899029, 12.282371383343161),\n",
        "        #            (-15.603507135507172, 15.2464923804279),\n",
        "        #            (-6.132882973622672, 8.046098172351265)]\n",
        "\n",
        "        # # CIFAR-10 preprocessing: GCN (with L1 norm) and min-max feature scaling to [0,1]\n",
        "        # transform = transforms.Compose([transforms.ToTensor(),\n",
        "        #                                 transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1')),\n",
        "        #                                 transforms.Normalize([min_max[normal_class][0]] * 3,\n",
        "        #                                                      [min_max[normal_class][1] - min_max[normal_class][0]] * 3)])\n",
        "\n",
        "        # target_transform = transforms.Lambda(lambda x: int(x in self.outlier_classes))\n",
        "\n",
        "        # train_set = \n",
        "        # # Subset train set to normal class\n",
        "        # train_idx_normal = get_target_label_idx(train_set.train_labels, self.normal_classes)\n",
        "        self.train_set = HttpDataset(train=True)\n",
        "        self.test_set = HttpDataset(train=False,useAnomals=False,validate=True)\n",
        "\n",
        "    def filter_data(self,idx):\n",
        "      self.train_set.filter_data(idx)\n",
        "class My_Dataset_preTrain(TorchvisionDataset):\n",
        "\n",
        "    def __init__(self, root: str, normal_class=5):\n",
        "        super().__init__(root)\n",
        "\n",
        "        self.n_classes = 1  # 0: normal, 1: outlier\n",
        "        # self.normal_classes = tuple([normal_class])\n",
        "        # self.outlier_classes = list(range(0, 10))\n",
        "        # self.outlier_classes.remove(normal_class)\n",
        "\n",
        "        # # Pre-computed min and max values (after applying GCN) from train data per class\n",
        "        # min_max = [(-28.94083453598571, 13.802961825439636),\n",
        "        #            (-6.681770233365245, 9.158067708230273),\n",
        "        #            (-34.924463588638204, 14.419298165027628),\n",
        "        #            (-10.599172931391799, 11.093187820377565),\n",
        "        #            (-11.945022995801637, 10.628045447867583),\n",
        "        #            (-9.691969487694928, 8.948326776180823),\n",
        "        #            (-9.174940012342555, 13.847014686472365),\n",
        "        #           (device)\n",
        "    # F (-6.876682005899029, 12.282371383343161),\n",
        "        #            (-15.603507135507172, 15.2464923804279),\n",
        "        #            (-6.132882973622672, 8.046098172351265)]\n",
        "\n",
        "        # # CIFAR-10 preprocessing: GCN (with L1 norm) and min-max feature scaling to [0,1]\n",
        "        # transform = transforms.Compose([transforms.ToTensor(),\n",
        "        #                                 transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1')),\n",
        "        #                                 transforms.Normalize([min_max[normal_class][0]] * 3,\n",
        "        #                                                      [min_max[normal_class][1] - min_max[normal_class][0]] * 3)])\n",
        "\n",
        "        # target_transform = transforms.Lambda(lambda x: int(x in self.outlier_classes))\n",
        "\n",
        "        # train_set = \n",
        "        # # Subset train set to normal class\n",
        "        # train_idx_normal = get_target_label_idx(train_set.train_labels, self.normal_classes)\n",
        "        self.train_set = HttpDataset(train=True)\n",
        "        self.test_set = HttpDataset(train=False,useAnomals=False)\n",
        "        # self.validation_set =\n",
        "\n",
        "\n",
        "    def filter_data(self,idx):\n",
        "      self.train_set.filter_data(idx)\n",
        "class My_Dataset_Test(TorchvisionDataset):\n",
        "\n",
        "    def __init__(self, root: str, normal_class=5):\n",
        "        super().__init__(root)\n",
        "        self.n_classes = 1  # 0: normal, 1: outlier\n",
        "        # self.normal_classes = tuple([normal_class])\n",
        "        # self.outlier_classes = list(range(0, 10))\n",
        "        # self.outlier_classes.remove(normal_class)\n",
        "\n",
        "        # # Pre-computed min and max values (after applying GCN) from train data per class\n",
        "        # min_max = [(-28.94083453598571, 13.802961825439636),\n",
        "        #            (-6.681770233365245, 9.158067708230273),\n",
        "        #            (-34.924463588638204, 14.419298165027628),\n",
        "        #            (-10.599172931391799, 11.093187820377565),\n",
        "        #            (-11.945022995801637, 10.628045447867583),\n",
        "        #            (-9.691969487694928, 8.948326776180823),\n",
        "        #            (-9.174940012342555, 13.847014686472365),\n",
        "        #            (-6.876682005899029, 12.282371383343161),\n",
        "        #            (-15.603507135507172, 15.2464923804279),\n",
        "        #            (-6.132882973622672, 8.046098172351265)]\n",
        "\n",
        "        # # CIFAR-10 preprocessing: GCN (with L1 norm) and min-max feature scaling to [0,1]\n",
        "        # transform = transforms.Compose([transforms.ToTensor(),\n",
        "        #                                 transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1')),\n",
        "        #                                 transforms.Normalize([min_max[normal_class][0]] * 3,\n",
        "        #                                                      [min_max[normal_class][1] - min_max[normal_class][0]] * 3)])\n",
        "\n",
        "        # target_transform = transforms.Lambda(lambda x: int(x in self.outlier_classes))\n",
        "\n",
        "        # train_set = \n",
        "        # # Subset train set to normal class\n",
        "        # train_idx_normal = get_target_label_idx(train_set.train_labels, self.normal_classes)\n",
        "        self.train_set = HttpDataset(train=True)\n",
        "        self.test_set = HttpDataset(train=False)\n",
        "        # self.validation_set =\n",
        "\n",
        "\n",
        "    def filter_data(self,idx):\n",
        "      self.train_set.filter_data(idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chZMmVW9dtov"
      },
      "source": [
        "# Deep-SVDD Net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VxRPjafkyhT"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAsiJliMV5Dz"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWV_6mtft2_N"
      },
      "outputs": [],
      "source": [
        "# dataset_p=My_Dataset_preTrain(root=\"ll\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_3MsznRclM2"
      },
      "outputs": [],
      "source": [
        "# d, _= dataset_p.loaders(batch_size=1)\n",
        "# for i in d:\n",
        "#   print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_baDdBkd3um"
      },
      "outputs": [],
      "source": [
        "class DeepSVDD(object):\n",
        "    \"\"\"A class for the Deep SVDD method.\n",
        "        objective: A string specifying the Deep SVDD objective (either 'one-class' or 'soft-boundary').\n",
        "        nu: Deep SVDD hyperparameter nu (must be 0 < nu <= 1).\n",
        "        R: Hypersphere radius R.\n",
        "        c: Hypersphere center c.\n",
        "        net_name: A string indicating the name of the neural network to use.\n",
        "        net: The neural network \\phi.\n",
        "        ae_net: The autoencoder network corresponding to \\phi for network weights pretraining.\n",
        "        trainer: DeepSVDDTrainer to train a Deep SVDD model.\n",
        "        optimizer_name: A string indicating the optimizer to use for training the Deep SVDD network.\n",
        "        ae_trainer: AETrainer to train an autoencoder in pretraining.\n",
        "        ae_optimizer_name: A string indicating the optimizer to use for pretraining the autoencoder.\n",
        "        results: A dictionary to save the results.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, objective: str = 'one-class', nu: float = 0.1):\n",
        "        \"\"\"Inits DeepSVDD with one of the two objectives and hyperparameter nu.\"\"\"\n",
        "\n",
        "        assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n",
        "        self.objective = objective\n",
        "        assert (0 < nu) & (nu <= 1), \"For hyperparameter nu, it must hold: 0 < nu <= 1.\"\n",
        "        self.nu = nu\n",
        "        self.R = 0.0  # hypersphere radius R\n",
        "        self.c = None  # hypersphere center c\n",
        "\n",
        "        self.net_name = None\n",
        "        self.net = None  # neural network \\phi\n",
        "\n",
        "        self.trainer = None\n",
        "        self.optimizer_name = None\n",
        "\n",
        "        self.ae_net = None  # autoencoder network for pretraining\n",
        "        self.ae_trainer = None\n",
        "        self.ae_optimizer_name = None\n",
        "\n",
        "        self.results = {\n",
        "            'train_time': None,\n",
        "            'test_auc': None,\n",
        "            'test_time': None,\n",
        "            'test_scores': None,\n",
        "        }\n",
        "\n",
        "    def set_network(self, net_name):\n",
        "        \"\"\"Builds the neural network \\phi.\"\"\"\n",
        "        # self.net_name = net_name\n",
        "        self.net = My_LeNet()\n",
        "\n",
        "    def train(self, dataset: BaseADDataset, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 50,\n",
        "              lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "              n_jobs_dataloader: int = 0):\n",
        "        \"\"\"Trains the Deep SVDD model on the training data.\"\"\"\n",
        "\n",
        "        self.optimizer_name = optimizer_name\n",
        "        self.trainer = DeepSVDDTrainer(self.objective, self.R, self.c, self.nu, optimizer_name, lr=lr,\n",
        "                                       n_epochs=n_epochs, lr_milestones=lr_milestones, batch_size=batch_size,\n",
        "                                       weight_decay=weight_decay, device=device, n_jobs_dataloader=n_jobs_dataloader)\n",
        "        # Get the model\n",
        "        self.net = self.trainer.train(dataset, self.net)\n",
        "        self.R = float(self.trainer.R.cpu().data.numpy())  # get float\n",
        "        self.c = self.trainer.c.cpu().data.numpy().tolist()  # get list\n",
        "        self.results['train_time'] = self.trainer.train_time\n",
        "\n",
        "    def test(self, dataset: BaseADDataset, device: str = 'cuda', n_jobs_dataloader: int = 0):\n",
        "        \"\"\"Tests the Deep SVDD model on the test data.\"\"\"\n",
        "\n",
        "        if self.trainer is None:\n",
        "            self.trainer = DeepSVDDTrainer(self.objective, self.R, self.c, self.nu,\n",
        "                                           device=device, n_jobs_dataloader=n_jobs_dataloader)\n",
        "\n",
        "        self.trainer.test(dataset, self.net)\n",
        "        # Get results\n",
        "        self.results['test_auc'] = self.trainer.test_auc\n",
        "        self.results['test_time'] = self.trainer.test_time\n",
        "        self.results['test_scores'] = self.trainer.test_scores\n",
        "\n",
        "    def pretrain(self, dataset: BaseADDataset, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 100,\n",
        "                 lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "                 n_jobs_dataloader: int = 0):\n",
        "        \"\"\"Pretrains the weights for the Deep SVDD network \\phi via autoencoder.\"\"\"\n",
        "\n",
        "        self.ae_net =My_LeNet_Autoencoder()\n",
        "        self.ae_optimizer_name = optimizer_name\n",
        "        self.ae_trainer = AETrainer(optimizer_name, lr=lr, n_epochs=n_epochs, lr_milestones=lr_milestones,\n",
        "                                    batch_size=batch_size, weight_decay=weight_decay, device=device,\n",
        "                                    n_jobs_dataloader=n_jobs_dataloader)\n",
        "        self.ae_net = self.ae_trainer.train(dataset, self.ae_net)\n",
        "        self.ae_trainer.test(dataset, self.ae_net)\n",
        "        self.init_network_weights_from_pretraining()\n",
        "\n",
        "    def init_network_weights_from_pretraining(self):\n",
        "        \"\"\"Initialize the Deep SVDD network weights from the encoder weights of the pretraining autoencoder.\"\"\"\n",
        "\n",
        "        net_dict = self.net.state_dict()\n",
        "        ae_net_dict = self.ae_net.state_dict()\n",
        "\n",
        "        # Filter out decoder network keys\n",
        "        ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n",
        "        # Overwrite values in the existing state_dict\n",
        "        net_dict.update(ae_net_dict)\n",
        "        # Load the new state_dict\n",
        "        self.net.load_state_dict(net_dict)\n",
        "\n",
        "    def save_model(self, export_model, save_ae=True):\n",
        "        \"\"\"Save Deep SVDD model to export_model.\"\"\"\n",
        "\n",
        "        net_dict = self.net.state_dict()\n",
        "        ae_net_dict = self.ae_net.state_dict() if save_ae else None\n",
        "\n",
        "        torch.save({'R': self.R,\n",
        "                    'c': self.c,\n",
        "                    'net_dict': net_dict,\n",
        "                    'ae_net_dict': ae_net_dict}, export_model)\n",
        "\n",
        "    def load_model(self, model_path, load_ae=False):\n",
        "        \"\"\"Load Deep SVDD model from model_path.\"\"\"\n",
        "\n",
        "        model_dict = torch.load(model_path)\n",
        "\n",
        "        self.R = model_dict['R']\n",
        "        self.c = model_dict['c']\n",
        "        self.net.load_state_dict(model_dict['net_dict'])\n",
        "        if load_ae:\n",
        "            if self.ae_net is None:\n",
        "                self.ae_net = My_LeNet_Autoencoder()\n",
        "            self.ae_net.load_state_dict(model_dict['ae_net_dict'])\n",
        "\n",
        "    def save_results(self, export_json):\n",
        "        \"\"\"Save results dict to a JSON-file.\"\"\"\n",
        "        with open(export_json, 'w') as fp:\n",
        "            json.dump(self.results, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NocuYQUP0vWL"
      },
      "outputs": [],
      "source": [
        "class MSVDD(object):\n",
        "  def __init__(self, objective: str = 'one-class', nu: float = 0.1):\n",
        "    self.pre_trained=False;\n",
        "    assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n",
        "    \n",
        "    self.objective = objective\n",
        "    assert (0 < nu) & (nu <= 1), \"For hyperparameter nu, it must hold: 0 < nu <= 1.\"\n",
        "    self.nu = nu\n",
        "    self.R = 0.0  # hypersphere radius R\n",
        "    self.c = None  # hypersphere center c\n",
        "\n",
        "    self.c_count =1;\n",
        "    self.net_name = None\n",
        "    self.net = None  # neural network \\phi\n",
        "# \n",
        "    self.trainer = None\n",
        "    self.optimizer_name = None\n",
        "\n",
        "    self.ae_net = None  # autoencoder network for pretraining\n",
        "    self.ae_trainer = None\n",
        "    self.ae_optimizer_name = None\n",
        "\n",
        "    self.results = {\n",
        "        'train_time': None,\n",
        "        'test_auc': None,\n",
        "        'test_time': None,\n",
        "        'test_scores': None,\n",
        "    }\n",
        "\n",
        "  \n",
        "  def train(self, dataset: BaseADDataset,valid_dataset: BaseADDataset, k, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 50,\n",
        "          lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "          n_jobs_dataloader: int = 0, load=False,resume=False):\n",
        "    assert self.pre_trained, \"Must preTrain first!\"\n",
        "    self.batch_size=batch_size\n",
        "    self.net= My_LeNet()\n",
        "    net_dict = self.net.state_dict()\n",
        "    ae_net_dict = self.ae_net.state_dict()\n",
        "    self.net = self.net.to(device)\n",
        "    # Filter out decoder network keys\n",
        "    ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n",
        "    # Overwrite values in the existing state_dict\n",
        "    net_dict.update(ae_net_dict)\n",
        "    # Loadwn the new state_dict\n",
        "    self.net.load_state_dict(net_dict)\n",
        "    train_loader, _ = dataset.loaders(batch_size=self.batch_size)\n",
        "    outs=None\n",
        "\n",
        "    # centers = init_c(k,dataset,self.net)\n",
        "\n",
        "\n",
        "    \"\"\"Trains the Deep SVDD model on the training data.\"\"\"\n",
        "    self.optimizer_name = optimizer_name\n",
        "    self.trainer= list()\n",
        "    self.filtered = list()\n",
        "    self.worker_net = list()\n",
        "    self.valid =list()\n",
        "    self.centers =list()\n",
        "   \n",
        "    \n",
        "    if(load):\n",
        "      self.load_clf(k,lr,n_epochs,lr_milestones,batch_size,weight_decay,device,n_jobs_dataloader)\n",
        "    else:\n",
        "      self.net.eval()\n",
        "      isFirst=True\n",
        "      with torch.no_grad():\n",
        "          for data in train_loader:\n",
        "              inputs, labels, idx = data\n",
        "              inputs = inputs.to(device)\n",
        "              if isFirst:\n",
        "                outs = self.net(inputs).cpu()\n",
        "                isFirst=False\n",
        "              else:\n",
        "                outs = np.append(outs,self.net(inputs).cpu(),axis=0)\n",
        "      data_idx= np.arange(0,len(outs))\n",
        "      from sklearn.cluster import KMeans\n",
        "      import math\n",
        "      kmeans = KMeans(n_clusters=k)\n",
        "      predict = kmeans.fit_predict(outs)\n",
        "      self.centers=kmeans.cluster_centers_\n",
        "      for center in self.centers:\n",
        "        self.trainer.append(DeepSVDDTrainer(self.objective, self.R, center, self.nu, optimizer_name, lr=lr,\n",
        "                                      n_epochs=n_epochs, lr_milestones=lr_milestones, batch_size=batch_size,\n",
        "                                      weight_decay=weight_decay, device=device, n_jobs_dataloader=n_jobs_dataloader))\n",
        "      for dd in range (0,len(self.trainer)):\n",
        "        self.filtered.append(data_idx[predict==(dd)])\n",
        "      print(self.centers)\n",
        "      print(self.filtered)\n",
        "      torch.save({\n",
        "          'c_count': self.c_count+1,\n",
        "          'filtered': self.filtered,\n",
        "          'centers': self.centers\n",
        "      }, '/content/gdrive/MyDrive/ArshadPeoject/model/clf_0.dict')\n",
        "      \n",
        "    self.train_worker(dataset,valid_dataset, resume)\n",
        "  def train_worker(self,dataset,valid_dataset,resume):\n",
        "    print(\"t\",len(self.trainer))\n",
        "    time=0\n",
        "    for cls_conter in range(self.c_count,len(self.trainer)+1):\n",
        "      wn= My_LeNet()\n",
        "      net_dict = wn.state_dict()\n",
        "      ae_net_dict = self.ae_net.state_dict()\n",
        "      # Filter out decoder network keys\n",
        "      ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n",
        "      # Overwrite values in the existing state_dict\n",
        "      net_dict.update(ae_net_dict)\n",
        "      # Loadwn the new state_dict\n",
        "      wn.load_state_dict(net_dict)\n",
        "      print(\"training net:\", cls_conter)\n",
        "      dataset.filter_data(self.filtered[cls_conter-1])\n",
        "      if resume:\n",
        "        wn, valid=self.trainer[cls_conter-1].load_and_continue(dataset, valid_dataset, wn)\n",
        "        resume=False\n",
        "      else:\n",
        "        wn, valid=self.trainer[cls_conter-1].train(dataset, valid_dataset, wn)\n",
        "      self.worker_net.append(wn)\n",
        "      self.valid.append(valid)\n",
        "      R = float(self.trainer[cls_conter-1].R.cpu().data.numpy())  # get float\n",
        "      c = self.trainer[cls_conter-1].c.cpu().data.numpy().tolist()  # get list\n",
        "      torch.save({\n",
        "          'c_count': cls_conter+1,\n",
        "          'filtered': self.filtered,\n",
        "          'centers': self.centers\n",
        "      }, '/content/gdrive/MyDrive/ArshadPeoject/model/clf_0.dict')\n",
        "      self.save_clf_model(wn,R,c,valid,'/content/gdrive/MyDrive/ArshadPeoject/model/clf.dict.'+str(cls_conter))\n",
        "      # time+=t.train_time\n",
        "      # cls_conter+=1\n",
        "    \n",
        "    self.results['train_time'] = time\n",
        "    return self.valid\n",
        "  \n",
        "  def test(self, dataset: BaseADDataset, device: str = 'cuda', n_jobs_dataloader: int = 0):\n",
        "      \"\"\"Tests the Deep SVDD model on the test data.\"\"\"\n",
        "      scores = list()\n",
        "      cls_conter=1\n",
        "      for t in self.trainer:\n",
        "        print(\"testing net:\", cls_conter)\n",
        "        t.test(dataset, self.worker_net[cls_conter-1])\n",
        "        scores.append(t.test_scores)\n",
        "        cls_conter+=1\n",
        "      indices=None\n",
        "      labels=None\n",
        "      min_score=None\n",
        "      for s in scores:\n",
        "        indices, labels, x, outputs= zip(*s)\n",
        "        indices, labels, x = np.array(indices), np.array(labels), np.array(x)\n",
        "        if min_score==None :\n",
        "          min_score=torch.tensor(np.array([float('inf')]*len(indices)))\n",
        "        min_score=torch.min(min_score, torch.tensor(x))\n",
        "        # print(indices)\n",
        "      return indices, labels, min_score\n",
        "      \n",
        "\n",
        "\n",
        "      # if self.trainer is None:\n",
        "      #     self.trainer = DeepSVDDTrainer(self.objective, self.R, self.c, self.nu,\n",
        "      #                                     device=device, n_jobs_dataloader=n_jobs_dataloader)\n",
        "\n",
        "  \n",
        "      # # Get results\n",
        "      # self.results['test_auc'] = self.trainer.test_auc\n",
        "      # self.results['test_time'] = self.trainer.test_time\n",
        "      # self.results['test_scores'] = self.trainer.test_scores   \n",
        "\n",
        "  def save_model(self, export_model, save_ae=True):\n",
        "      \"\"\"Save Deep SVDD model to export_model.\"\"\"\n",
        "\n",
        "      ae_net_dict = self.ae_net.state_dict() if save_ae else None\n",
        "\n",
        "      torch.save({'ae_net_dict': ae_net_dict}, export_model)\n",
        "\n",
        "  def save_clf_model(self,wn,R,C,valid, export_model, save_ae=True):\n",
        "      \"\"\"Save Deep SVDD model to export_model.\"\"\"\n",
        "      torch.save({'R': R,\n",
        "                  'c': C,\n",
        "                  'valid':valid,\n",
        "                  'net': wn.state_dict()}, export_model)\n",
        "\n",
        "\n",
        "  def load_model(self, model_path, load_ae=False):\n",
        "      \"\"\"Load Deep SVDD model from model_path.\"\"\"\n",
        "\n",
        "      model_dict = torch.load(model_path,map_location=torch.device('cpu'))\n",
        "      if self.ae_net is None:\n",
        "          self.ae_net = My_LeNet_Autoencoder()\n",
        "      self.ae_net.load_state_dict(model_dict['ae_net_dict'])\n",
        "\n",
        "  def load(self):\n",
        "\n",
        "    self.load_model(\"/content/gdrive/MyDrive/ArshadPeoject/model/base-auto-encoder.dict\")\n",
        "    self.pre_trained=True\n",
        "\n",
        "  def load_clf(self, clf_num,lr, n_epochs, lr_milestones,batch_size,weight_decay,device,n_jobs_dataloader):\n",
        "    self.worker_net = list()\n",
        "\n",
        "    model0 = torch.load('/content/gdrive/MyDrive/ArshadPeoject/model/clf_0.dict')\n",
        "    self.c_count = model0['c_count']\n",
        "    self.filtered = model0['filtered']\n",
        "    self.centers = model0['centers']\n",
        "    print('l',self.c_count)\n",
        "    # print(self.filtered)\n",
        "    \n",
        "    \n",
        "    try:\n",
        "      for cls_conter in range(0,self.c_count):\n",
        "        model = torch.load('/content/gdrive/MyDrive/ArshadPeoject/model/clf.dict.'+str(cls_conter+1),map_location=torch.device('cpu'))\n",
        "        R= model['R']\n",
        "        center=model['c']\n",
        "        wn= My_LeNet()\n",
        "        wn.load_state_dict(model['net'])\n",
        "        self.worker_net.append(wn)\n",
        "        # t.net = wn\n",
        "        self.trainer.append(DeepSVDDTrainer(self.objective, R, center, self.nu, self.optimizer_name, lr=lr,\n",
        "                                    n_epochs=n_epochs, lr_milestones=lr_milestones, batch_size=batch_size,\n",
        "                                    weight_decay=weight_decay, device=device, n_jobs_dataloader=n_jobs_dataloader))\n",
        "    except FileNotFoundError:\n",
        "      self.c_count=self.c_count\n",
        "    print('f',self.c_count)\n",
        "    print(\"ty\",len(self.trainer))\n",
        "\n",
        "    for ff in range(self.c_count-1 ,len(self.centers)):\n",
        "      self.trainer.append(DeepSVDDTrainer(self.objective, self.R, self.centers[ff], self.nu, self.optimizer_name, lr=lr,\n",
        "                                      n_epochs=n_epochs, lr_milestones=lr_milestones, batch_size=batch_size,\n",
        "                                      weight_decay=weight_decay, device=device, n_jobs_dataloader=n_jobs_dataloader))\n",
        "    print(\"t\",len(self.trainer))\n",
        "\n",
        "  def pretrain(self, dataset: BaseADDataset, valid_dataset: BaseADDataset, optimizer_name: str = 'adam', lr: float = 0.1, n_epochs: int = 100,\n",
        "              lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "              n_jobs_dataloader: int = 0,load=False):\n",
        "    \"\"\"Pretrains the weights for the Deep SVDD network \\phi via autoencoder.\"\"\"\n",
        "\n",
        "    self.ae_net =My_LeNet_Autoencoder()\n",
        "    self.ae_optimizer_name = optimizer_name\n",
        "    self.ae_trainer = AETrainer(optimizer_name, lr=lr, n_epochs=n_epochs, lr_milestones=lr_milestones,\n",
        "                                batch_size=batch_size, weight_decay=weight_decay, device=device,\n",
        "                                n_jobs_dataloader=n_jobs_dataloader)\n",
        "    if load :\n",
        "      self.ae_net ,valid= self.ae_trainer.load_and_continue(dataset,valid_dataset, self.ae_net)\n",
        "    else:\n",
        "      self.ae_net ,valid= self.ae_trainer.train(dataset,valid_dataset, self.ae_net)\n",
        "    self.ae_trainer.test(dataset, self.ae_net)\n",
        "    self.pre_trained=True\n",
        "    self.save_model('/content/gdrive/MyDrive/ArshadPeoject/model/base-auto-encoder.dict')\n",
        "    return valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4MZCofipGif"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U71lLFnztzOd"
      },
      "source": [
        "# K-means\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_7xDSYX679z",
        "outputId": "0c7bcbb9-62db-464c-91b7-3106d355c778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4 5 6 7]\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "import numpy as np\n",
        "outs =np.array([1,1,2,0,0,1])\n",
        "# kmeans = KMeans(n_clusters=3)\n",
        "# s=kmeans.fit_predict(outs)\n",
        "# centers=kmeans.cluster_centers_\n",
        "\n",
        "r= np.arange(1,8)\n",
        "print(print(r))\n",
        "\n",
        "# print(kmeans.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywTR4_Gprd2x",
        "outputId": "c5385b7b-05ec-47e3-8b37-43775e6e60b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "l 6\n",
            "f 6\n",
            "ty 5\n",
            "t 5\n",
            "t 5\n",
            "testing net: 1\n",
            "Starting testing...\n",
            "Testing time: 149.068\n",
            "Test set AUC: 86.95%\n",
            "Finished testing.\n",
            "testing net: 2\n",
            "Starting testing...\n",
            "Testing time: 147.638\n",
            "Test set AUC: 82.89%\n",
            "Finished testing.\n",
            "testing net: 3\n",
            "Starting testing...\n",
            "Testing time: 147.550\n",
            "Test set AUC: 90.01%\n",
            "Finished testing.\n",
            "testing net: 4\n",
            "Starting testing...\n",
            "Testing time: 148.370\n",
            "Test set AUC: 86.52%\n",
            "Finished testing.\n",
            "testing net: 5\n",
            "Starting testing...\n",
            "Testing time: 150.243\n",
            "Test set AUC: 89.11%\n",
            "Finished testing.\n",
            "tensor([9.5042e+00, 6.5290e+00, 6.7175e+00,  ..., 5.7447e+01, 7.6326e+03,\n",
            "        3.7389e+01], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# def main(dataset_name, net_name, xp_path, data_path, load_config, load_model, objective, nu, device, seed,\n",
        "#          optimizer_name, lr, n_epochs, lr_milestone, batch_size, weight_decay, pretrain, ae_optimizer_name, ae_lr,\n",
        "#          ae_n_epochs, ae_lr_milestone, ae_batch_size, ae_weight_decay, n_jobs_dataloader, normal_class):\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Deep SVDD, a fully deep method for anomaly detection.\n",
        "    :arg DATASET_NAME: Name of the dataset to load.\n",
        "    :arg NET_NAME: Name of the neural network to use.\n",
        "    :arg XP_PATH: Export path for logging the experiment.\n",
        "    :arg DATA_PATH: Root path of data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get configuration\n",
        "    # cfg = Config(locals().copy())\n",
        "\n",
        "    # Set up logging\n",
        "    # logging.basicConfig(level=logging.INFO)\n",
        "    # logger = logging.getLogger()\n",
        "    # logger.setLevel(logging.INFO)\n",
        "    # formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "    # log_file = xp_path + '/log.txt'\n",
        "    # file_handler = logging.FileHandler(log_file)\n",
        "    # file_handler.setLevel(logging.INFO)\n",
        "    # file_handler.setFormatter(formatter)\n",
        "    # logger.addHandler(file_handler)\n",
        "\n",
        "    # Print arguments\n",
        "    # logger.info('Log file is %s.' % log_file)\n",
        "    # logger.info('Data path is %s.' % data_path)\n",
        "    # logger.info('Export path is %s.' % xp_path)\n",
        "\n",
        "\n",
        "    # logger.info('Normal class: %d' % normal_class)\n",
        "    # logger.info('Network: %s' % net_name)\n",
        "\n",
        "    # If specified, load experiment config from JSON-file\n",
        "    # if load_config:\n",
        "    #     cfg.load_config(import_json=load_config)\n",
        "        # logger.info('Loaded configuration from %s.' % load_config)\n",
        "\n",
        "    # Print configuration\n",
        "    # logger.info('Deep SVDD objective: %s' % cfg.settings['objective'])\n",
        "    # logger.info('Nu-paramerter: %.2f' % cfg.settings['nu'])\n",
        "\n",
        "    # Set seed\n",
        "    # if cfg.settings['seed'] != -1:\n",
        "    #     random.seed(cfg.settings['seed'])\n",
        "    #     np.random.seed(cfg.settings['seed'])\n",
        "    #     torch.manual_seed(cfg.settings['seed'])\n",
        "        # logger.info('Set seed to %d.' % cfg.settings['seed'])\n",
        "\n",
        "    # Default device to 'cpu' if cuda is not available\n",
        "    device = 'cuda'\n",
        "    if not torch.cuda.is_available():\n",
        "        device = 'cpu'\n",
        "    # logger.info('Computation device: %s' % device)\n",
        "    # logger.info('Number of dataloader workers: %d' % n_jobs_dataloader)\n",
        "\n",
        "    # Load data\n",
        "    dataset_p=My_Dataset_preTrain(root=\"ll\")\n",
        "    dataset_vali =My_Dataset_cross(root=\"ll\")\n",
        "    # dataset =My_Dataset(root=\"ll\")\n",
        "\n",
        "    # Initialize DeepSVDD model and set neural network \\phi\n",
        "    deep_SVDD = MSVDD('one-class',0.1)\n",
        "    # deep_SVDD.set_network(\"lll\")\n",
        "    # If specified, load Deep SVDD model (radius R, center c, network weights, and possibly autoencoder weights)\n",
        "    # if load_model:\n",
        "    #     deep_SVDD.load_model(model_path=load_model, load_ae=True)\n",
        "        # logger.info('Loading model from %s.' % load_model)\n",
        "\n",
        "    # logger.info('Pretraining: %s' % pretrain)\n",
        "    pretrain=True;\n",
        "    if pretrain:\n",
        "        # Log pretraining details\n",
        "        # logger.info('Pretraining optimizer: %s' % cfg.settings['ae_optimizer_name'])\n",
        "        # logger.info('Pretraining learning rate: %g' % cfg.settings['ae_lr'])\n",
        "        # logger.info('Pretraining epochs: %d' % cfg.settings['ae_n_epochs'])\n",
        "        # logger.info('Pretraining learning rate scheduler milestones: %s' % (cfg.settings['ae_lr_milestone'],))\n",
        "        # logger.info('Pretraining batch size: %d' % cfg.settings['ae_batch_size'])\n",
        "        # logger.info('Pretraining weight decay: %g' % cfg.settings['ae_weight_decay'])\n",
        "\n",
        "        # Pretrain model on dataset (via autoencoder)\n",
        "        valid_pret=None\n",
        "        # valid_pret = deep_SVDD.pretrain(dataset_p, dataset_vali,device=device,load=True,n_epochs=148,lr_milestones=[70, 90] )\n",
        "        # import numpy as np\n",
        "        # import matplotlib.pyplot as plt\n",
        "        # from scipy.stats import norm\n",
        "        # import statistics\n",
        "        # from matplotlib.pyplot import figure\n",
        "        # from sklearn import preprocessing\n",
        "        # figure(figsize=(30, 20), dpi=150)\n",
        "\n",
        "        # plt.plot(range(0, len(valid_pret['loss']) ),valid_pret['loss'])\n",
        "        # plt.plot(range(0, len(valid_pret['valid']) ),valid_pret['valid'])\n",
        "        # plt.show()\n",
        "        deep_SVDD.load()\n",
        "        # ,\n",
        "        #                    optimizer_name=cfg.settings['ae_optimizer_name'],\n",
        "        #                    lr=cfg.settings['ae_lr'],\n",
        "        #                    n_epochs=cfg.settings['ae_n_epochs'],\n",
        "        #                    lr_milestones=cfg.settings['ae_lr_milestone'],\n",
        "        #                    batch_size=cfg.settings['ae_batch_size'],\n",
        "        #                    weight_decay=cfg.settings['ae_weight_decay'],\n",
        "        #                    device=device,\n",
        "        #                    n_jobs_dataloader=n_jobs_dataloader)\n",
        "\n",
        "    # Log training details\n",
        "    # logger.info('Training optimizer: %s' % cfg.settings['optimizer_name'])\n",
        "    # logger.info('Training learning rate: %g' % cfg.settings['lr'])\n",
        "    # logger.info('Training epochs: %d' % cfg.settings['n_epochs'])\n",
        "    # logger.info('Training learning rate scheduler milestones: %s' % (cfg.settings['lr_milestone'],))\n",
        "    # logger.info('Training batch size: %d' % cfg.settings['batch_size'])\n",
        "    # logger.info('Training weight decay: %g' % cfg.settings['weight_decay'])\n",
        "\n",
        "    # Train model on dataset,n_epoch\n",
        "    print(device)\n",
        "    valid_t=deep_SVDD.train(dataset_p,dataset_vali,5,device=device,lr_milestones=[100],load=True, resume=True,n_epochs=200, lr=0.1)\n",
        "    # deep_SVDD.load_clf(1,device=device,lr_milestones=[40],n_epochs=100, lr=0.1)\n",
        "   \n",
        "    # ,\n",
        "    #                 optimizer_name='adam',\n",
        "    #                 lr=cfg.settings['lr'],\n",
        "    #                 n_epochs=cfg.settings['n_epochs'],\n",
        "    #                 lr_milestones=cfg.settings['lr_milestone'],\n",
        "    #                 batch_size=cfg.settings['batch_size'],\n",
        "    #                 weight_decay=cfg.settings['weight_decay'],\n",
        "    #                 device=device,\n",
        "    #                 n_jobs_dataloader=n_jobs_dataloader)\n",
        "    test_dataset=My_Dataset_Test(root='ll')\n",
        "    # Test model\n",
        "    indices, labels, scores = deep_SVDD.test(test_dataset)\n",
        "    print(scores)\n",
        "    # Plot most anomalous and most normal (within-class) test samples\n",
        "    # inindicesdices, labels, scores, outputs= zip(*deep_SVDD.results['test_scores'])\n",
        "    # , labels, scores = np.array(indices), np.array(labels), np.array(scores)\n",
        "    idx_sorted = indices[labels == 0][np.argsort(scores[labels == 0])]  # sorted from lowest to highest anomaly score\n",
        "    return deep_SVDD, dataset_p, indices, labels, scores ,valid_pret\n",
        "    # if di.\n",
        "\n",
        "svdd, dataset, indices, labels, scores, v_pt= main()\n",
        "\n",
        "# @click.argument('dataset_name', type=click.Choice(['mnist', 'cifar10']))\n",
        "# @click.argument('net_name', type=click.Choice(['mnist_LeNet', 'cifar10_LeNet', 'cifar10_LeNet_ELU']))\n",
        "# @click.argument('xp_path', type=click.Path(exists=True))\n",
        "# @click.argument('data_path', type=click.Path(exists=True))\n",
        "# @click.option('--load_config', type=click.Path(exists=True), default=None,\n",
        "#               help='Config JSON-file path (default: None).')\n",
        "# @click.option('--load_model', type=click.Path(exists=True), default=None,\n",
        "#               help='Model file path (default: None).')\n",
        "# @click.option('--objective', type=click.Choice(['one-class', 'soft-boundary']), default='one-class',\n",
        "#               help='Specify Deep SVDD objective (\"one-class\" or \"soft-boundary\").')\n",
        "# @click.option('--nu', type=float, default=0.1, help='Deep SVDD hyperparameter nu (must be 0 < nu <= 1).')\n",
        "# @click.option('--device', type=str, default='cuda', help='Computation device to use (\"cpu\", \"cuda\", \"cuda:2\", etc.).')\n",
        "# @click.option('--seed', type=int, default=-1, help='Set seed. If -1, use randomization.')\n",
        "# @click.option('--optimizer_name', type=click.Choice(['adam', 'amsgrad']), default='adam',\n",
        "#               help='Name of the optimizer to use for Deep SVDD network training.')\n",
        "# @click.option('--lr', type=float, default=0.001,\n",
        "#               help='Initial learning rate for Deep SVDD network training. Default=0.001')\n",
        "# @click.option('--n_epochs', type=int, default=50, help='Number of epochs to train.')\n",
        "# @click.option('--lr_milestone', type=int, default=0, multiple=True,\n",
        "#               help='Lr scheduler milestones at which lr is multiplied by 0.1. Can be multiple and must be increasing.')\n",
        "# @click.option('--batch_size', type=int, default=128, help='Batch size for mini-batch training.')\n",
        "# @click.option('--weight_decay', type=float, default=1e-6,\n",
        "#               help='Weight decay (L2 penalty) hyperparameter for Deep SVDD objective.')\n",
        "# @click.option('--pretrain', type=bool, default=True,\n",
        "#               help='Pretrain neural network parameters via autoencoder.')\n",
        "# @click.option('--ae_optimizer_name', type=click.Choice(['adam', 'amsgrad']), default='adam',\n",
        "#               help='Name of the optimizer to use for autoencoder pretraining.')\n",
        "# @click.option('--ae_lr', type=float, default=0.001,\n",
        "#               help='Initial learning rate for autoencoder pretraining. Default=0.001')\n",
        "# @click.option('--ae_n_epochs', type=int, default=100, help='Number of epochs to train autoencoder.')\n",
        "# @click.option('--ae_lr_milestone', type=int, default=0, multiple=True,\n",
        "#               help='Lr scheduler milestones at which lr is multiplied by 0.1. Can be multiple and must be increasing.')\n",
        "# @click.option('--ae_batch_size', type=int, default=128, help='Batch size for mini-batch autoencoder training.')\n",
        "# @click.option('--ae_weight_decay', type=float, default=1e-6,\n",
        "#               help='Weight decay (L2 penalty) hyperparameter for autoencoder objective.')\n",
        "# @click.option('--n_jobs_dataloader', type=int, default=0,\n",
        "#               help='Number of workers for data loading. 0 means that the data will be loaded in the main process.')\n",
        "# @click.option('--normal_class', type=int, default=0,\n",
        "#               help='Specify the normal class of the dataset (all other classes are considered anomalous).')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Dxa4bhhPQs"
      },
      "outputs": [],
      "source": [
        "# v_pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJcNZ8-UwSqM"
      },
      "outputs": [],
      "source": [
        "# v_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiE9ngv_efqd"
      },
      "outputs": [],
      "source": [
        "# indices, labels, scores\n",
        "# indices# print(np.argsort(scores[labels == 0]))\n",
        "\n",
        "\n",
        "# idx_sorted = indices[labels == 0][np.argsort(scores[labels == 0])]  # sorted from lowest to highest anomaly score\n",
        "# print(len(idx_sorted))\n",
        "\n",
        "# print(dataset.test_set.data)\n",
        "# X_normals = vertical_concat.iloc[idx_sorted[:20]]\n",
        "# X_outliers = vertical_concat.iloc[idx_sorted[-20:]]\n",
        "# xp_path=\"/content/gdrive/MyDrive/ArshadPeoject\"\n",
        "# plot_images_grid(torch.tensor(X_normals.values), export_img=xp_path + '/normals', title='Most normal examples', padding=2)\n",
        "# plot_images_grid(torch.tensor(X_outliers.values), export_img=xp_path + '/outliers', title='Most anomalous examples', padding=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74lyQ02o8ivW"
      },
      "outputs": [],
      "source": [
        "# scores[labels == 0][np.argsort(scores[labels == 0])] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J-wOLAsDsx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7154d0b8-2d7d-40cd-fb0b-f71c2c5c2f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing net: 1\n",
            "Starting testing...\n",
            "Testing time: 38.001\n",
            "Test set AUC: 86.57%\n",
            "Finished testing.\n",
            "testing net: 2\n",
            "Starting testing...\n",
            "Testing time: 37.733\n",
            "Test set AUC: 82.91%\n",
            "Finished testing.\n",
            "testing net: 3\n",
            "Starting testing...\n",
            "Testing time: 37.987\n",
            "Test set AUC: 90.09%\n",
            "Finished testing.\n",
            "testing net: 4\n",
            "Starting testing...\n",
            "Testing time: 37.758\n",
            "Test set AUC: 86.20%\n",
            "Finished testing.\n",
            "testing net: 5\n",
            "Starting testing...\n",
            "Testing time: 37.573\n",
            "Test set AUC: 88.97%\n",
            "Finished testing.\n"
          ]
        }
      ],
      "source": [
        "ddd=My_Dataset(root=\"ll\")\n",
        "\n",
        "indices_v, labels_v, scores_v, = svdd.test(ddd)\n",
        "indices_v, labels_v, scores_v = np.array(indices_v), np.array(labels_v), np.array(scores_v)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8_2xdBdwx63"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import statistics\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# figure(figsize=(15, 10), dpi=100)\n",
        "# xxx=np.arange(0,100, 10)\n",
        "\n",
        "# plt.plot(xxx,v_t[4]['loss'])\n",
        "# plt.plot(xxx,v_t[4]['valid'])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tElYV55p8YT6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "outputId": "064e24d1-9a73-4049-adb2-adde6753805d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   3.7710855     4.41453362    5.15624857 ... 1563.82006836 1935.26794434\n",
            " 2861.99121094]\n",
            "[5.67413998e+00 5.67413998e+00 5.67460442e+00 ... 6.40662000e+05\n",
            " 6.40662000e+05 6.40662000e+05]\n",
            "18.23567191193501\n",
            "56.277381416568936\n",
            "30119.895128907912\n",
            "78390.93199603751\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "\n",
        "\n",
        "figure(figsize=(15, 10), dpi=100)\n",
        "\n",
        "# Plot between -10 an0.1d 10 with .001 steps.\n",
        "xxx=np.arange(0, 1000, 0.01)\n",
        "ww=scores_v[labels_v == 0][np.argsort(scores_v[labels_v == 0])]\n",
        "\n",
        "x_axis =ww\n",
        "qq=scores_v[labels_v == 1][np.argsort(scores_v[labels_v == 1])]\n",
        "x2_axis = qq\n",
        "print(x_axis)\n",
        "print(x2_axis)\n",
        "# Calculating mean and standard deviation\n",
        "mean = statistics.mean(x_axis)\n",
        "sd = statistics.stdev(x_axis)\n",
        "mean2 = statistics.mean(x2_axis)\n",
        "sd2 = statistics.stdev(x2_axis)\n",
        "print(mean)\n",
        "print(sd)\n",
        "print(mean2)\n",
        "print(sd2)\n",
        "plt.plot(xxx, (norm.pdf(xxx, mean, sd)))\n",
        "plt.plot(xxx, norm.pdf(xxx, mean2, sd2))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA3_t3Hl4tsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16103d7-f6a8-4893-ffb9-d56ba6b65ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 233.44564939 -197.00533378]\n"
          ]
        }
      ],
      "source": [
        "def solve(m1,m2,std1,std2):\n",
        "  a = 1/(2*std1**2) - 1/(2*std2**2)\n",
        "  b = m2/(std2**2) - m1/(std1**2)\n",
        "  c = m1**2 /(2*std1**2) - m2**2 / (2*std2**2) - np.log(std2/std1)\n",
        "  return np.roots([a,b,c])\n",
        "\n",
        "\n",
        "result = solve(mean,mean2,sd,sd2)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co2ZQjfZZtat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a58b1e5b-a65a-4940-ac7f-f90d602e47b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9444105070250458 0.6267991080478411\n",
            "0.753503107103692\n"
          ]
        }
      ],
      "source": [
        "tp=0\n",
        "tn=0\n",
        "fp=0\n",
        "fn=0\n",
        "alpha=1.5\n",
        "for i in indices_v:\n",
        "  if scores_v[i]<mean +alpha*sd and scores_v[i]>mean-alpha*sd:\n",
        "    if labels_v[i]==1:\n",
        "      fn+=1\n",
        "    else:\n",
        "      tn+=1\n",
        "  else:\n",
        "    if labels_v[i]==1:\n",
        "      tp+=1\n",
        "    else:\n",
        "      fp+=1\n",
        "  # if labels_v[i]==0:\n",
        "  #   if normal_min>=scores[i]:\n",
        "  #     normal_min=scores[i]\n",
        "    \n",
        "  #   if normal_max<=scores[i]:\n",
        "  #       normal_max=scores[i]\n",
        "  # else:\n",
        "     \n",
        "  #   if anomal_min>=scores[i]:\n",
        "  #     anomal_min=scores[i]\n",
        "    \n",
        "  #   if anomal_max<=scores[i]:\n",
        "  #     anomal_max=scores[i]\n",
        "precision = tp/(tp+fp)\n",
        "recall= tp/(tp+fn)\n",
        "print(precision, recall)\n",
        "f1_score = 2*precision*recall/(precision+recall)\n",
        "print(f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ_gzHLfoGpE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_celNYZmz5pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "347b9248-b023-4b48-8b26-9308fbb15534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14975 9972 18828 4760\n",
            "0.4430080170399077 0.7588041550544717\n",
            "0.5594157420897307\n",
            "========\n",
            "0.7588041550544717\n",
            "0.65375\n",
            "0.34624999999999995\n",
            "0.5140002060368806\n"
          ]
        }
      ],
      "source": [
        "normal_min=100\n",
        "anomal_min=100\n",
        "normal_max=-100\n",
        "anomal_max=-100\n",
        "tp=0\n",
        "tn=0\n",
        "fp=0\n",
        "fn=0\n",
        "alpha=0.2\n",
        "for i in indices:\n",
        "  if scores[i]<mean +alpha*sd and scores[i]>mean-alpha*sd:\n",
        "    if labels[i]==1:\n",
        "      fn+=1\n",
        "    else:\n",
        "      tn+=1\n",
        "  else:\n",
        "    if labels[i]==1:\n",
        "      tp+=1\n",
        "    else:\n",
        "      fp+=1\n",
        "\n",
        "print(tp, tn ,fp, fn)\n",
        "precision = tp/(tp+fp)\n",
        "recall= tp/(tp+fn)\n",
        "print(precision, recall)\n",
        "f1_score = 2*precision*recall/(precision+recall)\n",
        "print(f1_score)\n",
        "\n",
        "\n",
        "\n",
        "tpr = tp /(tp+fn)\n",
        "fpr = fp /(fp+tn)\n",
        "speci = 1-fpr\n",
        "acc =  (tp+tn)/(tp+tn+fp+fn)\n",
        "print(\"========\")\n",
        "print(tpr)\n",
        "print(fpr)\n",
        "print(speci)\n",
        "print(acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "\n",
        "fpr, tpr, thresh= roc_curve(labels_v, scores_v)\n",
        "pyplot.plot(fpr, tpr, linestyle='--', label='No Skill')\n",
        "roc_auc_score(labels_v, scores_v)"
      ],
      "metadata": {
        "id": "NASZxCJ9T7_s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d3f2b09d-601c-41be-c999-ab60920d3ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9008796906322499"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(\n",
        "#     {\n",
        "#       'test_1': {\n",
        "#           'desc': '520*520*1, K=5, ',\n",
        "#           'lables': labels_v,\n",
        "#           'scores': scores_v\n",
        "#           }\n",
        "#     }\n",
        "# ,'/content/gdrive/MyDrive/ArshadPeoject/result/res.dict')"
      ],
      "metadata": {
        "id": "HMCRsouuT74i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y33ia_Z71svs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c679a935-c1fd-4e2e-f7f4-15dd889db0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normal_min = 100\n",
            "anomal_min = 100\n",
            "normal_max = -100\n",
            "anomal_max = -100\n"
          ]
        }
      ],
      "source": [
        "print(\"normal_min =\",normal_min)\n",
        "print(\"anomal_min =\",anomal_min)\n",
        "print(\"normal_max =\",normal_max)\n",
        "print(\"anomal_max =\",anomal_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQi80lc4AW2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "35563385-d2e3-4de5-9827-4c24b0fc321d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4a4d86b150>]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(xxx, norm.pdf(xxx, mean2, sd2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_uzFTyq4vom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c573278c-cae7-4a8a-ec9c-4e7216bf5ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 23162\n",
            "My_LeNet(\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (pool5n5): MaxPool2d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv1): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2d1): BatchNorm2d(2, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
            "  (conv2): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2d2): BatchNorm2d(4, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
            "  (conv3): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2d3): BatchNorm2d(8, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
            "  (conv4): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2d4): BatchNorm2d(16, eps=0.0001, momentum=0.1, affine=False, track_running_stats=True)\n",
            "  (fc0): Linear(in_features=2704, out_features=8, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "net_parameters = filter(lambda p: p.requires_grad, svdd.net.parameters())\n",
        "params = sum([np.prod(p.size()) for p in net_parameters])\n",
        "print('Trainable parameters: {}'.format(params))\n",
        "print(svdd.net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRT3ZoPV4Xd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "1e9909ef-710e-45bc-8c5d-cc970bb32911"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3a162569dd2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mMy_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ll\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvertical_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manomal_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"col\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('classic')\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "dataset =My_Dataset(root=\"ll\")\n",
        "vertical_concat = pd.concat([dataset.test_set.data, dataset.test_set.anomal_data], axis=0,ignore_index=True)\n",
        "\n",
        "cols=[\"col\"]*200\n",
        "for i in range(0,200):\n",
        "  cols[i]=\"col-\"+str(i)\n",
        "\n",
        "# vv= vertical_concat.iloc[:,0:20]\n",
        "# vv=pd.concat([vv,lbldf], axis=1)\n",
        "# vv= vv.drop_duplicates()\n",
        "# vv = pd.concat([vv, lbldf], axis=1)\n",
        "print(vertical_concat)\n",
        "\n",
        "\n",
        "\n",
        "pp = sns.pairplot(vertical_concat, vars=cols,hue='label',palette=['red','blue'])\n",
        "\n",
        "# fig = pp.fig \n",
        "# fig.subplots_adjust(top=0.93, wspace=0.3)\n",
        "# t = fig.suptitle('Wine Attributes Pairwise Plots', fontsize=14)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of DrivationTree_(1)_ final.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}