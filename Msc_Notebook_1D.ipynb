{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOflCXiNdbfj"
      },
      "source": [
        "# Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_jmWZwoBGJ_",
        "outputId": "3cdc788b-cad4-43ae-8135-754051213043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting antlr4-python3-runtime\n",
            "  Downloading antlr4-python3-runtime-4.10.tar.gz (116 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▉                             | 10 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 30 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 51 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 116 kB 5.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.10-py3-none-any.whl size=144171 sha256=edfa1adcedcfab467d36c207b3acc78d3a3f40979db4725eb90cbd2c1801a037\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/3d/4d/b5f9dab12f8e1e752959553da73f44289afa5c2dd47fa377d7\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime\n",
            "Successfully installed antlr4-python3-runtime-4.10\n"
          ]
        }
      ],
      "source": [
        "pip install antlr4-python3-runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSl9dxYAXSXJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKW4FqB8ys5p",
        "outputId": "81c5403a-9c29-42e3-bf37-900ccf17bd47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/ArshadPeoject')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4UJaLqq0Kw3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "91197308-5cd9-4f70-86e1-202fc3596436"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a54eac0bda7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mantlr4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTerminalNodeImpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from my_parser.antlr.query.QueryLexer import QueryLexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryParser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQueryParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmy_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryVisitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQueryVisitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/ArshadPeoject/my_parser/antlr/query/QueryParser.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mQueryParser\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mParser\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mgrammarFileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Query.g4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/ArshadPeoject/my_parser/antlr/query/QueryParser.py\u001b[0m in \u001b[0;36mQueryParser\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mgrammarFileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Query.g4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0matn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mATNDeserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserializedATN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mdecisionsToDFA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mDFA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecisionToState\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/antlr4/atn/ATNDeserializer.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0matn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadATN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/antlr4/atn/ATNDeserializer.py\u001b[0m in \u001b[0;36mcheckVersion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mSERIALIZED_VERSION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not deserialize ATN with version \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" (expected \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSERIALIZED_VERSION\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\").\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadATN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Could not deserialize ATN with version \u0003 (expected 4)."
          ]
        }
      ],
      "source": [
        "from antlr4 import *\n",
        "from antlr4.tree.Tree import TerminalNodeImpl\n",
        "from my_parser.antlr.query.QueryLexer import QueryLexer\n",
        "from my_parser.antlr.query.QueryParser import QueryParser\n",
        "from my_parser.antlr.query.QueryVisitor import QueryVisitor\n",
        "\n",
        "from my_parser.antlr.path.PathLexer import PathLexer\n",
        "from my_parser.antlr.path.PathParser import PathParser\n",
        "from my_parser.antlr.path.PathVisitor import PathVisitor\n",
        "\n",
        "import re\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95oiJBDtWFt1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqx33-HMaQEZ"
      },
      "outputs": [],
      "source": [
        "# class MyParser(HttpGrammarParser):\n",
        "#   def accept(self, visitor:ParseTreeVisitor):\n",
        "#     print(\"a - \" , visitor.getRuleIndex())\n",
        "#     return super().accept(visitor)\n",
        "\n",
        "# class MyVisitor(HttpGrammarVisitor):\n",
        "#   def decompose(self, root):\n",
        "#     pass\n",
        "\n",
        "#   def visit(self, tree):\n",
        "#     print(\"v -\" , tree.getRuleIndex())\n",
        "#     return super().visit(tree)\n",
        "\n",
        " \n",
        "          \n",
        "#   def visitChildren(self, ctx):\n",
        "#     # print(\"ss\")\n",
        "#     # ctx.getLeaf()\n",
        "#     print(\"child -\" , ctx.getRuleIndex())\n",
        "#     print(\"c -\" ,type(ctx))\n",
        "\n",
        "#     ss = super().visitChildren(ctx)\n",
        "#     # print(ss)\n",
        "#     return ss\n",
        "  \n",
        "#     # def cus_visit(node,  root , S):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRxPsnq-diMa"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, name, parent=None):\n",
        "        self.name = name\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        # ...\n",
        "\n",
        "        if parent:\n",
        "            self.parent.children.append(self)\n",
        "\n",
        "def nodize(tree, p=None):\n",
        "  \n",
        "  if not isinstance(tree, TerminalNodeImpl):\n",
        "    root = Node(\"<\" + str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\") + \">\", p)\n",
        "    for i in tree.getChildren():\n",
        "      nodize(i,root)\n",
        "  else:\n",
        "    root = Node(\"'\" +str(tree)+ \"'\", p)\n",
        "  return root\n",
        "\n",
        "\n",
        "def print_tree(current_node, indent=\"\", last='updown'):\n",
        "\n",
        "    nb_children = lambda node: sum(nb_children(child) for child in node.children) + 1\n",
        "    size_branch = {child: nb_children(child) for child in current_node.children}\n",
        "\n",
        "    \"\"\" Creation of balanced lists for \"up\" branch and \"down\" branch. \"\"\"\n",
        "    up = current_node.children\n",
        "    down = []\n",
        "    while up and sum(size_branch[node] for node in down) < sum(size_branch[node] for node in up):\n",
        "        down.append(up.pop())\n",
        "\n",
        "    \"\"\" Printing of \"up\" branch. \"\"\"\n",
        "    for child in up:     \n",
        "        next_last = 'up' if up.index(child) is 0 else ''\n",
        "        next_indent = '{0}{1}{2}'.format(indent, ' ' if 'up' in last else '│', \" \" * len(current_node.name))\n",
        "        print_tree(child, indent=next_indent, last=next_last)\n",
        "\n",
        "    \"\"\" Printing of current node. \"\"\"\n",
        "    if last == 'up': start_shape = '┌'\n",
        "    elif last == 'down': start_shape = '└'\n",
        "    elif last == 'updown': start_shape = ' '\n",
        "    else: start_shape = '├'\n",
        "\n",
        "    if up: end_shape = '┤'\n",
        "    elif down: end_shape = '┐'\n",
        "    else: end_shape = ''\n",
        "\n",
        "    print ('{0}{1}{2}{3}'.format(indent, start_shape, current_node.name, end_shape))\n",
        "\n",
        "    \"\"\" Printing of \"down\" branch. \"\"\"\n",
        "    for child in down:\n",
        "        next_last = 'down' if down.index(child) is len(down) - 1 else ''\n",
        "        next_indent = '{0}{1}{2}'.format(indent, ' ' if 'down' in last else '│', \" \" * len(current_node.name))\n",
        "        print_tree(child, indent=next_indent, last=next_last)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHUrGtI_g3FH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4E7l7Xag4iL"
      },
      "outputs": [],
      "source": [
        "def getLeaf(root,prev):\n",
        "  leaf=set()\n",
        "  if not isinstance(root, TerminalNodeImpl):\n",
        "    for i in root.getChildren():\n",
        "      leaf.update(getLeaf(i,root))\n",
        "  else:\n",
        "    leaf.add(str(prev).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\"))\n",
        "  return leaf\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGoO5T6reV3"
      },
      "source": [
        "vector is : [path_vector].concat([query_Vector])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdyfhvLcrGsf"
      },
      "outputs": [],
      "source": [
        "def decomposeTree(root):\n",
        "  S=set()\n",
        "  children=root.getChildren()\n",
        "  for child in children:\n",
        "    visit(child,root,S)\n",
        "  return S\n",
        "\n",
        "def visit(node,root,S):\n",
        "  rootLeafs= getLeaf(root)\n",
        "  childLeafs = getLeaf(node)\n",
        "  diff=rootLeafs.difference(childLeafs)\n",
        "  extr=False\n",
        "  if not isinstance(node, TerminalNodeImpl):\n",
        "    children=node.getChildren()\n",
        "    for child in children:\n",
        "      extr=visit(child,node,S)\n",
        "  if len(diff)>0 and not extr:\n",
        "    S.add(\"<\" + str(node).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\") + \">\")\n",
        "    extr=True\n",
        "  return extr\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4azhS4CKgvO"
      },
      "outputs": [],
      "source": [
        "def getStates0(tree, root, S):\n",
        "   rootLeafs= getLeaf(root,None)\n",
        "  #  print(rootLeafs)\n",
        "   childLeafs = getLeaf(tree,None)\n",
        "  #  print(childLeafs)\n",
        "   diff=rootLeafs.difference(childLeafs)\n",
        "  #  print(diff)\n",
        "  #  print(len(diff))\n",
        "   extr=False\n",
        "   if not isinstance(tree, TerminalNodeImpl): \n",
        "     state = str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\")      #  print(state)\n",
        "     if len(diff)>0 and not extr:\n",
        "      # print(state , \" | \", tree.getText())\n",
        "      txt =tree.getText();\n",
        "      nsc=len(re.sub('[\\w]+' ,'', txt))\n",
        "      lav=len(txt)\n",
        "      rml=4096\n",
        "      t=S[1][int(state)]*S[0][int(state)]\n",
        "      S[0][int(state)]+=1\n",
        "      S[1][int(state)]=( t + int((1+(nsc/lav))*rml) + lav)/S[0][int(state)]\n",
        "      \n",
        "      extr=True\n",
        "     ch=tree.getChildren()\n",
        "     prev=tree\n",
        "     for c in ch:\n",
        "       getStates0(c,prev, S)\n",
        "       prev=c  \n",
        "\n",
        "# def getStates(dfa_states, s):\n",
        "#   for k in dfa_states:\n",
        "#     if k!=None and not k.stateNumber in s:\n",
        "#       s.add(k.stateNumber)\n",
        "#       if k.edges!=None:\n",
        "#           getStates(k.edges,s)\n",
        "  \n",
        "def getStates(tree, S):\n",
        "  #  S.add(\"<\" + str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\") + \">\")\n",
        "   if not isinstance(tree, TerminalNodeImpl): \n",
        "     state = str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\")\n",
        "    #  if state!='':\n",
        "    #   #  print(state)\n",
        "    #    S[int(state)]+=1\n",
        "     ch=tree.getChildren()\n",
        "     prev=tree\n",
        "     for c in ch:\n",
        "       getStates0(c,prev, S)\n",
        "       prev=c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEcsif87WzpR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "x = \"asdfkls2df#$&rwefe^ef#wef@!\"\n",
        "new = len(re.sub('[\\w]+' ,'', x))\n",
        "print(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3aXE6kkKYss"
      },
      "outputs": [],
      "source": [
        "# def getStates0(tree, root, S):\n",
        "#   #  rootLeafs= getLeaf(root)\n",
        "#   #  childLeafs = getLeaf(tree)\n",
        "#   #  diff=rootLeafs.difference(childLeafs)\n",
        "#    extr=False\n",
        "#    if not isinstance(tree, TerminalNodeImpl): \n",
        "#      state = str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\")\n",
        "#       #  print(state)\n",
        "#      ch=tree.getChildren()\n",
        "#      prev=tree\n",
        "#      for c in ch:\n",
        "#        extr=getStates0(c,prev, S)\n",
        "#        prev=c  \n",
        "#     #  if len(diff)>0 and not extr:\n",
        "#     #  print(state)\n",
        "#      print(state , \" | \", tree.getText())\n",
        "#      S[int(state)]+=1\n",
        "#       #  extr=True\n",
        "#    return extr\n",
        "\n",
        "\n",
        "# # def getStates(dfa_states, s):\n",
        "# #   for k in dfa_states:\n",
        "# #     if k!=None and not k.stateNumber in s:\n",
        "# #       s.add(k.stateNumber)\n",
        "# #       if k.edges!=None:\n",
        "# #           getStates(k.edges,s)\n",
        "  \n",
        "# def getStates(tree, S):\n",
        "#   #  S.add(\"<\" + str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\") + \">\")\n",
        "#    if not isinstance(tree, TerminalNodeImpl): \n",
        "\n",
        "#      state = str(tree).split(' ')[0].replace(\"[\",\"\").replace(\"]\",\"\")\n",
        "#      print(tree.getText())\n",
        "#      if state!='':\n",
        "#       #  print(state)\n",
        "#        print(state , \" | \", tree.getText())\n",
        "#        S[int(state)]+=1\n",
        "#      ch=tree.getChildren()\n",
        "#      for c in ch:\n",
        "#        getStates0(c,tree, S)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utt5GAD7-ZRP"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZu_ouwkBR54"
      },
      "outputs": [],
      "source": [
        "# \n",
        "url = \"http://localhost:8080/auth/code.jsp?state=AB2e&token=a2ef%20OR%201=1\".lower()\n",
        "\n",
        "path_start=url.index(\"/\", url.index(\"/\")+2)\n",
        "try:\n",
        "  path_end=url.index(\"?\")\n",
        "except:\n",
        "  path_end=len(url)\n",
        "\n",
        "basic = url[:path_start]\n",
        "path = url[path_start+1:path_end]\n",
        "query = url[path_end+1:]\n",
        "print(path)\n",
        "q_data = InputStream(query)\n",
        "p_data = InputStream(path)\n",
        "\n",
        "q_lexer = QueryLexer(q_data)\n",
        "q_stream = CommonTokenStream(q_lexer)\n",
        "q_parser =QueryParser(q_stream)\n",
        "q_tree = q_parser.query()\n",
        "print_tree(nodize(q_tree))\n",
        "p_lexer = PathLexer(p_data)\n",
        "p_stream = CommonTokenStream(p_lexer)\n",
        "p_parser =PathParser(p_stream)\n",
        "p_tree = p_parser.path()\n",
        "print_tree(nodize(p_tree))\n",
        "\n",
        "\n",
        "query_vector =[0]*65\n",
        "print(query_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuhB8UBzdebn"
      },
      "outputs": [],
      "source": [
        "def parse(url):\n",
        "  \n",
        "  url=url.lower()\n",
        "  path_start=url.index(\"/\", url.index(\"//\")+1)\n",
        "  try:\n",
        "    path_end=url.index(\"?\")\n",
        "  except:\n",
        "    path_end=len(url)\n",
        "\n",
        "  basic = url[:path_start]\n",
        "  path = url[path_start+1:path_end]\n",
        "  query = url[path_end+1:]\n",
        "\n",
        "  q_vec=np.zeros((2,285))\n",
        "  p_vec=np.zeros((2,235))\n",
        "  if query == '': \n",
        "    query=None\n",
        "  if path=='':\n",
        "    path=None\n",
        "  if query!=None:\n",
        "    q_data = InputStream(query)\n",
        "    q_lexer = QueryLexer(q_data)\n",
        "    q_stream = CommonTokenStream(q_lexer)\n",
        "    q_parser =QueryParser(q_stream)\n",
        "    q_tree = q_parser.query()\n",
        "    getStates(q_tree, q_vec)\n",
        "    visitor = QueryVisitor()\n",
        "    output =visitor.visit(q_tree)\n",
        "  # print(\"===========================================\")\n",
        "  if path!=None:\n",
        "    p_data = InputStream(path)\n",
        "    p_lexer = PathLexer(p_data)\n",
        "    p_stream = CommonTokenStream(p_lexer)\n",
        "    p_parser =PathParser(p_stream)\n",
        "    p_tree = p_parser.path()\n",
        "    getStates(p_tree, p_vec)\n",
        "    visitor = PathVisitor()\n",
        "    output =visitor.visit(p_tree)\n",
        "  # print(len(p_vec[1]))\n",
        "  # print(\"==================\")\n",
        "  # print(p_vec[0])\n",
        "  # print(\"  --  \")\n",
        "  # print(p_vec[1])\n",
        "  # print(\"==================\")\n",
        "  # print(q_vec[1])\n",
        "  # return np.concatenate((p_vec[1], q_vec[1]))\n",
        "  return q_vec,q_tree, p_vec,p_tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wny4a87vRozs"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxYEhSg3j-kw"
      },
      "outputs": [],
      "source": [
        "\n",
        "url= \"http://localhost:8080/tienda1/miembros/editar.jsp?te=select+*+from+tb_ss+where+1=1&modo=registro&login=olson&password=conFesuR%CDA&nombre=Aberardo&apellidos=Segovia&email=raya%40acumulador.com.iq&dni=06076324T&direccion=Calle+Atarfe%F1o%2C+1%2B15%2C+&ciudad=Marug%E1n&cp=15589&provincia=Lleida&ntc=0903154438828489&B1=Registrar\".lower()\n",
        "# path_start=url.index(\"/\", url.index(\"/\")+2)\n",
        "# try:\n",
        "#   path_end=url.index(\"?\")\n",
        "# except:\n",
        "#   path_end=len(url)\n",
        "\n",
        "# basic = url[:path_start]\n",
        "# path = url[path_start+1:path_end]\n",
        "# query = url[path_end+1:]\n",
        "# print(path)\n",
        "# q_data = InputStream(query)\n",
        "# p_data = InputStream(path)\n",
        "\n",
        "# q_lexer = QueryLexer(q_data)\n",
        "# q_stream = CommonTokenStream(q_lexer)\n",
        "# q_parser =QueryParser(q_stream)\n",
        "# q_tree = q_parser.query()\n",
        "# print_tree(nodize(q_tree))\n",
        "# p_lexer = PathLexer(p_data)\n",
        "# p_stream = CommonTokenStream(p_lexer)\n",
        "# p_parser =PathParser(p_stream)\n",
        "# p_tree = p_parser.path()\n",
        "# print_tree(nodize(p_tree))\n",
        "\n",
        "\n",
        "# query_vector =[0]*65\n",
        "# print(query_vector)\n",
        "q, q_t, p, p_t = parse(url)\n",
        "print(q[0])\n",
        "# print(q[1])\n",
        "\n",
        "print(\"=====\")\n",
        "print_tree(nodize(q_t))\n",
        "# print(p[0])\n",
        "# print(p[1])\n",
        "\n",
        "\n",
        "\n",
        "# # print(url[406:])\n",
        "# # url= \"get http:/dpxd.er.ui:8080/p2/ppath2/path_47-8rt?q=w&q=re&e=sd-4de-6\"\n",
        "# # url = \"get https:/colab.research.google.com/drive/sel/se-d_m/sed34-ws/sdw_er4-w/a33sdr-t15iiki-_v/ty5-der_sdw?p=qwe&qw23w=25,w=sdw-s dd96,r=ade,edef4,rfrf4_de-&buery=select * from tb_test where 1=1\".lower()\n",
        "# v =parse(\"https://stackoverflow.com/questions/42905267/antlr-error-no-viable-alternative-at-input-eof\")\n",
        "\n",
        "# print(v)\n",
        "# query_vector =[0]*65\n",
        "# path_vector= [0]*65"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxeVSd-vDB_0"
      },
      "outputs": [],
      "source": [
        "# for i in range(0,len(v)):\n",
        "#   if v[i] >0:\n",
        "#     print(i,\"=\",v[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTDsmAdHAklo"
      },
      "outputs": [],
      "source": [
        "# for i in range(0,len(v)):\n",
        "#   if v[i] >0:\n",
        "#     print(i,\"=\",v[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW26lbxUDdz5"
      },
      "outputs": [],
      "source": [
        "# for i in range(0,len(v)):\n",
        "#   if v[i] >0:\n",
        "#     print(i,\"=\",v[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejjR_h0QgP_2"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def analyse(p):\n",
        "  \n",
        "  tmp= p.split('\\n')\n",
        "  if(p.startswith(\"GET\")):\n",
        "    tm = tmp[0].split(' ')\n",
        "    return tm[0]+' '+tm[1]\n",
        "  tm = tmp[0].split(' ')\n",
        "  # print(tmp)\n",
        "  return tm[0]+' '+tm[1]+ '?' +tmp[len(tmp)-3]\n",
        "  \n",
        "def readLangs(lang1, lang2, path, reverse=False):\n",
        "    c=[\"col\"]*520\n",
        "    print(c)\n",
        "    for i in range(0,520):\n",
        "      c[i]=\"col-\"+str(i)\n",
        "    f = open('/content/gdrive/MyDrive/ArshadPeoject/data/anomalousTrafficTest.csv', \"w\")\n",
        "    cw = csv.writer(f)\n",
        "    cw.writerow(c)\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    filedata = open(path, encoding='utf-8').\\\n",
        "        read().strip()\n",
        "    filedata = re.sub(r\"GET\", r\"@GET\", filedata)\n",
        "    filedata = re.sub(r\"POST\", r\"@POST\", filedata)\n",
        "    lines= filedata.split(\"@\")\n",
        "    lines= lines[1:]\n",
        "    # print(lines[20982])\n",
        "    # Split every line into pairs and normalize\n",
        "    # pairs = [[normalizeString(s) for s in [l,l]] for l in lines]\n",
        "    for l in lines:\n",
        "      cw.writerow(parse(analyse(l)))\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pLDfCUlEY-a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def init_test_validation_data():\n",
        "  anomal_file = open('/content/gdrive/MyDrive/ArshadPeoject/data/anomalousTrafficTest.csv', \"r\")\n",
        "  normal_file = open('/content/gdrive/MyDrive/ArshadPeoject/data/normalTrafficTest.csv', \"r\")\n",
        "\n",
        "  normal_valid = open('/content/gdrive/MyDrive/ArshadPeoject/data/normal_valid.csv', \"w\")\n",
        "  normall_test = open('/content/gdrive/MyDrive/ArshadPeoject/data/normal_test.csv', \"w\")\n",
        "\n",
        "  valid_write =csv.writer(normal_valid)\n",
        "  test_write  =csv.writer(normall_test)\n",
        "  c=[\"col\"]*520\n",
        "  for i in range(0,520):\n",
        "    c[i]=\"col-\"+str(i)\n",
        "\n",
        "\n",
        "  reader = csv.reader(normal_file)\n",
        "  a=list()\n",
        "  isFirst=False\n",
        "  valid_write.writerow(c)\n",
        "  test_write.writerow(c)\n",
        "  for row in reader:\n",
        "    if not isFirst:\n",
        "      isFirst=True\n",
        "    else:\n",
        "      a.append(row)\n",
        "  random.shuffle(a)\n",
        "  count =int(0.2*len(a))\n",
        "  for i in range(0,len(a)):\n",
        "    if i <count:\n",
        "      valid_write.writerow(a[i])\n",
        "    else:\n",
        "      test_write.writerow(a[i])\n",
        "\n",
        "  normal_valid.close()\n",
        "  normall_test.close()\n",
        "  #===============================================\n",
        "  anomal_valid = open('/content/gdrive/MyDrive/ArshadPeoject/data/anomalous_valid.csv', \"w\")\n",
        "  anomal_test = open('/content/gdrive/MyDrive/ArshadPeoject/data/anomalous_test.csv', \"w\")\n",
        "  valid_write =csv.writer(anomal_valid)\n",
        "  test_write  =csv.writer(anomal_test)\n",
        "\n",
        "  reader = csv.reader(anomal_file)\n",
        "  a=list()\n",
        "  isFirst=False\n",
        "  valid_write.writerow(c)\n",
        "  test_write.writerow(c)\n",
        "  for row in reader:\n",
        "    if not isFirst:\n",
        "      isFirst=True\n",
        "    else:\n",
        "      a.append(row)\n",
        "  random.shuffle(a)\n",
        "  count =int(0.2*len(a))\n",
        "  for i in range(0,len(a)):\n",
        "    if i <count:\n",
        "      valid_write.writerow(a[i])\n",
        "    else:\n",
        "      test_write.writerow(a[i])\n",
        "\n",
        "  anomal_valid.close()\n",
        "  anomal_test.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1j_hyY_ryQq",
        "outputId": "a2c315b3-d493-48bc-c135-f72d902250c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.         10.49127422 10.49127422 10.49127422  0.         10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422  0.         10.49127422 10.49127422\n",
            "  0.         10.49127422  0.         10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422  0.         10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422  0.          0.         10.49127422\n",
            " 10.49127422 10.49127422  0.         10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422  0.         10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            "  0.         10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422  1.28093385 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            "  0.81093022  0.81093022  0.81093022 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422  1.28093385  1.28093385 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422  0.81093022 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422  0.81093022 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422  0.81093022  1.13579508 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422  3.21195538\n",
            "  4.21087838 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422  3.26579274 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422  1.95781405 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            "  3.56961603 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422  2.17595244 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422  2.18925641 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422  0.81093022 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422  6.06045742 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422  7.09007684 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422  7.09007684 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422  5.96948564\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422  2.19722458 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422  6.13456539 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422 10.49127422  5.94797944 10.49127422 10.49127422 10.49127422\n",
            " 10.49127422  8.69951475 10.49127422 10.49127422]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # anomal_file = open('/content/gdrive/MyDrive/ArshadPeoject/data/anomalous_test2.csv', \"r\")\n",
        "\n",
        "  # normal_valid = open('/content/gdrive/MyDrive/ArshadPeoject/data/anormal_train.csv', \"w\")\n",
        "  # normall_test = open('/content/gdrive/MyDrive/ArshadPeoject/data/anormal_test.csv', \"w\")\n",
        "\n",
        "  # valid_write =csv.writer(normal_valid)\n",
        "  # test_write  =csv.writer(normall_test)\n",
        "  # c=[\"col\"]*520\n",
        "  # for i in range(0,520):\n",
        "  #   c[i]=\"col-\"+str(i)\n",
        "\n",
        "\n",
        "  # reader = csv.reader(anomal_file)\n",
        "  # a=list()\n",
        "  # isFirst=False\n",
        "  # valid_write.writerow(c)\n",
        "  # test_write.writerow(c)\n",
        "  # for row in reader:\n",
        "  #   if not isFirst:\n",
        "  #     isFirst=True\n",
        "  #   else:\n",
        "  #     a.append(row)\n",
        "  # random.shuffle(a)\n",
        "  # for i in range(0,len(a)):\n",
        "  #   if i <10000:\n",
        "  #     valid_write.writerow(a[i])\n",
        "  #   else:\n",
        "  #     test_write.writerow(a[i])\n",
        "\n",
        "  # normal_valid.close()\n",
        "  # normall_test.close()"
      ],
      "metadata": {
        "id": "y2BaqGGmMfh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4zFmu0zzEpP"
      },
      "outputs": [],
      "source": [
        "# anomal_file = open('/content/gdrive/MyDrive/ArshadPeoject/data/normalTrafficTest.csv', \"r\")\n",
        "# rrr = csv.reader(anomal_file)\n",
        "# r=0\n",
        "# for row in rrr:\n",
        "#   print(row)\n",
        "#   print(len(row))\n",
        "#   r+=1\n",
        "#   if r==5: \n",
        "#     pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vgQ1IFJgOpe"
      },
      "outputs": [],
      "source": [
        "# anorm_train.close()\n",
        "# anorm_test.close()\n",
        "\n",
        "# readLangs('eng', 'fra', '/content/gdrive/MyDrive/ArshadPeoject/data/raw_data/anomalousTrafficTest.txt', False)\n",
        "# print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C21N-z-bzIjy"
      },
      "outputs": [],
      "source": [
        "# init_test_validation_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6WWkw8VeW3U"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import click\n",
        "import torch\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from abc import ABC, abstractmethod\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywpx0nfPexBl"
      },
      "source": [
        "# Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3Ex4wEkfhfo"
      },
      "outputs": [],
      "source": [
        "class BaseADDataset(ABC):\n",
        "    \"\"\"Anomaly detection dataset base class.\"\"\"\n",
        "\n",
        "    def __init__(self, root: str):\n",
        "        super().__init__()\n",
        "        self.root = root  # root path to data\n",
        "\n",
        "        self.n_classes = 2  # 0: normal, 1: outlier\n",
        "        self.normal_classes = None  # tuple with original class labels that define the normal class\n",
        "        self.outlier_classes = None  # tuple with original class labels that define the outlier class\n",
        "\n",
        "        self.train_set = None  # must be of type torch.utils.data.Dataset\n",
        "        self.test_set = None  # must be of type torch.utils.data.Dataset\n",
        "        self.validation_set = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def loaders(self, batch_size: int, shuffle_train=True, shuffle_test=False, num_workers: int = 0) -> (\n",
        "            DataLoader, DataLoader):\n",
        "        \"\"\"Implement data loaders of type torch.utils.data.DataLoader for train_set and test_set.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ebZEcowfi1x"
      },
      "outputs": [],
      "source": [
        "class BaseNet(nn.Module):\n",
        "    \"\"\"Base class for all neural networks.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.rep_dim = None  # representation dimensionality, i.e. dim of the last layer\n",
        "\n",
        "    def forward(self, *input):\n",
        "        \"\"\"\n",
        "        Forward pass logic\n",
        "        :return: Network output\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"Network summary.\"\"\"\n",
        "        net_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
        "        params = sum([np.prod(p.size()) for p in net_parameters])\n",
        "        print('Trainable parameters: {}'.format(params))\n",
        "        print(self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdD6k2WtfoSU"
      },
      "outputs": [],
      "source": [
        "class BaseTrainer(ABC):\n",
        "    \"\"\"Trainer base class.\"\"\"\n",
        "\n",
        "    def __init__(self, optimizer_name: str, lr: float, n_epochs: int, lr_milestones: tuple, batch_size: int,\n",
        "                 weight_decay: float, device: str, n_jobs_dataloader: int):\n",
        "        super().__init__()\n",
        "        self.optimizer_name = optimizer_name\n",
        "        self.lr = lr\n",
        "        self.n_epochs = n_epochs\n",
        "        self.lr_milestones = lr_milestones\n",
        "        self.batch_size = batch_size\n",
        "        self.weight_decay = weight_decay\n",
        "        self.device = device\n",
        "        self.n_jobs_dataloader = n_jobs_dataloader\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self, dataset: BaseADDataset, net: BaseNet) -> BaseNet:\n",
        "        \"\"\"\n",
        "        Implement train method that trains the given network using the train_set of dataset.\n",
        "        :return: Trained net\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def test(self, dataset: BaseADDataset, net: BaseNet):\n",
        "        \"\"\"\n",
        "        Implement test method that evaluates the test_set of dataset on the given network.\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBW5FxHDfvLk"
      },
      "outputs": [],
      "source": [
        "class TorchvisionDataset(BaseADDataset):\n",
        "    \"\"\"TorchvisionDataset class for datasets already implemented in torchvision.datasets.\"\"\"\n",
        "\n",
        "    def __init__(self, root: str):\n",
        "        super().__init__(root)\n",
        "\n",
        "    def loaders(self, batch_size: int, shuffle_train=True, shuffle_test=False, num_workers: int = 0) -> (\n",
        "            DataLoader, DataLoader):\n",
        "        train_loader = DataLoader(dataset=self.train_set, batch_size=batch_size, shuffle=shuffle_train,\n",
        "                                  num_workers=num_workers)\n",
        "        test_loader = DataLoader(dataset=self.test_set, batch_size=batch_size, shuffle=shuffle_test,\n",
        "                                 num_workers=num_workers)\n",
        "        # validation_loader =  DataLoader(dataset=self.validation_set, batch_size=batch_size, shuffle=shuffle_test,\n",
        "        #                          num_workers=num_workers)\n",
        "        return train_loader, test_loader\n",
        "        # , validation_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exfkHVG9eIJp"
      },
      "source": [
        "# Optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AV2pPmAeRtX"
      },
      "outputs": [],
      "source": [
        "class AETrainer(BaseTrainer):\n",
        "\n",
        "    def __init__(self, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 150, lr_milestones: tuple = (),\n",
        "                 batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda', n_jobs_dataloader: int = 0):\n",
        "        super().__init__(optimizer_name, lr, n_epochs, lr_milestones, batch_size, weight_decay, device,\n",
        "                         n_jobs_dataloader)\n",
        "        self.validation = {\n",
        "            'loss': list(),\n",
        "            'valid':list()\n",
        "        }\n",
        "\n",
        "\n",
        "    def store_tmp(self,c_epoch,opt,schedul,ae):\n",
        "      torch.save({'optim_nameR': self.optimizer_name ,\n",
        "                    'lr': self.lr,\n",
        "                    'n_epochs': self.n_epochs,\n",
        "                    'lr_milestones': self.lr_milestones,\n",
        "                    'batch_size': self.batch_size,\n",
        "                    'weight_decay': self.weight_decay,\n",
        "                    'device': self.device,\n",
        "                    'n_jobs_dataloader': self.n_jobs_dataloader,\n",
        "                    'current_epoch':c_epoch,\n",
        "                    'validations':self.validation,\n",
        "                    'optimaizer': opt.state_dict(),\n",
        "                    'sched': schedul.state_dict(), \n",
        "                    'ae': ae.state_dict()}, '/content/gdrive/MyDrive/ArshadPeoject/model/tmp/ae.dict')\n",
        "\n",
        "\n",
        "    def train(self, dataset: BaseADDataset, valid_dataset: BaseADDataset, ae_net: BaseNet):\n",
        "        logger = logging.getLogger()\n",
        "\n",
        "        # Set device for network\n",
        "        ae_net = ae_net.to(self.device)\n",
        "\n",
        "        # Get train data loader\n",
        "        # Set optimizer (Adam optimizer for now)\n",
        "        optimizer = optim.Adam(ae_net.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n",
        "                               amsgrad=self.optimizer_name == 'amsgrad')\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.lr_milestones, gamma=0.1)\n",
        "\n",
        "        # Training\n",
        "        print('Starting pretraining...')\n",
        "        start_time = time.time()\n",
        "        return self.train_worker(ae_net,optimizer,scheduler,0,dataset,valid_dataset)\n",
        "       \n",
        "    def train_worker(self, ae_net,optimizer,scheduler,epoch_start, dataset, valid_dataset):\n",
        "\n",
        "        train_loader, _ = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "        _,valid_loader =valid_dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "        \n",
        "        ae_net.train()\n",
        "\n",
        "        for epoch in range(epoch_start, self.n_epochs):\n",
        "\n",
        "            scheduler.step()\n",
        "            if epoch in self.lr_milestones:\n",
        "                print('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n",
        "\n",
        "            loss_epoch = 0.0\n",
        "            n_batches = 0\n",
        "            epoch_start_time = time.time()\n",
        "            for data in train_loader:\n",
        "                inputs, _, _ = data\n",
        "                inputs = inputs.to(self.device)\n",
        "\n",
        "                # Zero the network parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Update network parameters via backpropagation: forward + backward + optimize\n",
        "                outputs = ae_net(inputs)\n",
        "                # print(outputs)\n",
        "                scores = torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n",
        "                loss = torch.mean(scores)\n",
        "\n",
        "                # print(loss)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                loss_epoch += loss.item()\n",
        "                n_batches += 1\n",
        "\n",
        "            # log epoch statistics\n",
        "            \n",
        "            loss_eval=0.0\n",
        "            n_eval_batches=0\n",
        "            self.validation['loss'].append(loss_epoch / n_batches)\n",
        "            ae_net.eval()\n",
        "            with torch.no_grad():\n",
        "                for data in valid_loader:\n",
        "                    inputs, labels, idx = data\n",
        "                    e_inputs = inputs.to(self.device)\n",
        "                    e_outputs = ae_net(e_inputs)\n",
        "                    e_scores= torch.sum((e_outputs - e_inputs) ** 2, dim=tuple(range(1, e_outputs.dim())))\n",
        "                    eval_loss = torch.mean(e_scores)\n",
        "\n",
        "                    loss_eval += eval_loss.item()\n",
        "                    n_eval_batches += 1\n",
        "\n",
        "            self.validation['valid'].append(loss_eval / n_eval_batches)\n",
        "            ae_net.train()\n",
        "            # self.plot_loss()\n",
        "            self.store_tmp(epoch+1,optimizer,scheduler,ae_net);\n",
        "            epoch_train_time = time.time() - epoch_start_time\n",
        "            print('  Epoch {}/{}\\t Time: {:.3f}\\t train_Loss: {:.8f} eval_Loss: {:.8f} {:.8f} {:.8f}'\n",
        "                        .format(epoch + 1, self.n_epochs, epoch_train_time, loss_epoch / n_batches, loss_eval / n_eval_batches, float(scheduler.get_lr()[0]), float(scheduler.get_last_lr()[0])))\n",
        "\n",
        "        # pretrain_time = time.time() - start_time\n",
        "        # print('Pretraining time: %.3f' % pretrain_time)\n",
        "        print('Finished pretraining.')\n",
        "\n",
        "        return ae_net, self.validation\n",
        "\n",
        "    \n",
        "\n",
        "    def test(self, dataset: BaseADDataset, ae_net: BaseNet):\n",
        "        logger = logging.getLogger()\n",
        "\n",
        "        # Set device for network\n",
        "        ae_net = ae_net.to(self.device)\n",
        "\n",
        "        # Get test data loader\n",
        "        _, test_loader = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "        # Testing\n",
        "        print('Testing autoencoder...')\n",
        "        loss_epoch = 0.0\n",
        "        n_batches = 0\n",
        "        start_time = time.time()\n",
        "        idx_label_score = []\n",
        "        ae_net.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                inputs, labels, idx = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                outputs = ae_net(inputs)\n",
        "                scores= torch.sum((outputs - inputs) ** 2, dim=tuple(range(1, outputs.dim())))\n",
        "                loss = torch.mean(scores)\n",
        "\n",
        "                # Save triple of (idx, label, score) in a list\n",
        "    \n",
        "                idx_label_score += list(zip(idx.cpu().data.numpy().tolist(),\n",
        "                                            labels.cpu().data.numpy().tolist(),\n",
        "                                            scores.cpu().data.numpy().tolist()))\n",
        "\n",
        "                loss_epoch += loss.item()\n",
        "                n_batches += 1\n",
        "\n",
        "        print('Test set Loss: {:.8f}'.format(loss_epoch / n_batches))\n",
        "\n",
        "        _, labels, scores = zip(*idx_label_score)\n",
        "        labels = np.array(labels)\n",
        "        scores = np.array(scores)\n",
        "\n",
        "        # auc = roc_auc_score(labels, scores)\n",
        "        # print('Test set AUC: {:.2f}%'.format(100. * auc))\n",
        "\n",
        "        test_time = time.time() - start_time\n",
        "        print('Autoencoder testing time: %.3f' % test_time)\n",
        "        print('Finished testing autoencoder.')\n",
        "\n",
        "\n",
        "    def load_and_continue(self,dataset, valid_dataset, ae):\n",
        "      data =  torch.load('/content/gdrive/MyDrive/ArshadPeoject/model/tmp/ae.dict',map_location=torch.device(self.device))\n",
        "\n",
        "      self.optimizer_name = data['optim_nameR']\n",
        "      self.lr =  data['lr']\n",
        "      # self.n_epochs =  data['n_epochs']\n",
        "      # self.lr_milestones =  data['lr_milestones']\n",
        "      self.batch_size =  data['batch_size']\n",
        "      self.weight_decay =  data['weight_decay']\n",
        "      # self.device =  data['device']\n",
        "      self.n_jobs_dataloader =  data['n_jobs_dataloader']\n",
        "      self.validation = data['validations']\n",
        "      # print(device)\n",
        "      ae = ae.to(self.device)\n",
        "\n",
        "      # data['sched']['milestones']=[118,200]\n",
        "      # data['sched']['last_epoch']=0\n",
        "      print(data['sched'])\n",
        "      # print(data['optimaizer'])\n",
        "\n",
        "      curr_epoch =data['current_epoch']\n",
        "      print(curr_epoch)\n",
        "      ae.load_state_dict(data['ae'])\n",
        "\n",
        "\n",
        "      opt=optim.Adam(ae.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n",
        "                               amsgrad=self.optimizer_name == 'amsgrad')\n",
        "\n",
        "      sche= optim.lr_scheduler.MultiStepLR(opt, milestones=self.lr_milestones, gamma=0.1)\n",
        "    \n",
        "      opt.load_state_dict(data['optimaizer'])\n",
        "\n",
        "      sche.load_state_dict(data['sched'])\n",
        "      tt=optim.lr_scheduler.MultiStepLR(opt, milestones=self.lr_milestones, gamma=0.1)\n",
        "      sche.milestones=tt.milestones\n",
        "      print(sche.milestones)\n",
        "\n",
        "      return self.train_worker(ae,opt,sche,curr_epoch, dataset, valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkRA1Ve0wUHg"
      },
      "outputs": [],
      "source": [
        "a = np.array([[1, 2],[5, 5],[6,6]])\n",
        "b = np.array([[1, 1 ], [2, 2]])\n",
        "\n",
        "\n",
        "# print(torch.tensor(c))\n",
        "\n",
        "# c= torch.sum(torch.tensor(c), dim=0).cpu()\n",
        "\n",
        "# print(c)\n",
        "\n",
        "# # c= \n",
        "\n",
        "# print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd7Vl8bCVS5R"
      },
      "outputs": [],
      "source": [
        "# # from google.colab import output\n",
        "# # output.enable_custom_widget_manager()\n",
        "# # from ipywidgets import interactive,interact\n",
        "\n",
        "# def plot_loss():\n",
        "#         import numpy as np\n",
        "#         import matplotlib.pyplot as plt\n",
        "#         from scipy.stats import norm\n",
        "#         import statistics\n",
        "#         from matplotlib.pyplot import figure\n",
        "#         from sklearn import preprocessing\n",
        "#         figure(figsize=(30, 20), dpi=50)\n",
        " \n",
        "#         ll = np.random.rand(10,1)\n",
        "#         plt.plot(range(0,10) ,ll, label='train loss')\n",
        "#         plt.plot(range(0, 10 ),ll, label='validation_losss')\n",
        "#         plt.legend(loc=\"upper left\")\n",
        "#         # plt.ylim(-0, 02.)\n",
        "#         return plt\n",
        "# import io\n",
        "# import base64\n",
        "# from IPython.display import display\n",
        "# import ipywidgets\n",
        "# import matplotlib.pyplot as plt\n",
        "# def pltToImg(plt):\n",
        "#  s = io.BytesIO()\n",
        "#  plt.savefig(s, format='png', bbox_inches=\"tight\")\n",
        "#  s = base64.b64encode(s.getvalue()).decode(\"utf-8\").replace(\"\\n\", \"\")\n",
        "#  #plt.close()\n",
        "#  return '<img align=\"left\" src=\"data:image/png;base64,%s\">' % s\n",
        "# ii=0\n",
        "# a=ipywidgets.HTML(\n",
        "#     value=\"image here\"\n",
        "# )\n",
        "# for i in range(0,50000):\n",
        "#     if i%5000==0:\n",
        "#        ii+=1\n",
        "#        point=np.r_[ii,np.random.normal(0, 1, 1)]\n",
        "  \n",
        "#        a.value=(pltToImg(plot_loss()))\n",
        "#        a.value+=\" </br>\"\n",
        "#        a.value+=str(ii)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0sQGiKAsnyAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPArwLzofPT_"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class DeepSVDDTrainer(BaseTrainer):\n",
        "\n",
        "    def __init__(self, objective, R, c, nu: float, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 150,\n",
        "                 lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "                 n_jobs_dataloader: int = 0):\n",
        "        super().__init__(optimizer_name, lr, n_epochs, lr_milestones, batch_size, weight_decay, device,\n",
        "                         n_jobs_dataloader)\n",
        "        self.validation = {\n",
        "            'loss': list(),\n",
        "            'valid':list()\n",
        "        }\n",
        "        assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n",
        "        self.objective = objective\n",
        "\n",
        "        # Deep SVDD parameters\n",
        "        self.R = torch.tensor(R, device=self.device)  # radius R initialized with 0 by default.\n",
        "        self.c = torch.tensor(c, device=self.device) if c is not None else None\n",
        "        self.nu = nu\n",
        "\n",
        "        # Optimization parameters\n",
        "        self.warm_up_n_epochs = 10  # number of training epochs for soft-boundary Deep SVDD before radius R gets updated\n",
        "\n",
        "        # Results\n",
        "        self.train_time = None\n",
        "        self.test_auc = None\n",
        "        self.test_time = None\n",
        "        self.test_scores = None\n",
        "\n",
        "        self.mu=None\n",
        "        self.s=None\n",
        "        self.m_tam=None\n",
        "        self.cov_mat=None\n",
        "\n",
        "    def train(self, dataset: BaseADDataset, valid_dataset: BaseADDataset, net: BaseNet):\n",
        "        logger = logging.getLogger()\n",
        "\n",
        "        # Set device for network\n",
        "        net = net.to(self.device)\n",
        "\n",
        "        \n",
        "        # Set optimizer (Adam optimizer for now)\n",
        "        optimizer = optim.Adam(net.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n",
        "                               amsgrad=self.optimizer_name == 'amsgrad')\n",
        "        \n",
        "        train_loader, _ = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.lr_milestones, gamma=0.1)\n",
        "\n",
        "        # Initialize hypersphere center c (if c not loaded)\n",
        "        if self.c is None:\n",
        "            print('Initializing center c...')\n",
        "            self.c = self.init_center_c(train_loader, net)\n",
        "            print('Center c initialized.')\n",
        "\n",
        "        # Training\n",
        "        print('Starting training...')\n",
        "        start_time = time.time()\n",
        "        return self.train_worker(net,optimizer,scheduler,0,dataset,valid_dataset,train_loader)\n",
        "\n",
        "    def update_profile(self,  dataset: BaseADDataset,net: BaseNet):\n",
        "      train_loader, _ = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "      # Set device for network\n",
        "      net = net.to(self.device)\n",
        "\n",
        "      # Get test data loader\n",
        "\n",
        "\n",
        "      # Testing\n",
        "      print('Starting testing...')\n",
        "      start_time = time.time()\n",
        "      idx_label_score = []\n",
        "      net.eval()\n",
        "      inp = list()\n",
        "      lbl = list()\n",
        "      m_tam = None\n",
        "      c=0\n",
        "      with torch.no_grad():\n",
        "          for data in train_loader:\n",
        "              inputs, labels, idx = data\n",
        "              inputs = inputs.to(self.device)\n",
        "              outputs = net(inputs)\n",
        "              inp.append(outputs.cpu().data.numpy().tolist())\n",
        "              lbl.append(labels.cpu().data.numpy().tolist())\n",
        "\n",
        "      my_array= np.array(inp[0])\n",
        "      for i in range(1,len(inp)):\n",
        "        my_array= np.append(my_array,np.array(inp[i]),axis=0)\n",
        "\n",
        "      l = np.array(lbl[0])\n",
        "      for i in range(1,len(lbl)):\n",
        "        l= np.append(l,np.array(lbl[i]))\n",
        "\n",
        "      l=l.reshape(-1,1)\n",
        "      print(\"inup\",my_array.shape)\n",
        "      \n",
        "\n",
        "      \n",
        "      # print(/\"tt\", tam.shape)\n",
        "      mu, s, m_tam, cov_mat =self.profileGenerator(my_array)\n",
        "      pro = {\n",
        "          \"mu\": mu,\n",
        "          \"s\": s,\n",
        "          \"m_tam\": m_tam,\n",
        "          \"cov_mat\": cov_mat,\n",
        "      }\n",
        "      return pro\n",
        "\n",
        "\n",
        "    def train_worker(self,  net,optimizer,scheduler,epoch_start, dataset, valid_dataset, train_loader):\n",
        "      # Get train data loader\n",
        "      _,valid_loader =valid_dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "      net.train()\n",
        "      for epoch in range(epoch_start,self.n_epochs):\n",
        "\n",
        "          scheduler.step()\n",
        "          if epoch in self.lr_milestones:\n",
        "              print('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n",
        "\n",
        "          loss_epoch = 0.0\n",
        "          n_batches = 0\n",
        "          epoch_start_time = time.time()\n",
        "          for data in train_loader:\n",
        "              inputs, _, _ = data\n",
        "              inputs = inputs.to(self.device)\n",
        "\n",
        "              # Zero the network parameter gradients\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              # Update network parameters via backpropagation: forward + backward + optimize\n",
        "              outputs = net(inputs)\n",
        "              dist = torch.sum((outputs - self.c) ** 2, dim=1)\n",
        "              # dist= F.cosine_similarity(outputs, self.c, dim=1)\n",
        "\n",
        "              if self.objective == 'soft-boundary':\n",
        "                  scores = dist - self.R ** 2\n",
        "                  loss = self.R ** 2 + (1 / self.nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n",
        "              else:\n",
        "                  loss = torch.mean(dist)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              # Update hypersphere radius R on mini-batch distances\n",
        "              if (self.objective == 'soft-boundary') and (epoch >= self.warm_up_n_epochs):\n",
        "                  self.R.data = torch.tensor(get_radius(dist, self.nu), device=self.device)\n",
        "\n",
        "              loss_epoch += loss.item()\n",
        "              n_batches += 1\n",
        "\n",
        "          # log epoch statistics\n",
        "          \n",
        "          loss_eval=0.0\n",
        "          n_eval_batches=0\n",
        "          self.validation['loss'].append(loss_epoch / n_batches)\n",
        "          net.eval()\n",
        "          with torch.no_grad():\n",
        "              for data in valid_loader:\n",
        "                  inputs, labels, idx = data\n",
        "                  e_inputs = inputs.to(self.device)\n",
        "                  e_outputs = net(e_inputs)\n",
        "                  dist = torch.sum((outputs - self.c) ** 2, dim=1)\n",
        "                  # dist= F.cosine_similarity(outputs, self.c, dim=1)\n",
        "\n",
        "                  if self.objective == 'soft-boundary':\n",
        "                      e_scores = dist - self.R ** 2\n",
        "                  else:\n",
        "                      e_scores = dist                      \n",
        "                  \n",
        "                  eval_loss = torch.mean(e_scores)\n",
        "\n",
        "                  loss_eval += eval_loss.item()\n",
        "                  n_eval_batches += 1\n",
        "\n",
        "          self.validation['valid'].append(loss_eval / n_eval_batches)\n",
        "          # self.plot_loss()\n",
        "          net.train()\n",
        "          epoch_train_time = time.time() - epoch_start_time\n",
        "          print('  Epoch {}/{}\\t Time: {:.3f}\\t train_loss: {:.8f} test_loss: {:.8f}'\n",
        "                      .format(epoch + 1, self.n_epochs, epoch_train_time, loss_epoch / n_batches, loss_eval / n_eval_batches))\n",
        "          self.store_tmp(epoch+1,optimizer,scheduler,net)\n",
        "\n",
        "      # self.train_time = time.time() - start_time\n",
        "      # print('Training time: %.3f' % self.train_time)\n",
        "\n",
        "      print('Finished training.')\n",
        "\n",
        "      return net,self.validation\n",
        "\n",
        "    def plot_loss(self):\n",
        "        %matplotlib notebook\n",
        "        import numpy as np\n",
        "        import matplotlib.pyplot as plt\n",
        "        from scipy.stats import norm\n",
        "        import statistics\n",
        "        from matplotlib.pyplot import figure\n",
        "        from sklearn import preprocessing\n",
        "        figure(figsize=(30, 20), dpi=150)\n",
        "\n",
        "        plt.plot(range(0, len(self.validation['loss']) ),self.validation['loss'], label='train loss')\n",
        "        plt.plot(range(0, len(self.validation['valid']) ),self.validation['valid'], label='validation_losss')\n",
        "        plt.legend(loc=\"upper left\")\n",
        "        # plt.ylim(-0, 02.)\n",
        "        plt.show()\n",
        "\n",
        "    def test(self, dataset: BaseADDataset, net: BaseNet):\n",
        "        logger = logging.getLogger()\n",
        "\n",
        "        # Set device for network\n",
        "        net = net.to(self.device)\n",
        "\n",
        "        # Get test data loader\n",
        "        _, test_loader = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "        # Testing\n",
        "        print('Starting testing...')\n",
        "        start_time = time.time()\n",
        "        idx_label_score = []\n",
        "        net.eval()\n",
        "        inp = list()\n",
        "        lbl = list()\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                inputs, labels, idx = data\n",
        "\n",
        "               \n",
        "                inputs = inputs.to(self.device)\n",
        "                outputs = net(inputs)\n",
        "                inp.append(outputs.cpu().data.numpy().tolist())\n",
        "                lbl.append(labels.cpu().data.numpy().tolist())\n",
        "                dist = torch.sum((outputs - self.c) ** 2, dim=1)\n",
        "                # dist= F.cosine_similarity(outputs, self.c, dim=1)\n",
        "                if self.objective == 'soft-boundary':\n",
        "                    scores = dist - self.R ** 2\n",
        "                else:\n",
        "                    scores = dist\n",
        "\n",
        "                # Save triples of (idx, label, score) in a list\n",
        "                idx_label_score += list(zip(idx.cpu().data.numpy().tolist(),\n",
        "                                            labels.cpu().data.numpy().tolist(),\n",
        "                                            scores.cpu().data.numpy().tolist(),outputs))\n",
        "\n",
        "        self.test_time = time.time() - start_time\n",
        "        print('Testing time: %.3f' % self.test_time)\n",
        "\n",
        "        self.test_scores = idx_label_score\n",
        "\n",
        "        # Compute AUC\n",
        "        _, labels, scores,_ = zip(*idx_label_score)\n",
        "        labels = np.array(labels)\n",
        "        scores = np.array(scores)\n",
        "\n",
        "        self.test_auc = roc_auc_score(labels, scores)\n",
        "        print('Test set AUC: {:.2f}%'.format(100. * self.test_auc))\n",
        "\n",
        "        print('Finished testing.')\n",
        "\n",
        "        return inp,lbl\n",
        "\n",
        "    def init_center_c(self, train_loader: DataLoader, net: BaseNet, eps=0.1):\n",
        "        \"\"\"Initialize hypersphere center c as the mean from an initial forward pass on the data.\"\"\"\n",
        "\n",
        "        outs=None\n",
        "        net.eval()\n",
        "        isFirst=True\n",
        "        with torch.no_grad():\n",
        "            for data in train_loader:\n",
        "                inputs, labels, idx = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                if isFirst:\n",
        "                  outs = net(inputs).cpu()\n",
        "                  isFirst=False\n",
        "                else:\n",
        "                  outs = np.append(outs,net(inputs).cpu(),axis=0)\n",
        "\n",
        "        \n",
        "\n",
        "        from sklearn.cluster import KMeans\n",
        "        import math\n",
        "        kmeans = KMeans(n_clusters=3,)\n",
        "        kmeans.fit(outs)\n",
        "        c=kmeans.cluster_centers_\n",
        "        return torch.tensor(c,device=self.device)\n",
        "\n",
        "    def store_tmp(self,c_epoch,opt,schedul,ae):\n",
        "      torch.save({'optim_nameR': self.optimizer_name ,\n",
        "                    'lr': self.lr,\n",
        "                    'n_epochs': self.n_epochs,\n",
        "                    'lr_milestones': self.lr_milestones,\n",
        "                    'batch_size': self.batch_size,\n",
        "                    'weight_decay': self.weight_decay,\n",
        "                    'device': self.device,\n",
        "                    'n_jobs_dataloader': self.n_jobs_dataloader,\n",
        "                    'current_epoch':c_epoch,\n",
        "                    'validations':self.validation,\n",
        "                    'optimaizer': opt.state_dict(),\n",
        "                    'sched': schedul.state_dict(),\n",
        "                    'R': self.R,\n",
        "                    'c': self.c, \n",
        "                    'ae': ae.state_dict()}, '/content/gdrive/MyDrive/ArshadPeoject/model/tmp/ae-clf.dict')\n",
        "\n",
        "    def load_and_continue(self,dataset, valid_dataset, ae):\n",
        "          data =  torch.load('/content/gdrive/MyDrive/ArshadPeoject/model/tmp/ae-clf.dict',map_location=torch.device(self.device))\n",
        "          train_loader, _ = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "          self.optimizer_name = data['optim_nameR']\n",
        "          self.lr =  data['lr']\n",
        "          # self.n_epochs =  data['n_epochs']\n",
        "          # self.lr_milestones =  data['lr_milestones']\n",
        "          self.batch_size =  data['batch_size']\n",
        "          self.weight_decay =  data['weight_decay']\n",
        "          # self.device =  data['device']\n",
        "          self.n_jobs_dataloader =  data['n_jobs_dataloader']\n",
        "          self.validation = data['validations']\n",
        "\n",
        "          self.R= data['R']\n",
        "          self.c= data['c']\n",
        "\n",
        "          # print(device)\n",
        "          ae = ae.to(self.device)\n",
        "\n",
        "          # data['sched']['milestones']=[118,200]\n",
        "          # data['sched']['last_epoch']=0\n",
        "          print(data['sched'])\n",
        "          # print(data['optimaizer'])\n",
        "\n",
        "          curr_epoch =data['current_epoch']\n",
        "          print(curr_epoch)\n",
        "          ae.load_state_dict(data['ae'])\n",
        "\n",
        "\n",
        "          opt=optim.Adam(ae.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n",
        "                                  amsgrad=self.optimizer_name == 'amsgrad')\n",
        "\n",
        "          sche= optim.lr_scheduler.MultiStepLR(opt, milestones=self.lr_milestones, gamma=0.1)\n",
        "        \n",
        "          opt.load_state_dict(data['optimaizer'])\n",
        "\n",
        "          sche.load_state_dict(data['sched'])\n",
        "          tt=optim.lr_scheduler.MultiStepLR(opt, milestones=self.lr_milestones, gamma=0.1)\n",
        "          sche.milestones=tt.milestones\n",
        "          print(sche.milestones)\n",
        "          return self.train_worker(ae,opt,sche,curr_epoch, dataset, valid_dataset,train_loader)\n",
        "\n",
        "     # Finalizeing\n",
        "    def genTAM(self, vector):\n",
        "      l=len(vector)\n",
        "\n",
        "\n",
        "     \n",
        "      ttt=np.array([vector[0]])\n",
        "      # print(\"kk\", ttt.shape)\n",
        "\n",
        "      for i in range(1,l):\n",
        "        ttt=np.append(ttt,[vector[i]],axis=0)\n",
        "      # print(\"fin\")\n",
        "      # print(ttt.shape)\n",
        "      return ttt;\n",
        "\n",
        "    def calcCov(self, tam, m_tam):\n",
        "      d = len(m_tam)-1 \n",
        "      cov_d =int((d*(d+1))/2)\n",
        "      \n",
        "      cov_mat=np.zeros(shape=(cov_d,cov_d))\n",
        "      cc=0\n",
        "      for j in range(0,d):\n",
        "        for k in range(0,d):\n",
        "          if k>=j:\n",
        "              break\n",
        "          c=0\n",
        "          for l in range(0,d):\n",
        "            for v in range(0,d):\n",
        "              if v>=l:\n",
        "                break\n",
        "              cov_mat[cc][c]=self.sigma(tam, m_tam, j+1, k, l+1, v, len(tam))\n",
        "              c+=1\n",
        "          cc+=1\n",
        "      return cov_mat\n",
        "\n",
        "    def sigma(self, tam, m_tam, j, k , l, v, g,cov=0):\n",
        "      # print(j,k,l,v)\n",
        "      # print(tam.shape)\n",
        "      # print(m_tam.shape)\n",
        "      \n",
        "      for i in range(0, g):\n",
        "        cov+=(tam[i][j][k] - m_tam[j][k])*(tam[i][l][v] - m_tam[l][v])/(g-1)\n",
        "      return cov\n",
        "\n",
        "    \n",
        "    def md(self, t, cov_inv, m_t, mask):\n",
        "      tmp=(t - m_t)[:,mask]\n",
        "      print(tmp.shape)\n",
        "      print(t.shape)\n",
        "      print(cov_inv.shape)\n",
        "      print(m_t.shape)\n",
        "      a=np.matmul(tmp,cov_inv)\n",
        "      print(a.shape)\n",
        "      b=np.sum(np.multiply(a,tmp),axis=1)\n",
        "      print(b.shape)\n",
        "      return np.sqrt(np.absolute(b))\n",
        "      \n",
        "    def mahalanobis(self, x=None, data=None, cov=None):\n",
        "      mu = np.mean(data)\n",
        "      x_mu = x - mu\n",
        "      if not cov:\n",
        "          cov = np.cov(data.T)\n",
        "      inv_covmat = np.linalg.inv(cov)\n",
        "      print(cov)\n",
        "      print(inv_covmat)\n",
        "      left = np.dot(x_mu, inv_covmat)\n",
        "      mahal = np.zeros(len(left))\n",
        "      for i in range(1, len(left)):\n",
        "        mahal[i]=np.dot(left[i],x_mu[i].T)\n",
        "      # mahal = np.dot(left, x_mu.T)\n",
        "\n",
        "      return mahal, cov ,mu\n",
        "\n",
        "    def profileGenerator(self, tam):\n",
        "        g=len(tam)\n",
        "        print(\"g\", tam.shape)\n",
        "        mdd ,cov_mat, m_tam=self.mahalanobis(tam,tam)\n",
        "        print(\"md\", mdd)\n",
        "        mu=np.average(mdd);\n",
        "        print(\"mu\", mu)\n",
        "        s= np.sqrt(np.sum(np.power(mdd-mu,2)/g-1))\n",
        "        print(\"s\", s)\n",
        "        return mu, s, m_tam, cov_mat\n",
        "\n",
        "def get_radius(dist: torch.Tensor, nu: float):\n",
        "    \"\"\"Optimally solve for radius R via the (1-nu)-quantile of distances.\"\"\"\n",
        "    return np.quantile(np.sqrt(dist.clone().data.cpu().numpy()), 1 - nu)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "x = np.array([[1,2,3],\n",
        "               [3,4,5],\n",
        "               [5,6,7],\n",
        "              [5,6,7],\n",
        "               [7,8,9],\n",
        "               [9,0,1]])\n",
        "\n",
        "i,j,k = x.shape\n",
        "\n",
        "xx = x.reshape(i,j*k).T\n",
        "\n",
        "\n",
        "y = np.array([8,7,6]])\n",
        "\n",
        "\n",
        "yy = y.reshape(i,j*k).T\n",
        "\n",
        "results =  cdist(xx,yy,'mahalanobis')\n",
        "\n",
        "results = np.diag(results)\n",
        "print (results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "lg5ktNMcVFP8",
        "outputId": "35fe8460-2c70-4218-9bad-3c5f4651755d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-9fccfbef09f8>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    y = np.array([8,7,6]])\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWK01PrHL7NS"
      },
      "outputs": [],
      "source": [
        "# class DeepSVDDTrainer(BaseTrainer):\n",
        "\n",
        "#     def __init__(self, objective, R, c, nu: float, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 150,\n",
        "#                  lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "#                  n_jobs_dataloader: int = 0):\n",
        "#         super().__init__(optimizer_name, lr, n_epochs, lr_milestones, batch_size, weight_decay, device,\n",
        "#                          n_jobs_dataloader)\n",
        "#         self.validation = {\n",
        "#             'loss': list(),\n",
        "#             'valid':list()\n",
        "#         }\n",
        "#         assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n",
        "#         self.objective = objective\n",
        "\n",
        "#         # Deep SVDD parameters\n",
        "#         self.R = torch.tensor(R, device=self.device)  # radius R initialized with 0 by default.\n",
        "#         self.c = torch.tensor(c, device=self.device) if c is not None else None\n",
        "#         self.nu = nu\n",
        "\n",
        "#         # Optimization parameters\n",
        "#         self.warm_up_n_epochs = 10  # number of training epochs for soft-boundary Deep SVDD before radius R gets updated\n",
        "\n",
        "#         # Results\n",
        "#         self.train_time = None\n",
        "#         self.test_auc = None\n",
        "#         self.test_time = None\n",
        "#         self.test_scores = None\n",
        "\n",
        "#         self.mu=None\n",
        "#         self.s=None\n",
        "#         self.m_tam=None\n",
        "#         self.cov_mat=None\n",
        "\n",
        "#     def train(self, dataset: BaseADDataset, valid_dataset: BaseADDataset, net: BaseNet):\n",
        "#         logger = logging.getLogger()\n",
        "\n",
        "#         # Set device for network\n",
        "#         net = net.to(self.device)\n",
        "\n",
        "        \n",
        "#         # Set optimizer (Adam optimizer for now)\n",
        "#         optimizer = optim.Adam(net.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n",
        "#                                amsgrad=self.optimizer_name == 'amsgrad')\n",
        "        \n",
        "#         train_loader, _ = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "#         # Set learning rate scheduler\n",
        "#         scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.lr_milestones, gamma=0.1)\n",
        "\n",
        "#         # Initialize hypersphere center c (if c not loaded)\n",
        "#         if self.c is None:\n",
        "#             print('Initializing center c...')\n",
        "#             self.c = self.init_center_c(train_loader, net)\n",
        "#             print('Center c initialized.')\n",
        "\n",
        "#         # Training\n",
        "#         print('Starting training...')\n",
        "#         start_time = time.time()\n",
        "#         return self.train_worker(net,optimizer,scheduler,0,dataset,valid_dataset,train_loader)\n",
        "\n",
        "#     def update_profile(self,  dataset: BaseADDataset,net: BaseNet):\n",
        "#       train_loader, _ = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "#       # Set device for network\n",
        "#       net = net.to(self.device)\n",
        "\n",
        "#       # Get test data loader\n",
        "\n",
        "\n",
        "#       # Testing\n",
        "#       print('Starting testing...')\n",
        "#       start_time = time.time()\n",
        "#       idx_label_score = []\n",
        "#       net.eval()\n",
        "#       inp = list()\n",
        "#       lbl = list()\n",
        "#       m_tam = None\n",
        "#       c=0\n",
        "#       with torch.no_grad():\n",
        "#           for data in train_loader:\n",
        "#               inputs, labels, idx = data\n",
        "#               inputs = inputs.to(self.device)\n",
        "#               outputs = net(inputs)\n",
        "#               t=outputs.cpu().data.numpy().tolist()\n",
        "#               c+=len(t)\n",
        "#               tam = self.genTAM(t)\n",
        "#               if m_tan==None:\n",
        "#                 m_tam = np.sum(tam, axis=0)\n",
        "#               else:\n",
        "#                 mtam =m_tam+np.sum(tam,axis=0)\n",
        "                               \n",
        "#         with torch.no_grad():\n",
        "#           for data in train_loader:\n",
        "#               inputs, labels, idx = data\n",
        "#               inputs = inputs.to(self.device)\n",
        "#               outputs = net(inputs)\n",
        "#               cov_mat= self.calcCov(tam,m_tam) \n",
        "\n",
        "#         with torch.no_grad():\n",
        "#           for data in train_loader:\n",
        "#               inputs, labels, idx = data\n",
        "#               inputs = inputs.to(self.device)\n",
        "#               outputs = net(inputs)\n",
        "#         self.mu, self.s, self.m_tam, self.cov_mat = self.profileGenerator(tam,c,cov_mat)\n",
        "#       # my_array= np.array(inp[0])\n",
        "#       # for i in range(1,len(inp)):\n",
        "#       #   my_array= np.append(my_array,np.array(inp[i]),axis=0)\n",
        "\n",
        "#       # l = np.array(lbl[0])\n",
        "#       # for i in range(1,len(lbl)):\n",
        "#       #   l= np.append(l,np.array(lbl[i]))\n",
        "\n",
        "#       # l=l.reshape(-1,1)\n",
        "#       # print(\"inup\",my_array.shape)\n",
        "      \n",
        "\n",
        "      \n",
        "#       return zip(self.mu, self.s, self.m_tam, self.cov_mat)\n",
        "\n",
        "\n",
        "#     def train_worker(self,  net,optimizer,scheduler,epoch_start, dataset, valid_dataset, train_loader):\n",
        "#       # Get train data loader\n",
        "#       _,valid_loader =valid_dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "#       net.train()\n",
        "#       for epoch in range(epoch_start,self.n_epochs):\n",
        "\n",
        "#           scheduler.step()\n",
        "#           if epoch in self.lr_milestones:\n",
        "#               print('  LR scheduler: new learning rate is %g' % float(scheduler.get_lr()[0]))\n",
        "\n",
        "#           loss_epoch = 0.0\n",
        "#           n_batches = 0\n",
        "#           epoch_start_time = time.time()\n",
        "#           for data in train_loader:\n",
        "#               inputs, _, _ = data\n",
        "#               inputs = inputs.to(self.device)\n",
        "\n",
        "#               # Zero the network parameter gradients\n",
        "#               optimizer.zero_grad()\n",
        "\n",
        "#               # Update network parameters via backpropagation: forward + backward + optimize\n",
        "#               outputs = net(inputs)\n",
        "#               dist = torch.sum((outputs - self.c) ** 2, dim=1)\n",
        "#               if self.objective == 'soft-boundary':\n",
        "#                   scores = dist - self.R ** 2\n",
        "#                   loss = self.R ** 2 + (1 / self.nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n",
        "#               else:\n",
        "#                   loss = torch.mean(dist)\n",
        "#               loss.backward()\n",
        "#               optimizer.step()\n",
        "\n",
        "#               # Update hypersphere radius R on mini-batch distances\n",
        "#               if (self.objective == 'soft-boundary') and (epoch >= self.warm_up_n_epochs):\n",
        "#                   self.R.data = torch.tensor(get_radius(dist, self.nu), device=self.device)\n",
        "\n",
        "#               loss_epoch += loss.item()\n",
        "#               n_batches += 1\n",
        "\n",
        "#           # log epoch statistics\n",
        "          \n",
        "#           loss_eval=0.0\n",
        "#           n_eval_batches=0\n",
        "#           self.validation['loss'].append(loss_epoch / n_batches)\n",
        "#           net.eval()\n",
        "#           with torch.no_grad():\n",
        "#               for data in valid_loader:\n",
        "#                   inputs, labels, idx = data\n",
        "#                   e_inputs = inputs.to(self.device)\n",
        "#                   e_outputs = net(e_inputs)\n",
        "#                   dist = torch.sum((outputs - self.c) ** 2, dim=1)\n",
        "#                   if self.objective == 'soft-boundary':\n",
        "#                       e_scores = dist - self.R ** 2\n",
        "#                   else:\n",
        "#                       e_scores = dist                      \n",
        "                  \n",
        "#                   eval_loss = torch.mean(e_scores)\n",
        "\n",
        "#                   loss_eval += eval_loss.item()\n",
        "#                   n_eval_batches += 1\n",
        "\n",
        "#           self.validation['valid'].append(loss_eval / n_eval_batches)\n",
        "#           # self.plot_loss()\n",
        "#           net.train()\n",
        "#           epoch_train_time = time.time() - epoch_start_time\n",
        "#           print('  Epoch {}/{}\\t Time: {:.3f}\\t train_loss: {:.8f} test_loss: {:.8f}'\n",
        "#                       .format(epoch + 1, self.n_epochs, epoch_train_time, loss_epoch / n_batches, loss_eval / n_eval_batches))\n",
        "#           self.store_tmp(epoch+1,optimizer,scheduler,net)\n",
        "\n",
        "#       # self.train_time = time.time() - start_time\n",
        "#       # print('Training time: %.3f' % self.train_time)\n",
        "\n",
        "#       print('Finished training.')\n",
        "\n",
        "#       return net,self.validation\n",
        "\n",
        "#     def plot_loss(self):\n",
        "#         %matplotlib notebook\n",
        "#         import numpy as np\n",
        "#         import matplotlib.pyplot as plt\n",
        "#         from scipy.stats import norm\n",
        "#         import statistics\n",
        "#         from matplotlib.pyplot import figure\n",
        "#         from sklearn import preprocessing\n",
        "#         figure(figsize=(30, 20), dpi=150)\n",
        "\n",
        "#         plt.plot(range(0, len(self.validation['loss']) ),self.validation['loss'], label='train loss')\n",
        "#         plt.plot(range(0, len(self.validation['valid']) ),self.validation['valid'], label='validation_losss')\n",
        "#         plt.legend(loc=\"upper left\")\n",
        "#         # plt.ylim(-0, 02.)\n",
        "#         plt.show()\n",
        "\n",
        "#     def test(self, dataset: BaseADDataset, net: BaseNet):\n",
        "#         logger = logging.getLogger()\n",
        "\n",
        "#         # Set device for network\n",
        "#         net = net.to(self.device)\n",
        "\n",
        "#         # Get test data loader\n",
        "#         _, test_loader = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "#         # Testing\n",
        "#         print('Starting testing...')\n",
        "#         start_time = time.time()\n",
        "#         idx_label_score = []\n",
        "#         net.eval()\n",
        "#         inp = list()\n",
        "#         lbl = list()\n",
        "#         with torch.no_grad():\n",
        "#             for data in test_loader:\n",
        "#                 inputs, labels, idx = data\n",
        "\n",
        "               \n",
        "#                 inputs = inputs.to(self.device)\n",
        "#                 outputs = net(inputs)\n",
        "#                 inp.append(outputs.cpu().data.numpy().tolist())\n",
        "#                 lbl.append(labels.cpu().data.numpy().tolist())\n",
        "#                 dist = torch.sum((outputs - self.c) ** 2, dim=1)\n",
        "#                 if self.objective == 'soft-boundary':\n",
        "#                     scores = dist - self.R ** 2\n",
        "#                 else:\n",
        "#                     scores = dist\n",
        "\n",
        "#                 # Save triples of (idx, label, score) in a list\n",
        "#                 idx_label_score += list(zip(idx.cpu().data.numpy().tolist(),\n",
        "#                                             labels.cpu().data.numpy().tolist(),\n",
        "#                                             scores.cpu().data.numpy().tolist(),outputs))\n",
        "\n",
        "#         self.test_time = time.time() - start_time\n",
        "#         print('Testing time: %.3f' % self.test_time)\n",
        "\n",
        "#         self.test_scores = idx_label_score\n",
        "\n",
        "#         # Compute AUC\n",
        "#         _, labels, scores,_ = zip(*idx_label_score)\n",
        "#         labels = np.array(labels)\n",
        "#         scores = np.array(scores)\n",
        "\n",
        "#         self.test_auc = roc_auc_score(labels, scores)\n",
        "#         print('Test set AUC: {:.2f}%'.format(100. * self.test_auc))\n",
        "\n",
        "#         print('Finished testing.')\n",
        "\n",
        "#         return inp,lbl\n",
        "\n",
        "#     def init_center_c(self, train_loader: DataLoader, net: BaseNet, eps=0.1):\n",
        "#         \"\"\"Initialize hypersphere center c as the mean from an initial forward pass on the data.\"\"\"\n",
        "\n",
        "#         outs=None\n",
        "#         net.eval()\n",
        "#         isFirst=True\n",
        "#         with torch.no_grad():\n",
        "#             for data in train_loader:\n",
        "#                 inputs, labels, idx = data\n",
        "#                 inputs = inputs.to(self.device)\n",
        "#                 if isFirst:\n",
        "#                   outs = net(inputs).cpu()\n",
        "#                   isFirst=False\n",
        "#                 else:\n",
        "#                   outs = np.append(outs,net(inputs).cpu(),axis=0)\n",
        "\n",
        "        \n",
        "\n",
        "#         from sklearn.cluster import KMeans\n",
        "#         import math\n",
        "#         kmeans = KMeans(n_clusters=3,)\n",
        "#         kmeans.fit(outs)\n",
        "#         c=kmeans.cluster_centers_\n",
        "#         return torch.tensor(c,device=self.device)\n",
        "\n",
        "#     def store_tmp(self,c_epoch,opt,schedul,ae):\n",
        "#       torch.save({'optim_nameR': self.optimizer_name ,\n",
        "#                     'lr': self.lr,\n",
        "#                     'n_epochs': self.n_epochs,\n",
        "#                     'lr_milestones': self.lr_milestones,\n",
        "#                     'batch_size': self.batch_size,\n",
        "#                     'weight_decay': self.weight_decay,\n",
        "#                     'device': self.device,\n",
        "#                     'n_jobs_dataloader': self.n_jobs_dataloader,\n",
        "#                     'current_epoch':c_epoch,\n",
        "#                     'validations':self.validation,\n",
        "#                     'optimaizer': opt.state_dict(),\n",
        "#                     'sched': schedul.state_dict(),\n",
        "#                     'R': self.R,\n",
        "#                     'c': self.c, \n",
        "#                     'ae': ae.state_dict()}, '/content/gdrive/MyDrive/ArshadPeoject/model/tmp/ae-clf.dict')\n",
        "\n",
        "#     def load_and_continue(self,dataset, valid_dataset, ae):\n",
        "#           data =  torch.load('/content/gdrive/MyDrive/ArshadPeoject/model/tmp/ae-clf.dict',map_location=torch.device(self.device))\n",
        "#           train_loader, _ = dataset.loaders(batch_size=self.batch_size, num_workers=self.n_jobs_dataloader)\n",
        "\n",
        "#           self.optimizer_name = data['optim_nameR']\n",
        "#           self.lr =  data['lr']\n",
        "#           # self.n_epochs =  data['n_epochs']\n",
        "#           # self.lr_milestones =  data['lr_milestones']\n",
        "#           self.batch_size =  data['batch_size']\n",
        "#           self.weight_decay =  data['weight_decay']\n",
        "#           # self.device =  data['device']\n",
        "#           self.n_jobs_dataloader =  data['n_jobs_dataloader']\n",
        "#           self.validation = data['validations']\n",
        "\n",
        "#           self.R= data['R']\n",
        "#           self.c= data['c']\n",
        "\n",
        "#           # print(device)\n",
        "#           ae = ae.to(self.device)\n",
        "\n",
        "#           # data['sched']['milestones']=[118,200]\n",
        "#           # data['sched']['last_epoch']=0\n",
        "#           print(data['sched'])\n",
        "#           # print(data['optimaizer'])\n",
        "\n",
        "#           curr_epoch =data['current_epoch']\n",
        "#           print(curr_epoch)\n",
        "#           ae.load_state_dict(data['ae'])\n",
        "\n",
        "\n",
        "#           opt=optim.Adam(ae.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n",
        "#                                   amsgrad=self.optimizer_name == 'amsgrad')\n",
        "\n",
        "#           sche= optim.lr_scheduler.MultiStepLR(opt, milestones=self.lr_milestones, gamma=0.1)\n",
        "        \n",
        "#           opt.load_state_dict(data['optimaizer'])\n",
        "\n",
        "#           sche.load_state_dict(data['sched'])\n",
        "#           tt=optim.lr_scheduler.MultiStepLR(opt, milestones=self.lr_milestones, gamma=0.1)\n",
        "#           sche.milestones=tt.milestones\n",
        "#           print(sche.milestones)\n",
        "#           return self.train_worker(ae,opt,sche,curr_epoch, dataset, valid_dataset,train_loader)\n",
        "\n",
        "#      # Finalizeing\n",
        "#     def genTAM(self, vector):\n",
        "#       l=len(vector)\n",
        "\n",
        "\n",
        "#       d = np.absolute(np.array([vector[0]]))\n",
        "#       # print(d)\n",
        "#       d_t= d.transpose()\n",
        "#       f_d=np.dot(d_t,d)/2\n",
        "#       np.fill_diagonal(f_d,0)\n",
        "#       ttt=np.array([f_d])\n",
        "#       # print(\"kk\", ttt.shape)\n",
        "\n",
        "#       for i in range(1,l):\n",
        "      \n",
        "#         d = np.absolute(np.array([vector[0]]))\n",
        "#         d_t= d.transpose()\n",
        "#         f_d=np.dot(d_t,d)/2\n",
        "#         np.fill_diagonal(f_d,0)\n",
        "#         ttt=np.append(ttt,[f_d],axis=0)\n",
        "#       # print(\"fin\")\n",
        "#       # print(ttt.shape)\n",
        "#       return ttt;\n",
        "\n",
        "#     def calcCov(self, tam, m_tam,cov_mat=None):\n",
        "#       d = len(m_tam)-1 \n",
        "#       cov_d =int((d*(d+1))/2)\n",
        "#       if cov_mat==None:\n",
        "#          cov_mat=np.zeros(shape=(cov_d,cov_d))\n",
        "         \n",
        "#       for j in range(0,d):\n",
        "#         for k in range(0,d):\n",
        "#           if k>=j:\n",
        "#               break\n",
        "#           c=0\n",
        "#           for l in range(0,d):\n",
        "#             for v in range(0,d):\n",
        "#               if v>=l:\n",
        "#                 continue\n",
        "#               cov_mat[c]=self.sigma(tam, m_tam, j+1, k+1, l+1, v+1, g, conv_mat[c]) )\n",
        "#       return cov_mat\n",
        "\n",
        "#     def sigma(self, tam, m_tam, j, k , l, v, g,cov=0):\n",
        "#       # print(j,k,l,v)\n",
        "#       print(tam.shape)\n",
        "#       # print(m_tam.shape)\n",
        "      \n",
        "#       for i in range(0, g):\n",
        "#         cov+=(tam[i][j][k] - m_tam[j][k])*(tam[i][l][v] - m_tam[l][v])/(g-1)\n",
        "#       return cov\n",
        "\n",
        "    \n",
        "#     def md(self, t, cov_inv, m_t, mask):\n",
        "#       tmp=(t - m_t)[mask]\n",
        "#       return np.sqrt(np.matmul(np.matmul(tmp,cov_inv),tmp.T))\n",
        "      \n",
        "       \n",
        "\n",
        "#     def profileGenerator(self, tam, m_tam,cov_mat,md=None):\n",
        "#         # print(tam.shape)\n",
        "#         l= len(tam[0])\n",
        "#         g=len(tam)\n",
        "#         mask = np.zeros(shape=(l,l))\n",
        "#         for i in range(0,l):\n",
        "#           for j in range(0,l):\n",
        "#             if(j<i):\n",
        "#               mask[i][j]=1\n",
        "#             else:\n",
        "#               continue\n",
        "\n",
        "      \n",
        "#         md= np.zeros(l)\n",
        "#         # m_tam = np.sum(tam, axis=0)/g\n",
        "        \n",
        "#         cov_inv= np.linalg.inv(cov_mat)\n",
        "#         for i in range(0,g):\n",
        "#           md[i]=self.md(tam[i], cov_inv, m_tam, mask==1)\n",
        "#         mu=np.average(md);\n",
        "#         s= np.sqrt(np.sum(np.power(md-mu,2)/g-1))\n",
        "#         return mu, s, m_tam, cov_mat,g\n",
        "\n",
        "# def get_radius(dist: torch.Tensor, nu: float):\n",
        "#     \"\"\"Optimally solve for radius R via the (1-nu)-quantile of distances.\"\"\"\n",
        "#     return np.quantile(np.sqrt(dist.clone().data.cpu().numpy()), 1 - nu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSsJgijKgdHL"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXRlkRKzgh4E"
      },
      "outputs": [],
      "source": [
        "def read(base_path):\n",
        "  n_exps = 3\n",
        "  n_seeds = 3\n",
        "\n",
        "  exps = range(n_exps)\n",
        "  seeds = range(1, n_seeds)\n",
        "\n",
        "  for exp in exps:\n",
        "\n",
        "      exp_folder = str(exp) + 'vsall'\n",
        "      aucs = np.zeros(n_seeds, dtype=np.float32)\n",
        "\n",
        "      for seed in seeds:\n",
        "\n",
        "          seed_folder = 'seed_' + str(seed)\n",
        "          file_name = 'results.json'\n",
        "          file_path = base_path + '/' + exp_folder + '/' + seed_folder + '/' + file_name\n",
        "\n",
        "          with open(file_path, 'r') as fp:\n",
        "              results = json.load(fp)\n",
        "\n",
        "          aucs[seed - 1] = results['test_auc']\n",
        "\n",
        "      mean = np.mean(aucs[aucs > 0])\n",
        "      std = np.std(aucs[aucs > 0])\n",
        "\n",
        "      # Write results\n",
        "      log_file = '{}/result.txt'.format(base_path)\n",
        "      log = open(log_file, 'a')\n",
        "      log.write('Experiment: {}\\n'.format(exp_folder))\n",
        "      log.write('Test Set AUC [mean]: {} %\\n'.format(round(float(mean * 100), 4)))\n",
        "      log.write('Test Set AUC [std]: {} %\\n'.format(round(float(std * 100), 4)))\n",
        "      log.write('\\n')\n",
        "\n",
        "  log.write('\\n')\n",
        "  log.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuud9CRcgnB1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    \"\"\"Base class for experimental setting/configuration.\"\"\"\n",
        "\n",
        "    def __init__(self, settings):\n",
        "        self.settings = settings\n",
        "\n",
        "    def load_config(self, import_json):\n",
        "        \"\"\"Load settings dict from import_json (path/filename.json) JSON-file.\"\"\"\n",
        "\n",
        "        with open(import_json, 'r') as fp:\n",
        "            settings = json.load(fp)\n",
        "\n",
        "        for key, value in settings.items():\n",
        "            self.settings[key] = value\n",
        "\n",
        "    def save_config(self, export_json):\n",
        "        \"\"\"Save settings dict to export_json (path/filename.json) JSON-file.\"\"\"\n",
        "\n",
        "        with open(export_json, 'w') as fp:\n",
        "            json.dump(self.settings, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSiaYH1jiRS8"
      },
      "outputs": [],
      "source": [
        "def plot_images_grid(x: torch.tensor, export_img, title: str = '', nrow=8, padding=2, normalize=True, pad_value=0):\n",
        "    \"\"\"Plot 4D Tensor of images of shape (B x C x H x W) as a grid.\"\"\"\n",
        "\n",
        "    grid = make_grid(x, nrow=nrow, padding=padding, normalize=normalize, pad_value=pad_value)\n",
        "    npgrid = grid.cpu().numpy()\n",
        "\n",
        "    plt.imshow(np.transpose(npgrid, (1, 2, 0)), interpolation='nearest')\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.xaxis.set_visible(True)\n",
        "    ax.yaxis.set_visible(True)\n",
        "\n",
        "    if not (title == ''):\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.savefig(export_img, bbox_inches='tight', pad_inches=1)\n",
        "    plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxxF5BTlieyp"
      },
      "source": [
        "# Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3BShsTAik1e"
      },
      "outputs": [],
      "source": [
        "class My_LeNet(BaseNet):\n",
        "\n",
        "    def __init__(self):\n",
        "      \n",
        "        super().__init__()\n",
        "        self.rep_dim = 8\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(520, 512, bias=False)\n",
        "        self.bn1d0 = nn.BatchNorm1d(512, eps=1e-04, affine=False)\n",
        "        self.conv1 = nn.Conv2d(1,2,3, bias=False, padding=1)\n",
        "        self.bn2d1 = nn.BatchNorm2d(2, eps=1e-04, affine=False)\n",
        "        self.conv2 = nn.Conv2d(2,4,3, bias=False, padding=1)\n",
        "        self.bn2d2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.conv3 = nn.Conv2d(4,8,3, bias=False, padding=1)\n",
        "        self.bn2d3 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.conv4 = nn.Conv2d(8,16,3, bias=False, padding=1)\n",
        "        self.bn2d4 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "        self.fc0 = nn.Linear(32, self.rep_dim, bias=False)\n",
        "        self.dropout0 = nn.Dropout(0.25)\n",
        "\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # x=x.unsqueeze(1)\n",
        "        # print(x)\n",
        "        x = self.bn1d0(self.fc1(x))\n",
        "        x\n",
        "        x = x.view(x.size(0), 1,16, 32)\n",
        "        # print(x.size())\n",
        "        # print(x)\n",
        "        # q=2/0\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
        "        x = self.conv4(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d4(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x= self.fc0(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class My_LeNet_Autoencoder(BaseNet):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rep_dim = 8\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(520, 512, bias=False)\n",
        "        self.bn1d0 = nn.BatchNorm1d(512, eps=1e-04, affine=False)\n",
        "        self.dropout0 = nn.Dropout(0.25)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1,2,3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d1 = nn.BatchNorm2d(2, eps=1e-04, affine=False)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(2,4,3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(4,8,3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d3 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(8,16,3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.conv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d4 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "        # self.conv4 = nn.Conv2d(8,16,3, bias=False, padding=1)\n",
        "        # nn.init.xavier_uniform_(self.conv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        # self.bn2d4 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "\n",
        "        self.fc0 = nn.Linear(32, self.rep_dim, bias=False)\n",
        "\n",
        "        self.bn1d1 = nn.BatchNorm1d(self.rep_dim, eps=1e-04, affine=False)\n",
        "        # Encoder (must match the Deep SVDD network above\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(self.rep_dim, 32, bias=False)\n",
        "\n",
        "        self.bn1d2 = nn.BatchNorm1d(32, eps=1e-04, affine=False)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "\n",
        "        self.deconv0 = nn.ConvTranspose2d(16, 8, 3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.deconv0.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d5 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(8, 4, 3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.deconv1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d6 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "\n",
        "        self.deconv2 = nn.ConvTranspose2d(4, 2, 3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.deconv2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d7 = nn.BatchNorm2d(2, eps=1e-04, affine=False)\n",
        "\n",
        "        self.deconv3 = nn.ConvTranspose2d(2, 1, 3, bias=False, padding=1)\n",
        "        nn.init.xavier_uniform_(self.deconv3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        self.bn2d8 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "\n",
        "        # self.deconv4 = nn.ConvTranspose2d(8, 1, 3, bias=False, padding=1)\n",
        "        # nn.init.xavier_uniform_(self.deconv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        # self.bn2d9 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "        self.fc2 = nn.Linear(512, 520, bias=False)\n",
        "\n",
        "        # self.deconv5 = nn.ConvTranspose2d(8, 1, 3, bias=False, padding=1)\n",
        "        # nn.init.xavier_uniform_(self.deconv5.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "        # self.bn2d9 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "        # self.fc2 = nn.Linear(256, 200, bias=False)\n",
        "\n",
        "\n",
        "          # print(x.size())\n",
        "        # x=x.unsqueeze(1)\n",
        "        # print(x.size())\n",
        "    def forward(self, x):\n",
        "        # print(x.size())\n",
        "      \n",
        "        x = self.bn1d0(self.fc1(x))\n",
        "        # print(x.size())\n",
        "        x = x.view(x.size(0), 1,16, 32)\n",
        "        x= self.dropout0(x)\n",
        "\n",
        "        # print(x)\n",
        "        # print(x.size())\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
        "        # print(x.size())\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
        "        # print(x.size())\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2d4(x)))\n",
        "        # print(x.size())\n",
        "        # print(x.size())\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # print(x.size())\n",
        "        # print(x.size())\n",
        "        x= self.bn1d1(self.fc0(x))\n",
        "        # print(x.size())\n",
        "        x= self.dropout1(x)\n",
        "\n",
        "        x= self.bn1d2(self.fc3(x))\n",
        "        x= self.dropout0(x)\n",
        "\n",
        "        # print(x.size())\n",
        "        x = x.view(x.size(0), 16,1,2)\n",
        "\n",
        "        x = self.deconv0(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn2d5(x)), scale_factor=2)\n",
        "        # print(x.size())\n",
        "        x = self.deconv1(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn2d6(x)), scale_factor=2)\n",
        "        # print(x.size())\n",
        "        x = self.deconv2(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn2d7(x)), scale_factor=2)\n",
        "        # print(x.size())\n",
        "        x = self.deconv3(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn2d8(x)), scale_factor=2)\n",
        "        # print(x.size())\n",
        "        # x = self.deconv4(x)\n",
        "        # x = F.interpolate(F.leaky_relu(self.bn2d9(x)), scale_factor=2)\n",
        "        # # x.squeeze(1)\n",
        "        # print(x.size())\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # print(x.size())\n",
        "        x =self.fc2(x)\n",
        "\n",
        "        # print(x.size())\n",
        "        # x = torch.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBGw7i3MPkbO"
      },
      "outputs": [],
      "source": [
        "# class My_LeNet(BaseNet):\n",
        "\n",
        "#     def __init__(self):\n",
        "      \n",
        "#         super().__init__()\n",
        "#         self.rep_dim = 50\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "#         self.fc1 = nn.Linear(200, 256, bias=False)\n",
        "#         self.bn1d1 = nn.BatchNorm1d(240, eps=1e-04, affine=False)\n",
        "#         self.conv1 = nn.Conv2d(1,8,3, bias=False, padding=1)\n",
        "#         self.bn2d1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "#         self.conv2 = nn.Conv2d(8,16,3, bias=False, padding=1)\n",
        "#         self.bn2d2 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "#         self.conv3 = nn.Conv2d(16,32,3, bias=False, padding=1)\n",
        "#         self.bn2d3 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n",
        "#         self.conv4 = nn.Conv2d(32,64,3, bias=False, padding=1)\n",
        "#         self.bn2d4 = nn.BatchNorm2d(64, eps=1e-04, affine=False)\n",
        "#         self.fc0 = nn.Linear(64, self.rep_dim, bias=False)\n",
        "\n",
        "        \n",
        "#     def forward(self, x):\n",
        "        \n",
        "#         x=x.unsqueeze(1)\n",
        "#         # print(x)\n",
        "#         x = self.fc1(x)\n",
        "#         x = x.view(x.size(0), 1,16, 16)\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
        "#         x = self.conv3(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
        "#         x = self.conv4(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d4(x)))\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x= self.fc0(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class My_LeNet_Autoencoder(BaseNet):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.rep_dim = 50\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "#         self.fc1 = nn.Linear(200, 256, bias=False)\n",
        "#         self.bn1d1 = nn.BatchNorm1d(240, eps=1e-04, affine=False)\n",
        "#         self.conv1 = nn.Conv2d(1,8,3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.conv1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.conv2 = nn.Conv2d(8,16,3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.conv2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d2 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.conv3 = nn.Conv2d(16,32,3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.conv3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d3 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.conv4 = nn.Conv2d(32,64,3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.conv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d4 = nn.BatchNorm2d(64, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.fc0 = nn.Linear(64, self.rep_dim, bias=False)\n",
        "\n",
        "#         self.bn1d = nn.BatchNorm1d(self.rep_dim, eps=1e-04, affine=False)\n",
        "#         # Encoder (must match the Deep SVDD network above\n",
        "    \n",
        "\n",
        "#         # Decoder\n",
        "#         self.deconv0 = nn.ConvTranspose2d(50, 32, 3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.deconv0.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d5 = nn.BatchNorm2d(32, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.deconv1 = nn.ConvTranspose2d(32, 16, 3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.deconv1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d6 = nn.BatchNorm2d(16, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.deconv2 = nn.ConvTranspose2d(16, 8, 3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.deconv2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d7 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "\n",
        "#         self.deconv3 = nn.ConvTranspose2d(8, 1, 3, bias=False, padding=1)\n",
        "#         nn.init.xavier_uniform_(self.deconv3.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         self.bn2d8 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "\n",
        "#         # self.deconv4 = nn.ConvTranspose2d(8, 1, 3, bias=False, padding=1)\n",
        "#         # nn.init.xavier_uniform_(self.deconv4.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         # self.bn2d9 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "#         self.fc2 = nn.Linear(256, 200, bias=False)\n",
        "\n",
        "#         # self.deconv5 = nn.ConvTranspose2d(8, 1, 3, bias=False, padding=1)\n",
        "#         # nn.init.xavier_uniform_(self.deconv5.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
        "#         # self.bn2d9 = nn.BatchNorm2d(1, eps=1e-04, affine=False)\n",
        "#         # self.fc2 = nn.Linear(256, 200, bias=False)\n",
        "\n",
        "\n",
        "      \n",
        "#     def forward(self, x):\n",
        "#         x=x.unsqueeze(1)\n",
        "#         # print(x.size())\n",
        "#         x = self.fc1(x)\n",
        "#         # print(x.size())\n",
        "#         x = x.view(x.size(0), 1,16, 16)\n",
        "#         # print(x.size())\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d1(x)))\n",
        "#         # print(x.size())\n",
        "#         x = self.conv2(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d2(x)))\n",
        "#         # print(x.size())\n",
        "#         x = self.conv3(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d3(x)))\n",
        "#         # print(x.size())\n",
        "#         x = self.conv4(x)\n",
        "#         x = self.pool(F.leaky_relu(self.bn2d4(x)))\n",
        "#         # print(x.size())\n",
        "#         # print(x.size())\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         # print(x.size())\n",
        "#         x= self.bn1d(self.fc0(x))\n",
        "#         # print(x.size())\n",
        "#         x = x.view(x.size(0), 50,1,1)\n",
        "#         # print(x.size())\n",
        "#         x = F.leaky_relu(x)\n",
        "#         x = self.deconv0(x)\n",
        "#         x = F.interpolate(F.leaky_relu(self.bn2d5(x)), scale_factor=2)\n",
        "#         # print(x.size())\n",
        "#         x = self.deconv1(x)\n",
        "#         x = F.interpolate(F.leaky_relu(self.bn2d6(x)), scale_factor=2)\n",
        "#         # print(x.size())\n",
        "#         x = self.deconv2(x)\n",
        "#         x = F.interpolate(F.leaky_relu(self.bn2d7(x)), scale_factor=2)\n",
        "#         # print(x.size())\n",
        "#         x = self.deconv3(x)\n",
        "#         x = F.interpolate(F.leaky_relu(self.bn2d8(x)), scale_factor=2)\n",
        "#         # print(x.size())\n",
        "#         # x = self.deconv4(x)\n",
        "#         # x = F.interpolate(F.leaky_relu(self.bn2d9(x)), scale_factor=2)\n",
        "#         # # x.squeeze(1)\n",
        "#         # print(x.size())\n",
        "\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         # print(x.size())\n",
        "#         x =self.fc2(x)\n",
        "\n",
        "#         # print(x.size())\n",
        "#         # x = torch.sigmoid(x)\n",
        "#         return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCA8TyOWfc9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa712a0e-8f4d-4b8d-b401-058f4cee7e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.483314773547883\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.53452248,  0.26726124,  0.80178373])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "x=np.array([-4,2,6])\n",
        "print(np.linalg.norm(x))\n",
        "x/np.linalg.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX75D3oaQbwx"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlZlll3Y63bM"
      },
      "outputs": [],
      "source": [
        "# train_set_path=open(\"/content/gdrive/MyDrive/ArshadPeoject/data/normalTrafficTraining.csv\",\"r\")\n",
        "# reader = csv.reader(train_set_path)\n",
        "# c=0\n",
        "# for row in reader:\n",
        "#    print(row)\n",
        "#    c+=1\n",
        "#    if c==11:\n",
        "#       break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZcK7qT3XLIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8fadd6-701e-4f1c-bdc7-87a05f225a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5 2 5]\n",
            " [4 5 8]]\n"
          ]
        }
      ],
      "source": [
        "arrrr = np.array([[5,2,5],[4,5,8]])\n",
        "\n",
        "# l2norm = np.sqrt((arrrr * arrrr).sum(axis=1))\n",
        "# arrrr=np.nan_to_num(arrrr / l2norm.reshape(2,1))\n",
        "# min = arrrr.min(axis=1)\n",
        "# min = np.array([min]).transpose()\n",
        "# max = arrrr.max(axis=1)\n",
        "# max = np.array([max]).transpose()\n",
        "# r=max-min\n",
        "# r=np.where(r==0,1,r)\n",
        "# arrrr= np.subtract(arrrr,min)\n",
        "# arrrr=np.divide(arrrr,r)\n",
        "print(arrrr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEqbHlnkxOc7"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QVY2sudQgm3"
      },
      "outputs": [],
      "source": [
        "class HttpDataset(Dataset):\n",
        "    import numpy as np\n",
        "    import math\n",
        "    def __init__(self, train, validate=False, useAnomals=True, containanomalOnTrain= False):\n",
        "        self.cols=520\n",
        "        train_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/normalTrafficTraining.csv\"\n",
        "        # train_anomal_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/anormal_train.csv\"\n",
        "\n",
        "        test_normal_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/normal_test.csv\"\n",
        "        validation_normal_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/normal_valid.csv\"\n",
        "\n",
        "        test_anormal_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/anomalous_test.csv\"\n",
        "        validation_anormal_set_path=\"/content/gdrive/MyDrive/ArshadPeoject/data/anomalous_valid.csv\"\n",
        "\n",
        "        self.idf = self.idf_counter(train_set_path,self.cols)\n",
        "\n",
        "        self.transform =transforms.ToTensor();\n",
        "        self.target_transform =  transforms.Lambda(lambda x: int(x in self.outlier_classes))\n",
        "        if not train:\n",
        "        \n",
        "          self.anomal_data_test = pd.read_csv(test_anormal_set_path)\n",
        "          lbldf = pd.DataFrame([1]*len(self.anomal_data_test),columns=['label'])\n",
        "          self.anomal_data_test=pd.concat([self.anomal_data_test,lbldf], axis=1)\n",
        "          self.anomal_data_test=self.anomal_data_test.apply(pd.to_numeric,downcast=\"float\")\n",
        "\n",
        "\n",
        "          self.anomal_data_valid = pd.read_csv(validation_anormal_set_path)\n",
        "          lbldf = pd.DataFrame([1]*len(self.anomal_data_valid),columns=['label'])\n",
        "          self.anomal_data_valid=pd.concat([self.anomal_data_valid,lbldf], axis=1)\n",
        "          self.anomal_data_valid=self.anomal_data_valid.apply(pd.to_numeric,downcast=\"float\")\n",
        "\n",
        "          self.normal_data_test = pd.read_csv(test_normal_set_path)\n",
        "          lbldf = pd.DataFrame([0]*len(self.normal_data_test),columns=['label'])\n",
        "          self.normal_data_test=pd.concat([ self.normal_data_test,lbldf], axis=1)\n",
        "          self.normal_data_test=self.normal_data_test.apply(pd.to_numeric,downcast=\"float\")\n",
        "\n",
        "          self.normal_data_valid = pd.read_csv(validation_normal_set_path)\n",
        "          lbldf = pd.DataFrame([0]*len(self.normal_data_valid),columns=['label'])\n",
        "          self.normal_data_valid=pd.concat([ self.normal_data_valid,lbldf], axis=1)\n",
        "          self.normal_data_valid=self.normal_data_valid.apply(pd.to_numeric,downcast=\"float\")\n",
        "\n",
        "\n",
        "        self.useAnomals=useAnomals\n",
        "        self.validate=validate\n",
        "        self.train=train\n",
        "        self.caInt = False\n",
        "        self.data = pd.read_csv(train_set_path)\n",
        "        lbldf = pd.DataFrame([0]*len(self.data),columns=['label'])\n",
        "        self.data=pd.concat([self.data,lbldf], axis=1)\n",
        "        self.data =self.data.apply(pd.to_numeric,downcast=\"float\")\n",
        "\n",
        "        # self.anomalTrainData = pd.read_csv(train_anomal_set_path)\n",
        "        # lbldf = pd.DataFrame([0]*len(self.anomalTrainData),columns=['label'])\n",
        "        # self.anomalTrainData=pd.concat([self.anomalTrainData,lbldf], axis=1)\n",
        "        # self.anomalTrainData =self.anomalTrainData.apply(pd.to_numeric,downcast=\"float\")\n",
        "        self.filtered_data = self.data\n",
        "    \n",
        "    def idf_counter(self , path, col):\n",
        "\n",
        "      normal_file = open(path, \"r\")\n",
        "      reader = csv.reader(normal_file)\n",
        "      c=0\n",
        "      occInDoc = np.array([0.0]*col)\n",
        "      isFirst=False\n",
        "      for row in reader:\n",
        "        if not isFirst:\n",
        "          isFirst=True\n",
        "        else:\n",
        "          c+=1\n",
        "          for i in range(0,col):\n",
        "            if int(row[i])>0 :\n",
        "              occInDoc[i]+=1;\n",
        "\n",
        "      for i in range(0,col):\n",
        "        if occInDoc[i]==0:\n",
        "          occInDoc[i]=math.log(c);\n",
        "        else:\n",
        "          if(occInDoc[i]==c):\n",
        "            occInDoc[i]-=1\n",
        "          occInDoc[i] = math.log(c/occInDoc[i])\n",
        "      return occInDoc\n",
        "    \n",
        "    def get_idf(self):\n",
        "      return self.idf\n",
        "\n",
        "    def __len__(self):\n",
        "      if self.train :\n",
        "        if self.caInt:\n",
        "          return len(self.filtered_data) + len (self.anomalTrainData)\n",
        "        else:\n",
        "          return len(self.filtered_data)\n",
        "\n",
        "      else:\n",
        "        if self.validate:\n",
        "          if(self.useAnomals):\n",
        "            return len(self.normal_data_valid)+len(self.anomal_data_valid)\n",
        "          else:\n",
        "            return len(self.normal_data_valid)\n",
        "        else:\n",
        "          if(self.useAnomals):\n",
        "            return len(self.normal_data_test)+len(self.anomal_data_test)\n",
        "          else:\n",
        "            return len(self.normal_data_test)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(\"idx=\",idx)\n",
        "        label=0\n",
        "        if self.train:\n",
        "          if self.caInt and idx>=len(self.data):\n",
        "            i=idx-len(self.data)\n",
        "            data = self.anomalTrainData.iloc[i,:self.cols].to_numpy()\n",
        "          else:\n",
        "            data = self.filtered_data.iloc[idx,:self.cols].to_numpy()\n",
        "        else:\n",
        "          if self.validate:\n",
        "              len_d=len(self.normal_data_valid)\n",
        "              if self.useAnomals:\n",
        "                if(idx >= len_d):\n",
        "                  i=idx-len_d\n",
        "                  # print(\"i=\",str(i),\"  len=\",str(len(anomal_data)))\n",
        "                  data = self.anomal_data_valid.iloc[i,:self.cols].to_numpy()\n",
        "                  label=1\n",
        "                else:\n",
        "                  data = self.normal_data_valid.iloc[idx,:self.cols].to_numpy()\n",
        "              else:\n",
        "                data = self.normal_data_valid.iloc[idx,:self.cols].to_numpy()\n",
        "          else:\n",
        "            if self.useAnomals:\n",
        "              len_d=len(self.normal_data_test)\n",
        "              if(idx >= len_d):\n",
        "                i=idx-len_d\n",
        "                # print(\"i=\",str(i),\"  len=\",str(len(anomal_data)))\n",
        "                data = self.anomal_data_test.iloc[i,:self.cols].to_numpy()\n",
        "                label=1\n",
        "              else:\n",
        "                data = self.normal_data_test.iloc[idx,:self.cols].to_numpy() \n",
        "            else:\n",
        "              data = self.normal_data_test.iloc[idx,:self.cols].to_numpy()\n",
        "           \n",
        "  \n",
        "        data =(data/np.sum(data))*self.idf\n",
        "        data= np.float32(data)\n",
        "       \n",
        "        min = np.min(data)\n",
        "        max = np.max(data)\n",
        "        r= max - min\n",
        "\n",
        "        # print(\"ortg, \", data)\n",
        "        data= (data-min)/r\n",
        "        # print(data)\n",
        "        # print(idx, avg , \", \",data)\n",
        "        return  data, label,idx\n",
        "    \n",
        "    def filter_data(self, indexes):\n",
        "      self.filtered_data= self.data.iloc[indexes]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4K_2mPbbL-y"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QAplBeFXQAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f6e0b8-f24d-4242-aa42-f892d6d3ac52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np \n",
        "\n",
        "t=np.array([[1,2,3,4]])\n",
        "t_trans = t.transpose()\n",
        "# print(t_trans)\n",
        "# print(t)\n",
        "r = np.dot(t_trans,t)\n",
        "np.fill_diagonal(r,0)\n",
        "# print(r)r\n",
        "\n",
        "# print(type(np.array(r)))\n",
        "print(r.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YwBhDLW7OsW"
      },
      "outputs": [],
      "source": [
        "from pandas._libs.lib import to_object_array_tuples\n",
        "class My_Dataset(TorchvisionDataset):\n",
        "\n",
        "    def __init__(self, root: str, normal_class=5):\n",
        "        super().__init__(root)\n",
        "\n",
        "        self.n_classes = 1  # 0: normal, 1: outlier\n",
        "        # self.normal_classes = tuple([normal_class])\n",
        "        # self.outlier_classes = list(range(0, 10))\n",
        "        # self.outlier_classes.remove(normal_class)\n",
        "\n",
        "        # # Pre-computed min and max values (after applying GCN) from train data per class\n",
        "        # min_max = [(-28.94083453598571, 13.802961825439636),\n",
        "        #            (-6.681770233365245, 9.158067708230273),\n",
        "        #            (-34.924463588638204, 14.419298165027628),\n",
        "        #            (-10.599172931391799, 11.093187820377565),\n",
        "        #            (-11.945022995801637, 10.628045447867583),\n",
        "        #            (-9.691969487694928, 8.948326776180823),\n",
        "        #            (-9.174940012342555, 13.847014686472365),\n",
        "        #            (-6.876682005899029, 12.282371383343161),\n",
        "        #            (-15.603507135507172, 15.2464923804279),\n",
        "        #            (-6.132882973622672, 8.046098172351265)]\n",
        "\n",
        "        # # CIFAR-10 preprocessing: GCN (with L1 norm) and min-max feature scaling to [0,1]\n",
        "        # transform = transforms.Compose([transforms.ToTensor(),\n",
        "        #                                 transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1')),\n",
        "        #                                 transforms.Normalize([min_max[normal_class][0]] * 3,\n",
        "        #                                                      [min_max[normal_class][1] - min_max[normal_class][0]] * 3)])\n",
        "\n",
        "        # target_transform = transforms.Lambda(lambda x: int(x in self.outlier_classes))\n",
        "\n",
        "        # train_set = \n",
        "        # # Subset train set to normal class\n",
        "        # train_idx_normal = get_target_label_idx(train_set.train_labels, self.normal_classes)\n",
        "        self.train_set = HttpDataset(train=True)\n",
        "        self.test_set = HttpDataset(train=False,validate=True)\n",
        "\n",
        "    def filter_data(self,idx):\n",
        "      self.train_set.filter_data(idx)\n",
        "\n",
        "\n",
        "class My_Dataset_cross(TorchvisionDataset):\n",
        "\n",
        "    def __init__(self, root: str, normal_class=5,anomal=False):\n",
        "        super().__init__(root)\n",
        "\n",
        "        self.n_classes = 1  # 0: normal, 1: outlier\n",
        "        # self.normal_classes = tuple([normal_class])\n",
        "        # self.outlier_classes = list(range(0, 10))\n",
        "        # self.outlier_classes.remove(normal_class)\n",
        "\n",
        "        # # Pre-computed min and max values (after applying GCN) from train data per class\n",
        "        # min_max = [(-28.94083453598571, 13.802961825439636),\n",
        "        #            (-6.681770233365245, 9.158067708230273),\n",
        "        #            (-34.924463588638204, 14.419298165027628),\n",
        "        #            (-10.599172931391799, 11.093187820377565),\n",
        "        #            (-11.945022995801637, 10.628045447867583),\n",
        "        #            (-9.691969487694928, 8.948326776180823),\n",
        "        #            (-9.174940012342555, 13.847014686472365),\n",
        "        #            (-6.876682005899029, 12.282371383343161),\n",
        "        #            (-15.603507135507172, 15.2464923804279),\n",
        "        #            (-6.132882973622672, 8.046098172351265)]\n",
        "\n",
        "        # # CIFAR-10 preprocessing: GCN (with L1 norm) and min-max feature scaling to [0,1]\n",
        "        # transform = transforms.Compose([transforms.ToTensor(),\n",
        "        #                                 transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1')),\n",
        "        #                                 transforms.Normalize([min_max[normal_class][0]] * 3,\n",
        "        #                                                      [min_max[normal_class][1] - min_max[normal_class][0]] * 3)])\n",
        "\n",
        "        # target_transform = transforms.Lambda(lambda x: int(x in self.outlier_classes))\n",
        "\n",
        "        # train_set = \n",
        "        # # Subset train set to normal class\n",
        "        # train_idx_normal = get_target_label_idx(train_set.train_labels, self.normal_classes)\n",
        "        self.train_set = HttpDataset(train=True)\n",
        "        self.test_set = HttpDataset(train=False,validate=True)\n",
        "\n",
        "    def filter_data(self,idx):\n",
        "      self.train_set.filter_data(idx)\n",
        "class My_Dataset_preTrain(TorchvisionDataset):\n",
        "\n",
        "    def __init__(self, root: str, normal_class=5, anomalInTrain=False):\n",
        "        super().__init__(root)\n",
        "\n",
        "        self.n_classes = 1  # 0: normal, 1: outlier\n",
        "        # self.normal_classes = tuple([normal_class])\n",
        "        # self.outlier_classes = list(range(0, 10))\n",
        "        # self.outlier_classes.remove(normal_class)\n",
        "\n",
        "        # # Pre-computed min and max values (after applying GCN) from train data per class\n",
        "        # min_max = [(-28.94083453598571, 13.802961825439636),\n",
        "        #            (-6.681770233365245, 9.158067708230273),\n",
        "        #            (-34.924463588638204, 14.419298165027628),\n",
        "        #            (-10.599172931391799, 11.093187820377565),\n",
        "        #            (-11.945022995801637, 10.628045447867583),\n",
        "        #            (-9.691969487694928, 8.948326776180823),\n",
        "        #            (-9.174940012342555, 13.847014686472365),\n",
        "        #           (device)\n",
        "    # F (-6.876682005899029, 12.282371383343161),\n",
        "        #            (-15.603507135507172, 15.2464923804279),\n",
        "        #            (-6.132882973622672, 8.046098172351265)]\n",
        "\n",
        "        # # CIFAR-10 preprocessing: GCN (with L1 norm) and min-max feature scaling to [0,1]\n",
        "        # transform = transforms.Compose([transforms.ToTensor(),\n",
        "        #                                 transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1')),\n",
        "        #                                 transforms.Normalize([min_max[normal_class][0]] * 3,\n",
        "        #                                                      [min_max[normal_class][1] - min_max[normal_class][0]] * 3)])\n",
        "\n",
        "        # target_transform = transforms.Lambda(lambda x: int(x in self.outlier_classes))\n",
        "\n",
        "        # train_set = \n",
        "        # # Subset train set to normal class\n",
        "        # train_idx_normal = get_target_label_idx(train_set.train_labels, self.normal_classes)\n",
        "        self.train_set = HttpDataset(train=True,containanomalOnTrain=anomalInTrain)\n",
        "        self.test_set = HttpDataset(train=False,useAnomals=True)\n",
        "        # self.validation_set =\n",
        "\n",
        "\n",
        "    def filter_data(self,idx):\n",
        "      self.train_set.filter_data(idx)\n",
        "class My_Dataset_Test(TorchvisionDataset):\n",
        "\n",
        "    def __init__(self, root: str, normal_class=5):\n",
        "        super().__init__(root)\n",
        "        self.n_classes = 1  # 0: normal, 1: outlier\n",
        "        # self.normal_classes = tuple([normal_class])\n",
        "        # self.outlier_classes = list(range(0, 10))\n",
        "        # self.outlier_classes.remove(normal_class)\n",
        "\n",
        "        # # Pre-computed min and max values (after applying GCN) from train data per class\n",
        "        # min_max = [(-28.94083453598571, 13.802961825439636),\n",
        "        #            (-6.681770233365245, 9.158067708230273),\n",
        "        #            (-34.924463588638204, 14.419298165027628),\n",
        "        #            (-10.599172931391799, 11.093187820377565),\n",
        "        #            (-11.945022995801637, 10.628045447867583),\n",
        "        #            (-9.691969487694928, 8.948326776180823),\n",
        "        #            (-9.174940012342555, 13.847014686472365),\n",
        "        #            (-6.876682005899029, 12.282371383343161),\n",
        "        #            (-15.603507135507172, 15.2464923804279),\n",
        "        #            (-6.132882973622672, 8.046098172351265)]\n",
        "\n",
        "        # # CIFAR-10 preprocessing: GCN (with L1 norm) and min-max feature scaling to [0,1]\n",
        "        # transform = transforms.Compose([transforms.ToTensor(),\n",
        "        #                                 transforms.Lambda(lambda x: global_contrast_normalization(x, scale='l1')),\n",
        "        #                                 transforms.Normalize([min_max[normal_class][0]] * 3,\n",
        "        #                                                      [min_max[normal_class][1] - min_max[normal_class][0]] * 3)])\n",
        "\n",
        "        # target_transform = transforms.Lambda(lambda x: int(x in self.outlier_classes))\n",
        "\n",
        "        # train_set = \n",
        "        # # Subset train set to normal class\n",
        "        # train_idx_normal = get_target_label_idx(train_set.train_labels, self.normal_classes)\n",
        "        self.train_set = HttpDataset(train=True)\n",
        "        self.test_set = HttpDataset(train=False)\n",
        "        # self.validation_set =\n",
        "\n",
        "\n",
        "    def filter_data(self,idx):\n",
        "      self.train_set.filter_data(idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chZMmVW9dtov"
      },
      "source": [
        "# Deep-SVDD Net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VxRPjafkyhT"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAsiJliMV5Dz"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWV_6mtft2_N"
      },
      "outputs": [],
      "source": [
        "# dataset_p=My_Dataset_preTrain(root=\"ll\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_3MsznRclM2"
      },
      "outputs": [],
      "source": [
        "# d, _= dataset_p.loaders(batch_size=1)\n",
        "# for i in d:\n",
        "#   print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_baDdBkd3um"
      },
      "outputs": [],
      "source": [
        "class DeepSVDD(object):\n",
        "    \"\"\"A class for the Deep SVDD method.\n",
        "        objective: A string specifying the Deep SVDD objective (either 'one-class' or 'soft-boundary').\n",
        "        nu: Deep SVDD hyperparameter nu (must be 0 < nu <= 1).\n",
        "        R: Hypersphere radius R.\n",
        "        c: Hypersphere center c.\n",
        "        net_name: A string indicating the name of the neural network to use.\n",
        "        net: The neural network \\phi.\n",
        "        ae_net: The autoencoder network corresponding to \\phi for network weights pretraining.\n",
        "        trainer: DeepSVDDTrainer to train a Deep SVDD model.\n",
        "        optimizer_name: A string indicating the optimizer to use for training the Deep SVDD network.\n",
        "        ae_trainer: AETrainer to train an autoencoder in pretraining.\n",
        "        ae_optimizer_name: A string indicating the optimizer to use for pretraining the autoencoder.\n",
        "        results: A dictionary to save the results.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, objective: str = 'one-class', nu: float = 0.1):\n",
        "        \"\"\"Inits DeepSVDD with one of the two objectives and hyperparameter nu.\"\"\"\n",
        "\n",
        "        assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n",
        "        self.objective = objective\n",
        "        assert (0 < nu) & (nu <= 1), \"For hyperparameter nu, it must hold: 0 < nu <= 1.\"\n",
        "        self.nu = nu\n",
        "        self.R = 0.0  # hypersphere radius R\n",
        "        self.c = None  # hypersphere center c\n",
        "\n",
        "        self.net_name = None\n",
        "        self.net = None  # neural network \\phi\n",
        "\n",
        "        self.trainer = None\n",
        "        self.optimizer_name = None\n",
        "\n",
        "        self.ae_net = None  # autoencoder network for pretraining\n",
        "        self.ae_trainer = None\n",
        "        self.ae_optimizer_name = None\n",
        "\n",
        "        self.results = {\n",
        "            'train_time': None,\n",
        "            'test_auc': None,\n",
        "            'test_time': None,\n",
        "            'test_scores': None,\n",
        "        }\n",
        "\n",
        "    def set_network(self, net_name):\n",
        "        \"\"\"Builds the neural network \\phi.\"\"\"\n",
        "        # self.net_name = net_name\n",
        "        self.net = My_LeNet()\n",
        "\n",
        "    def train(self, dataset: BaseADDataset, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 50,\n",
        "              lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "              n_jobs_dataloader: int = 0):\n",
        "        \"\"\"Trains the Deep SVDD model on the training data.\"\"\"\n",
        "\n",
        "        self.optimizer_name = optimizer_name\n",
        "        self.trainer = DeepSVDDTrainer(self.objective, self.R, self.c, self.nu, optimizer_name, lr=lr,\n",
        "                                       n_epochs=n_epochs, lr_milestones=lr_milestones, batch_size=batch_size,\n",
        "                                       weight_decay=weight_decay, device=device, n_jobs_dataloader=n_jobs_dataloader)\n",
        "        # Get the model\n",
        "        self.net = self.trainer.train(dataset, self.net)\n",
        "        self.R = float(self.trainer.R.cpu().data.numpy())  # get float\n",
        "        self.c = self.trainer.c.cpu().data.numpy().tolist()  # get list\n",
        "        self.results['train_time'] = self.trainer.train_time\n",
        "\n",
        "    def test(self, dataset: BaseADDataset, device: str = 'cuda', n_jobs_dataloader: int = 0):\n",
        "        \"\"\"Tests the Deep SVDD model on the test data.\"\"\"\n",
        "\n",
        "        if self.trainer is None:\n",
        "            self.trainer = DeepSVDDTrainer(self.objective, self.R, self.c, self.nu,\n",
        "                                           device=device, n_jobs_dataloader=n_jobs_dataloader)\n",
        "\n",
        "        self.trainer.test(dataset, self.net)\n",
        "        # Get results\n",
        "        self.results['test_auc'] = self.trainer.test_auc\n",
        "        self.results['test_time'] = self.trainer.test_time\n",
        "        self.results['test_scores'] = self.trainer.test_scores\n",
        "\n",
        "    def pretrain(self, dataset: BaseADDataset, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs: int = 100,\n",
        "                 lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "                 n_jobs_dataloader: int = 0):\n",
        "        \"\"\"Pretrains the weights for the Deep SVDD network \\phi via autoencoder.\"\"\"\n",
        "\n",
        "        self.ae_net =My_LeNet_Autoencoder()\n",
        "        self.ae_optimizer_name = optimizer_name\n",
        "        self.ae_trainer = AETrainer(optimizer_name, lr=lr, n_epochs=n_epochs, lr_milestones=lr_milestones,\n",
        "                                    batch_size=batch_size, weight_decay=weight_decay, device=device,\n",
        "                                    n_jobs_dataloader=n_jobs_dataloader)\n",
        "        self.ae_net = self.ae_trainer.train(dataset, self.ae_net)\n",
        "        self.ae_trainer.test(dataset, self.ae_net)\n",
        "        self.init_network_weights_from_pretraining()\n",
        "\n",
        "    def init_network_weights_from_pretraining(self):\n",
        "        \"\"\"Initialize the Deep SVDD network weights from the encoder weights of the pretraining autoencoder.\"\"\"\n",
        "\n",
        "        net_dict = self.net.state_dict()\n",
        "        ae_net_dict = self.ae_net.state_dict()\n",
        "\n",
        "        # Filter out decoder network keys\n",
        "        ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n",
        "        # Overwrite values in the existing state_dict\n",
        "        net_dict.update(ae_net_dict)\n",
        "        # Load the new state_dict\n",
        "        self.net.load_state_dict(net_dict)\n",
        "\n",
        "    def save_model(self, export_model, save_ae=True):\n",
        "        \"\"\"Save Deep SVDD model to export_model.\"\"\"\n",
        "\n",
        "        net_dict = self.net.state_dict()\n",
        "        ae_net_dict = self.ae_net.state_dict() if save_ae else None\n",
        "\n",
        "        torch.save({'R': self.R,\n",
        "                    'c': self.c,\n",
        "                    'net_dict': net_dict,\n",
        "                    'ae_net_dict': ae_net_dict}, export_model)\n",
        "\n",
        "    def load_model(self, model_path, load_ae=False):\n",
        "        \"\"\"Load Deep SVDD model from model_path.\"\"\"\n",
        "\n",
        "        model_dict = torch.load(model_path)\n",
        "\n",
        "        self.R = model_dict['R']\n",
        "        self.c = model_dict['c']\n",
        "        self.net.load_state_dict(model_dict['net_dict'])\n",
        "        if load_ae:\n",
        "            if self.ae_net is None:\n",
        "                self.ae_net = My_LeNet_Autoencoder()\n",
        "            self.ae_net.load_state_dict(model_dict['ae_net_dict'])\n",
        "\n",
        "    def save_results(self, export_json):\n",
        "        \"\"\"Save results dict to a JSON-file.\"\"\"\n",
        "        with open(export_json, 'w') as fp:\n",
        "            json.dump(self.results, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "NocuYQUP0vWL"
      },
      "outputs": [],
      "source": [
        "class MSVDD(object):\n",
        "  def __init__(self, objective: str = 'one-class', nu: float = 0.1):\n",
        "    self.pre_trained=False;\n",
        "    assert objective in ('one-class', 'soft-boundary'), \"Objective must be either 'one-class' or 'soft-boundary'.\"\n",
        "    \n",
        "    self.objective = objective\n",
        "    assert (0 < nu) & (nu <= 1), \"For hyperparameter nu, it must hold: 0 < nu <= 1.\"\n",
        "    self.nu = nu\n",
        "    self.R = 0.0  # hypersphere radius R\n",
        "    self.c = None  # hypersphere center c\n",
        "\n",
        "    self.c_count =1;\n",
        "    self.net_name = None\n",
        "    self.net = None  # neural network \\phi\n",
        "# \n",
        "    self.trainer = None\n",
        "    self.optimizer_name = None\n",
        "\n",
        "    self.ae_net = None  # autoencoder network for pretraining\n",
        "    self.ae_trainer = None\n",
        "    self.ae_optimizer_name = None\n",
        "\n",
        "    self.results = {\n",
        "        'train_time': None,\n",
        "        'test_auc': None,\n",
        "        'test_time': None,\n",
        "        'test_scores': None,\n",
        "    }\n",
        "\n",
        "  \n",
        "  def train(self, dataset: BaseADDataset,valid_dataset: BaseADDataset, k, optimizer_name: str = 'adam', lr: float = 0.001, n_epochs=[50],\n",
        "          lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "          n_jobs_dataloader: int = 0, load=False,resume=False):\n",
        "    assert self.pre_trained, \"Must preTrain first!\"\n",
        "    self.batch_size=batch_size\n",
        "    self.net= My_LeNet()\n",
        "    net_dict = self.net.state_dict()\n",
        "    ae_net_dict = self.ae_net.state_dict()\n",
        "    self.net = self.net.to(device)\n",
        "    # Filter out decoder network keys\n",
        "    ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n",
        "    # Overwrite values in the existing state_dict\n",
        "    net_dict.update(ae_net_dict)\n",
        "    # Loadwn the new state_dict\n",
        "    self.net.load_state_dict(net_dict)\n",
        "    train_loader, _ = dataset.loaders(batch_size=self.batch_size)\n",
        "    outs=None\n",
        "\n",
        "    # centers = init_c(k,dataset,self.net)\n",
        "\n",
        "\n",
        "    \"\"\"Trains the Deep SVDD model on the training data.\"\"\"\n",
        "    self.optimizer_name = optimizer_name\n",
        "    self.trainer= list()\n",
        "    self.filtered = list()\n",
        "    self.worker_net = list()\n",
        "    self.valid =list()\n",
        "    self.centers =list()\n",
        "    self.profile = list()\n",
        "   \n",
        "    \n",
        "    if(load):\n",
        "      self.load_clf(k,lr,n_epochs,lr_milestones,batch_size,weight_decay,device,n_jobs_dataloader)\n",
        "    else:\n",
        "      self.net.eval()\n",
        "      isFirst=True\n",
        "      with torch.no_grad():\n",
        "          for data in train_loader:\n",
        "              inputs, labels, idx = data\n",
        "              inputs = inputs.to(device)\n",
        "              if isFirst:\n",
        "                outs = self.net(inputs).cpu()\n",
        "                isFirst=False\n",
        "              else:\n",
        "                outs = np.append(outs,self.net(inputs).cpu(),axis=0)\n",
        "      data_idx= np.arange(0,len(outs))\n",
        "      from sklearn.cluster import KMeans\n",
        "      import math\n",
        "      kmeans = KMeans(n_clusters=k)\n",
        "      predict = kmeans.fit_predict(outs)\n",
        "      self.centers=kmeans.cluster_centers_\n",
        "      cont=0\n",
        "      for center in self.centers:\n",
        "        cnz=np.count_nonzero(predict==cont)\n",
        "        ep=200\n",
        "        # if cnz>10000:\n",
        "        #   ep=40\n",
        "        # elif cnz>5000:\n",
        "        #   ep=30\n",
        "        # elif cnz>600:\n",
        "        #   ep=20\n",
        "        # else:\n",
        "        #   ep=10\n",
        "        \n",
        "        self.trainer.append(DeepSVDDTrainer(self.objective, self.R, center, self.nu, optimizer_name, lr=lr,\n",
        "                                      n_epochs=ep, lr_milestones=lr_milestones, batch_size=batch_size,\n",
        "                                      weight_decay=weight_decay, device=device, n_jobs_dataloader=n_jobs_dataloader))\n",
        "        cont+=1\n",
        "\n",
        "      for dd in range (0,len(self.trainer)):\n",
        "        self.filtered.append(data_idx[predict==(dd)])\n",
        "      print(self.centers)\n",
        "      print(self.filtered)\n",
        "      torch.save({\n",
        "          'c_count': self.c_count+1,\n",
        "          'filtered': self.filtered,\n",
        "          'centers': self.centers\n",
        "      }, '/content/gdrive/MyDrive/ArshadPeoject/model/clf_0.dict')\n",
        "      \n",
        "    return self.train_worker(dataset,valid_dataset, resume)\n",
        "      \n",
        "  def train_worker(self,dataset,valid_dataset,resume):\n",
        "    print(\"pp\",self.valid)\n",
        "    print(\"tt\",len(self.trainer))\n",
        "    time=0\n",
        "    for cls_conter in range(self.c_count,len(self.trainer)+1):\n",
        "      wn= My_LeNet()\n",
        "      net_dict = wn.state_dict()\n",
        "      ae_net_dict = self.ae_net.state_dict()\n",
        "      # Filter out decoder network keys\n",
        "      ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n",
        "      # Overwrite values in the existing state_dict\n",
        "      net_dict.update(ae_net_dict)\n",
        "      # Loadwn the new state_dict\n",
        "      wn.load_state_dict(net_dict)\n",
        "      print(\"training net:\", cls_conter)\n",
        "      dataset.filter_data(self.filtered[cls_conter-1])\n",
        "      if resume:\n",
        "        wn, valid=self.trainer[cls_conter-1].load_and_continue(dataset, valid_dataset, wn)\n",
        "        resume=False\n",
        "      else:\n",
        "        wn, valid=self.trainer[cls_conter-1].train(dataset, valid_dataset, wn)\n",
        "      self.worker_net.append(wn)\n",
        "      # self.valid.append(valid)\n",
        "      R = float(self.trainer[cls_conter-1].R.cpu().data.numpy())  # get float\n",
        "      c = self.trainer[cls_conter-1].c.cpu().data.numpy().tolist()  # get list\n",
        "      torch.save({\n",
        "          'c_count': cls_conter+1,\n",
        "          'filtered': self.filtered,\n",
        "          'centers': self.centers\n",
        "      }, '/content/gdrive/MyDrive/ArshadPeoject/model/clf_0.dict')\n",
        "      self.save_clf_model(wn,R,c,valid,'/content/gdrive/MyDrive/ArshadPeoject/model/clf.dict.'+str(cls_conter))\n",
        "      # time+=t.train_time\n",
        "      # cls_conter+=1\n",
        "    print(self.valid)\n",
        "    self.results['train_time'] = time\n",
        "    return self.valid\n",
        "\n",
        "  def update_profile(self, dataset: BaseADDataset):\n",
        "     cls_conter=1\n",
        "     for t in self.trainer:\n",
        "       dataset.filter_data(self.filtered[cls_conter-1])\n",
        "       profile = t.update_profile(dataset,self.worker_net[cls_conter-1])\n",
        "       self.profile.append(profile)\n",
        "       cls_conter+=1\n",
        "     return self.profile\n",
        "\n",
        "\n",
        "  def test(self, dataset: BaseADDataset, device: str = 'cuda', n_jobs_dataloader: int = 0):\n",
        "      \"\"\"Tests the Deep SVDD model on the test data.\"\"\"\n",
        "      scores = list()\n",
        "      inp= list()\n",
        "      cls_conter=1\n",
        "      for t in self.trainer:\n",
        "        print(\"testing net:\", cls_conter)\n",
        "        inpt, lblt = t.test(dataset, self.worker_net[cls_conter-1])\n",
        "        inp.append(inpt)\n",
        "        scores.append(t.test_scores)\n",
        "        cls_conter+=1\n",
        "      indices=None\n",
        "      labels=None\n",
        "      min_score=None\n",
        "      for s in scores:\n",
        "        indices, labels, x, outputs= zip(*s)\n",
        "        indices, labels, x = np.array(indices), np.array(labels), np.array(x)\n",
        "        if min_score==None :\n",
        "          min_score=torch.tensor(np.array([float('inf')]*len(indices)))\n",
        "        min_score=torch.min(min_score, torch.tensor(x))\n",
        "        # print(indices)\n",
        "      return indices, labels, min_score ,inp, lblt\n",
        "      \n",
        "\n",
        "\n",
        "      # if self.trainer is None:\n",
        "      #     self.trainer = DeepSVDDTrainer(self.objective, self.R, self.c, self.nu,\n",
        "      #                                     device=device, n_jobs_dataloader=n_jobs_dataloader)\n",
        "\n",
        "  \n",
        "      # # Get results\n",
        "      # self.results['test_auc'] = self.trainer.test_auc\n",
        "      # self.results['test_time'] = self.trainer.test_time\n",
        "      # self.results['test_scores'] = self.trainer.test_scores   \n",
        "\n",
        "  def save_model(self, export_model, save_ae=True):\n",
        "      \"\"\"Save Deep SVDD model to export_model.\"\"\"\n",
        "\n",
        "      ae_net_dict = self.ae_net.state_dict() if save_ae else None\n",
        "\n",
        "      torch.save({'ae_net_dict': ae_net_dict}, export_model)\n",
        "\n",
        "  def save_clf_model(self,wn,R,C,valid, export_model, save_ae=True):\n",
        "      \"\"\"Save Deep SVDD model to export_model.\"\"\"\n",
        "      torch.save({'R': R,\n",
        "                  'c': C,\n",
        "                  'valid':valid,\n",
        "                  'net': wn.state_dict()}, export_model)\n",
        "\n",
        "\n",
        "  def load_model(self, model_path, load_ae=False):\n",
        "      \"\"\"Load Deep SVDD model from model_path.\"\"\"\n",
        "\n",
        "      model_dict = torch.load(model_path)\n",
        "      if self.ae_net is None:\n",
        "          self.ae_net = My_LeNet_Autoencoder()\n",
        "      self.ae_net.load_state_dict(model_dict['ae_net_dict'])\n",
        "\n",
        "  def load(self):\n",
        "\n",
        "    self.load_model(\"/content/gdrive/MyDrive/ArshadPeoject/model/base-auto-encoder.dict\")\n",
        "    self.pre_trained=True\n",
        "\n",
        "  def load_clf(self, clf_num,lr, n_epochs, lr_milestones,batch_size,weight_decay,device,n_jobs_dataloader):\n",
        "    self.worker_net = list()\n",
        "\n",
        "    model0 = torch.load('/content/gdrive/MyDrive/ArshadPeoject/model/clf_0.dict')\n",
        "    self.c_count = model0['c_count']\n",
        "    self.filtered = model0['filtered']\n",
        "    self.centers = model0['centers']\n",
        "    print('l',self.c_count)\n",
        "    # print(self.filtered)\n",
        "    \n",
        "    \n",
        "    try:\n",
        "      for cls_conter in range(0,self.c_count):\n",
        "        model = torch.load('/content/gdrive/MyDrive/ArshadPeoject/model/clf.dict.'+str(cls_conter+1))\n",
        "        R= model['R']\n",
        "        center=model['c']\n",
        "        self.valid = model['valid']\n",
        "        print(self.valid)\n",
        "        wn= My_LeNet()\n",
        "        wn.load_state_dict(model['net'])\n",
        "        self.worker_net.append(wn)\n",
        "        # t.net = wn\n",
        "        self.trainer.append(DeepSVDDTrainer(self.objective, R, center, self.nu, self.optimizer_name, lr=lr,\n",
        "                                    n_epochs=n_epochs, lr_milestones=lr_milestones, batch_size=batch_size,\n",
        "                                    weight_decay=weight_decay, device=device, n_jobs_dataloader=n_jobs_dataloader))\n",
        "       \n",
        "    except FileNotFoundError:\n",
        "      self.c_count=self.c_count\n",
        "    print('f',self.c_count)\n",
        "    print(\"ty\",len(self.trainer))\n",
        "\n",
        "    for ff in range(self.c_count-1 ,len(self.centers)):\n",
        "      self.trainer.append(DeepSVDDTrainer(self.objective, self.R, self.centers[ff], self.nu, self.optimizer_name, lr=lr,\n",
        "                                      n_epochs=n_epochs, lr_milestones=lr_milestones, batch_size=batch_size,\n",
        "                                      weight_decay=weight_decay, device=device, n_jobs_dataloader=n_jobs_dataloader))\n",
        "    print(\"tr\",len(self.trainer))\n",
        "\n",
        "  def pretrain(self, dataset: BaseADDataset, valid_dataset: BaseADDataset, optimizer_name: str = 'adam', lr: float = 0.1, n_epochs: int = 100,\n",
        "              lr_milestones: tuple = (), batch_size: int = 128, weight_decay: float = 1e-6, device: str = 'cuda',\n",
        "              n_jobs_dataloader: int = 0,load=False):\n",
        "    \"\"\"Pretrains the weights for the Deep SVDD network \\phi via autoencoder.\"\"\"\n",
        "\n",
        "    self.ae_net =My_LeNet_Autoencoder()\n",
        "    self.ae_optimizer_name = optimizer_name\n",
        "    self.ae_trainer = AETrainer(optimizer_name, lr=lr, n_epochs=n_epochs, lr_milestones=lr_milestones,\n",
        "                                batch_size=batch_size, weight_decay=weight_decay, device=device,\n",
        "                                n_jobs_dataloader=n_jobs_dataloader)\n",
        "    if load :\n",
        "      self.ae_net ,valid= self.ae_trainer.load_and_continue(dataset,valid_dataset, self.ae_net)\n",
        "    else:\n",
        "      self.ae_net ,valid= self.ae_trainer.train(dataset,valid_dataset, self.ae_net)\n",
        "    self.ae_trainer.test(dataset, self.ae_net)\n",
        "    self.pre_trained=True\n",
        "    self.save_model('/content/gdrive/MyDrive/ArshadPeoject/model/base-auto-encoder.dict')\n",
        "    return valid\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4MZCofipGif"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U71lLFnztzOd"
      },
      "source": [
        "# K-means\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9LPeBBhLx9t"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_7xDSYX679z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf264dac-e225-4ae4-d1c4-abc923cb7314"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 4, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import math\n",
        "import numpy as np\n",
        "outs =np.array([[1,2,3]])\n",
        "outs*outs\n",
        "# l2norm = np.sqrt((outs * outs).sum(axis=1))\n",
        "# outs / l2norm.reshape(2,1)\n",
        "# print(kmeans.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YZn-mL4LzBt"
      },
      "outputs": [],
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "# import math\n",
        "# import numpy as np\n",
        "# outs =np.array([1,2,3])\n",
        "# out2 =np.sqrt((outs * outs).sum(axis=1))\n",
        "# d = np.array([outs])\n",
        "# d_t= d.transpose()\n",
        "# f_d=np.dot(d_t,d)/2\n",
        "# np.fill_diagonal(f_d,0)\n",
        "# l2norm = np.sqrt((f_d * f_d).sum(axis=1))\n",
        "\n",
        "\n",
        "# dd = np.array([out2])\n",
        "# dd_t= dd.transpose()\n",
        "# f_dd=np.dot(dd_t,dd)/2\n",
        "# print(f_d / l2norm.reshape(3,1))\n",
        "# print(f_d / l2norm.reshape(3,1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mJpcguSgDz0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywTR4_Gprd2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbba76bf-4f4b-4b19-d2b0-5f024da460d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'milestones': Counter({40: 1, 60: 1}), 'gamma': 0.1, 'base_lrs': [0.1], 'last_epoch': 400, '_step_count': 401, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [0.0010000000000000002]}\n",
            "400\n",
            "Counter({40: 1, 60: 1})\n",
            "Finished pretraining.\n",
            "Testing autoencoder...\n",
            "Test set Loss: 0.52696268\n",
            "Autoencoder testing time: 15.693\n",
            "Finished testing autoencoder.\n",
            "cuda\n",
            "[[ -12.878931     25.172314     15.938625    -13.862874     19.495764\n",
            "    10.780349    -26.579588    -35.260563  ]\n",
            " [  18.230082    182.46295     179.47525      31.69762     -26.782288\n",
            "   138.54271      10.784475   -228.26404   ]\n",
            " [  10.38303      33.87725      30.334787     -4.333289      9.67043\n",
            "    29.913033    -16.55962     -47.017563  ]\n",
            " [  -7.6588387    47.298317     42.12072       0.23677492    3.3213406\n",
            "    35.677727    -12.757204    -63.034645  ]\n",
            " [  -9.006496     25.50729      18.485168    -10.916994     16.735926\n",
            "    15.03489     -23.29747     -37.620255  ]]\n",
            "[array([    1,     3,     4, ..., 35989, 35990, 35999]), array([   28,    39,    97, ..., 35940, 35981, 35997]), array([    2,     7,     8, ..., 35993, 35994, 35996]), array([   16,    22,    37, ..., 35986, 35995, 35998]), array([    0,     6,    27, ..., 35957, 35976, 35977])]\n",
            "pp []\n",
            "tt 5\n",
            "training net: 1\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1/200\t Time: 10.104\t train_loss: 180.46126219 test_loss: 0.51534903\n",
            "  Epoch 2/200\t Time: 9.954\t train_loss: 0.71157632 test_loss: 0.18766807\n",
            "  Epoch 3/200\t Time: 10.740\t train_loss: 0.54533130 test_loss: 0.96404344\n",
            "  Epoch 4/200\t Time: 10.112\t train_loss: 0.35333270 test_loss: 0.21022771\n",
            "  Epoch 5/200\t Time: 10.122\t train_loss: 0.32410087 test_loss: 0.24960193\n",
            "  Epoch 6/200\t Time: 10.061\t train_loss: 0.23288595 test_loss: 0.17111665\n",
            "  Epoch 7/200\t Time: 9.963\t train_loss: 0.36279032 test_loss: 0.15566513\n",
            "  Epoch 8/200\t Time: 9.957\t train_loss: 0.20123275 test_loss: 0.48928255\n",
            "  Epoch 9/200\t Time: 10.027\t train_loss: 0.21577792 test_loss: 0.11471137\n",
            "  Epoch 10/200\t Time: 10.014\t train_loss: 0.21779987 test_loss: 0.42809871\n",
            "  Epoch 11/200\t Time: 10.077\t train_loss: 0.23786778 test_loss: 0.10942339\n",
            "  Epoch 12/200\t Time: 10.880\t train_loss: 0.18052923 test_loss: 0.18376274\n",
            "  Epoch 13/200\t Time: 10.786\t train_loss: 0.15470170 test_loss: 0.30357578\n",
            "  Epoch 14/200\t Time: 10.037\t train_loss: 0.17421733 test_loss: 0.17661351\n",
            "  Epoch 15/200\t Time: 9.972\t train_loss: 0.43268644 test_loss: 0.78363144\n",
            "  Epoch 16/200\t Time: 10.128\t train_loss: 0.30104882 test_loss: 0.22436403\n",
            "  Epoch 17/200\t Time: 10.086\t train_loss: 0.14484189 test_loss: 0.03279517\n",
            "  Epoch 18/200\t Time: 10.156\t train_loss: 0.14890873 test_loss: 0.14723885\n",
            "  Epoch 19/200\t Time: 9.993\t train_loss: 0.19302674 test_loss: 0.85739344\n",
            "  Epoch 20/200\t Time: 10.052\t train_loss: 0.18627553 test_loss: 0.06898736\n",
            "  Epoch 21/200\t Time: 9.982\t train_loss: 0.18696618 test_loss: 0.39395869\n",
            "  Epoch 22/200\t Time: 10.031\t train_loss: 0.13891557 test_loss: 0.09785631\n",
            "  Epoch 23/200\t Time: 10.058\t train_loss: 0.20291843 test_loss: 0.70203078\n",
            "  Epoch 24/200\t Time: 10.063\t train_loss: 0.66536084 test_loss: 1.92598259\n",
            "  Epoch 25/200\t Time: 10.031\t train_loss: 0.54273849 test_loss: 0.10793975\n",
            "  Epoch 26/200\t Time: 9.926\t train_loss: 0.19527140 test_loss: 0.20600334\n",
            "  Epoch 27/200\t Time: 10.025\t train_loss: 0.17988203 test_loss: 0.15158921\n",
            "  Epoch 28/200\t Time: 10.057\t train_loss: 0.20712100 test_loss: 0.23836622\n",
            "  Epoch 29/200\t Time: 10.067\t train_loss: 0.12913027 test_loss: 0.12591547\n",
            "  Epoch 30/200\t Time: 10.025\t train_loss: 0.03014966 test_loss: 0.02375298\n",
            "  LR scheduler: new learning rate is 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:419: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 31/200\t Time: 10.006\t train_loss: 0.02615592 test_loss: 0.01089842\n",
            "  Epoch 32/200\t Time: 9.975\t train_loss: 0.01820824 test_loss: 0.03734320\n",
            "  Epoch 33/200\t Time: 9.953\t train_loss: 0.02658982 test_loss: 0.02592507\n",
            "  Epoch 34/200\t Time: 9.987\t train_loss: 0.02081998 test_loss: 0.00982649\n",
            "  Epoch 35/200\t Time: 10.132\t train_loss: 0.01900804 test_loss: 0.01437460\n",
            "  Epoch 36/200\t Time: 10.104\t train_loss: 0.01820628 test_loss: 0.01413565\n",
            "  Epoch 37/200\t Time: 9.946\t train_loss: 0.01665478 test_loss: 0.01906893\n",
            "  Epoch 38/200\t Time: 9.980\t train_loss: 0.01516122 test_loss: 0.01546386\n",
            "  Epoch 39/200\t Time: 9.961\t train_loss: 0.01768689 test_loss: 0.02060873\n",
            "  Epoch 40/200\t Time: 9.970\t train_loss: 0.01362590 test_loss: 0.00662415\n",
            "  LR scheduler: new learning rate is 0.001\n",
            "  Epoch 41/200\t Time: 10.089\t train_loss: 0.01260393 test_loss: 0.01725483\n",
            "  Epoch 42/200\t Time: 10.027\t train_loss: 0.01217439 test_loss: 0.00835648\n",
            "  Epoch 43/200\t Time: 10.024\t train_loss: 0.01558582 test_loss: 0.00705159\n",
            "  Epoch 44/200\t Time: 10.044\t train_loss: 0.01177305 test_loss: 0.00922930\n",
            "  Epoch 45/200\t Time: 9.962\t train_loss: 0.01398901 test_loss: 0.00770525\n",
            "  Epoch 46/200\t Time: 9.975\t train_loss: 0.01414870 test_loss: 0.02419784\n",
            "  Epoch 47/200\t Time: 9.959\t train_loss: 0.01285105 test_loss: 0.03490619\n",
            "  Epoch 48/200\t Time: 9.869\t train_loss: 0.01330271 test_loss: 0.00579694\n",
            "  Epoch 49/200\t Time: 9.946\t train_loss: 0.01630989 test_loss: 0.01040716\n",
            "  Epoch 50/200\t Time: 9.881\t train_loss: 0.01806516 test_loss: 0.00697422\n",
            "  Epoch 51/200\t Time: 9.822\t train_loss: 0.01288749 test_loss: 0.10975207\n",
            "  Epoch 52/200\t Time: 9.858\t train_loss: 0.01255712 test_loss: 0.01881200\n",
            "  Epoch 53/200\t Time: 9.896\t train_loss: 0.01691375 test_loss: 0.01234487\n",
            "  Epoch 54/200\t Time: 9.840\t train_loss: 0.01445024 test_loss: 0.01673055\n",
            "  Epoch 55/200\t Time: 9.771\t train_loss: 0.01364853 test_loss: 0.01455289\n",
            "  Epoch 56/200\t Time: 9.882\t train_loss: 0.01344505 test_loss: 0.06523067\n",
            "  Epoch 57/200\t Time: 9.833\t train_loss: 0.01150308 test_loss: 0.00960496\n",
            "  Epoch 58/200\t Time: 9.772\t train_loss: 0.01150251 test_loss: 0.01221898\n",
            "  Epoch 59/200\t Time: 9.900\t train_loss: 0.01268613 test_loss: 0.00725772\n",
            "  Epoch 60/200\t Time: 9.846\t train_loss: 0.01747919 test_loss: 0.01002644\n",
            "  Epoch 61/200\t Time: 9.855\t train_loss: 0.01318794 test_loss: 0.00951880\n",
            "  Epoch 62/200\t Time: 9.888\t train_loss: 0.01182711 test_loss: 0.01553881\n",
            "  Epoch 63/200\t Time: 9.907\t train_loss: 0.01239515 test_loss: 0.00953964\n",
            "  Epoch 64/200\t Time: 9.877\t train_loss: 0.01268251 test_loss: 0.01096095\n",
            "  Epoch 65/200\t Time: 9.779\t train_loss: 0.01216573 test_loss: 0.00926334\n",
            "  Epoch 66/200\t Time: 9.866\t train_loss: 0.01490383 test_loss: 0.00734475\n",
            "  Epoch 67/200\t Time: 9.850\t train_loss: 0.01591018 test_loss: 0.05856712\n",
            "  Epoch 68/200\t Time: 9.881\t train_loss: 0.01232106 test_loss: 0.00894377\n",
            "  Epoch 69/200\t Time: 9.788\t train_loss: 0.01206982 test_loss: 0.01581239\n",
            "  Epoch 70/200\t Time: 9.873\t train_loss: 0.01122544 test_loss: 0.00757409\n",
            "  Epoch 71/200\t Time: 9.965\t train_loss: 0.01263730 test_loss: 0.01360220\n",
            "  Epoch 72/200\t Time: 9.981\t train_loss: 0.01342575 test_loss: 0.01535720\n",
            "  Epoch 73/200\t Time: 9.777\t train_loss: 0.01495909 test_loss: 0.01242836\n",
            "  Epoch 74/200\t Time: 9.839\t train_loss: 0.01333903 test_loss: 0.01104794\n",
            "  Epoch 75/200\t Time: 9.870\t train_loss: 0.01154528 test_loss: 0.00897933\n",
            "  Epoch 76/200\t Time: 9.907\t train_loss: 0.01547653 test_loss: 0.02801733\n",
            "  Epoch 77/200\t Time: 9.840\t train_loss: 0.01308377 test_loss: 0.02214180\n",
            "  Epoch 78/200\t Time: 9.809\t train_loss: 0.01307991 test_loss: 0.00733494\n",
            "  Epoch 79/200\t Time: 9.794\t train_loss: 0.01488191 test_loss: 0.03127480\n",
            "  Epoch 80/200\t Time: 9.846\t train_loss: 0.01246281 test_loss: 0.01021786\n",
            "  Epoch 81/200\t Time: 9.913\t train_loss: 0.01288653 test_loss: 0.02106653\n",
            "  Epoch 82/200\t Time: 9.903\t train_loss: 0.00973398 test_loss: 0.03339145\n",
            "  Epoch 83/200\t Time: 9.858\t train_loss: 0.01185806 test_loss: 0.02098623\n",
            "  Epoch 84/200\t Time: 9.769\t train_loss: 0.01413718 test_loss: 0.04222012\n",
            "  Epoch 85/200\t Time: 9.798\t train_loss: 0.01211907 test_loss: 0.00582228\n",
            "  Epoch 86/200\t Time: 9.844\t train_loss: 0.01259190 test_loss: 0.00820641\n",
            "  Epoch 87/200\t Time: 9.787\t train_loss: 0.01214152 test_loss: 0.27730748\n",
            "  Epoch 88/200\t Time: 9.904\t train_loss: 0.01284714 test_loss: 0.01028855\n",
            "  Epoch 89/200\t Time: 9.838\t train_loss: 0.01138620 test_loss: 0.01064491\n",
            "  Epoch 90/200\t Time: 9.922\t train_loss: 0.01187217 test_loss: 0.00393224\n",
            "  Epoch 91/200\t Time: 9.805\t train_loss: 0.01092224 test_loss: 0.00680693\n",
            "  Epoch 92/200\t Time: 9.775\t train_loss: 0.01122545 test_loss: 0.00479415\n",
            "  Epoch 93/200\t Time: 9.855\t train_loss: 0.01055599 test_loss: 0.01079209\n",
            "  Epoch 94/200\t Time: 9.871\t train_loss: 0.01233451 test_loss: 0.08527404\n",
            "  Epoch 95/200\t Time: 9.861\t train_loss: 0.01475096 test_loss: 0.02641037\n",
            "  Epoch 96/200\t Time: 9.860\t train_loss: 0.01125746 test_loss: 0.01640186\n",
            "  Epoch 97/200\t Time: 9.852\t train_loss: 0.01273627 test_loss: 0.02307501\n",
            "  Epoch 98/200\t Time: 9.834\t train_loss: 0.00877104 test_loss: 0.00728353\n",
            "  Epoch 99/200\t Time: 9.793\t train_loss: 0.00895189 test_loss: 0.00794411\n",
            "  Epoch 100/200\t Time: 9.767\t train_loss: 0.01540762 test_loss: 0.00593277\n",
            "  Epoch 101/200\t Time: 9.868\t train_loss: 0.01075108 test_loss: 0.00458747\n",
            "  Epoch 102/200\t Time: 9.783\t train_loss: 0.00980499 test_loss: 0.01037643\n",
            "  Epoch 103/200\t Time: 9.787\t train_loss: 0.01025866 test_loss: 0.00641188\n",
            "  Epoch 104/200\t Time: 9.790\t train_loss: 0.01271313 test_loss: 0.00655243\n",
            "  Epoch 105/200\t Time: 9.815\t train_loss: 0.01474127 test_loss: 0.00718862\n",
            "  Epoch 106/200\t Time: 9.836\t train_loss: 0.01078793 test_loss: 0.00640740\n",
            "  Epoch 107/200\t Time: 9.881\t train_loss: 0.01193521 test_loss: 0.00652451\n",
            "  Epoch 108/200\t Time: 9.937\t train_loss: 0.00886350 test_loss: 0.00393389\n",
            "  Epoch 109/200\t Time: 9.807\t train_loss: 0.00888432 test_loss: 0.00847855\n",
            "  Epoch 110/200\t Time: 9.877\t train_loss: 0.00852064 test_loss: 0.00701873\n",
            "  Epoch 111/200\t Time: 9.897\t train_loss: 0.01062808 test_loss: 0.00549128\n",
            "  Epoch 112/200\t Time: 9.945\t train_loss: 0.00927115 test_loss: 0.00390210\n",
            "  Epoch 113/200\t Time: 9.940\t train_loss: 0.00981022 test_loss: 0.00820552\n",
            "  Epoch 114/200\t Time: 9.943\t train_loss: 0.00973993 test_loss: 0.02072532\n",
            "  Epoch 115/200\t Time: 9.846\t train_loss: 0.00764004 test_loss: 0.02595413\n",
            "  Epoch 116/200\t Time: 9.822\t train_loss: 0.00914240 test_loss: 0.00597313\n",
            "  Epoch 117/200\t Time: 9.804\t train_loss: 0.00987242 test_loss: 0.00337710\n",
            "  Epoch 118/200\t Time: 9.752\t train_loss: 0.01188128 test_loss: 0.00813899\n",
            "  Epoch 119/200\t Time: 9.812\t train_loss: 0.00865711 test_loss: 0.01526628\n",
            "  Epoch 120/200\t Time: 9.917\t train_loss: 0.01087116 test_loss: 0.01202279\n",
            "  Epoch 121/200\t Time: 9.821\t train_loss: 0.01019613 test_loss: 0.01272851\n",
            "  Epoch 122/200\t Time: 9.816\t train_loss: 0.01033589 test_loss: 0.00412447\n",
            "  Epoch 123/200\t Time: 9.785\t train_loss: 0.00840462 test_loss: 0.00654156\n",
            "  Epoch 124/200\t Time: 9.803\t train_loss: 0.00819676 test_loss: 0.00880461\n",
            "  Epoch 125/200\t Time: 9.857\t train_loss: 0.01042255 test_loss: 0.00751341\n",
            "  Epoch 126/200\t Time: 9.904\t train_loss: 0.00750326 test_loss: 0.00594746\n",
            "  Epoch 127/200\t Time: 9.751\t train_loss: 0.00772683 test_loss: 0.01076034\n",
            "  Epoch 128/200\t Time: 9.745\t train_loss: 0.01011010 test_loss: 0.00581621\n",
            "  Epoch 129/200\t Time: 9.857\t train_loss: 0.00976083 test_loss: 0.00860299\n",
            "  Epoch 130/200\t Time: 9.832\t train_loss: 0.01222026 test_loss: 0.01229554\n",
            "  Epoch 131/200\t Time: 9.805\t train_loss: 0.00718791 test_loss: 0.00814567\n",
            "  Epoch 132/200\t Time: 9.939\t train_loss: 0.00964729 test_loss: 0.00786594\n",
            "  Epoch 133/200\t Time: 9.843\t train_loss: 0.01024954 test_loss: 0.00585000\n",
            "  Epoch 134/200\t Time: 9.794\t train_loss: 0.00798455 test_loss: 0.03169400\n",
            "  Epoch 135/200\t Time: 9.893\t train_loss: 0.01009227 test_loss: 0.00335135\n",
            "  Epoch 136/200\t Time: 9.858\t train_loss: 0.00810311 test_loss: 0.01526847\n",
            "  Epoch 137/200\t Time: 9.860\t train_loss: 0.00803790 test_loss: 0.03933454\n",
            "  Epoch 138/200\t Time: 9.809\t train_loss: 0.00897648 test_loss: 0.00515099\n",
            "  Epoch 139/200\t Time: 9.871\t train_loss: 0.00761135 test_loss: 0.01816677\n",
            "  Epoch 140/200\t Time: 9.780\t train_loss: 0.00719076 test_loss: 0.00433213\n",
            "  Epoch 141/200\t Time: 9.785\t train_loss: 0.00700087 test_loss: 0.00541282\n",
            "  Epoch 142/200\t Time: 9.874\t train_loss: 0.00786361 test_loss: 0.00530771\n",
            "  Epoch 143/200\t Time: 9.831\t train_loss: 0.00827441 test_loss: 0.00810418\n",
            "  Epoch 144/200\t Time: 9.895\t train_loss: 0.00676107 test_loss: 0.00665364\n",
            "  Epoch 145/200\t Time: 9.839\t train_loss: 0.00606584 test_loss: 0.00782073\n",
            "  Epoch 146/200\t Time: 9.820\t train_loss: 0.00700811 test_loss: 0.00580274\n",
            "  Epoch 147/200\t Time: 9.887\t train_loss: 0.00686845 test_loss: 0.00638666\n",
            "  Epoch 148/200\t Time: 9.806\t train_loss: 0.00631374 test_loss: 0.00470665\n",
            "  Epoch 149/200\t Time: 9.870\t train_loss: 0.00881842 test_loss: 0.00372670\n",
            "  Epoch 150/200\t Time: 10.007\t train_loss: 0.00808289 test_loss: 0.00430970\n",
            "  Epoch 151/200\t Time: 9.930\t train_loss: 0.00819378 test_loss: 0.00615419\n",
            "  Epoch 152/200\t Time: 9.840\t train_loss: 0.00658025 test_loss: 0.01114455\n",
            "  Epoch 153/200\t Time: 9.904\t train_loss: 0.00823931 test_loss: 0.01218872\n",
            "  Epoch 154/200\t Time: 9.892\t train_loss: 0.00611906 test_loss: 0.00629226\n",
            "  Epoch 155/200\t Time: 9.877\t train_loss: 0.00610395 test_loss: 0.00652729\n",
            "  Epoch 156/200\t Time: 9.966\t train_loss: 0.00740499 test_loss: 0.01351830\n",
            "  Epoch 157/200\t Time: 9.913\t train_loss: 0.00707527 test_loss: 0.00637169\n",
            "  Epoch 158/200\t Time: 9.739\t train_loss: 0.00692313 test_loss: 0.00735847\n",
            "  Epoch 159/200\t Time: 9.755\t train_loss: 0.00725572 test_loss: 0.01472261\n",
            "  Epoch 160/200\t Time: 9.818\t train_loss: 0.00633360 test_loss: 0.00626495\n",
            "  Epoch 161/200\t Time: 9.876\t train_loss: 0.00583374 test_loss: 0.00388507\n",
            "  Epoch 162/200\t Time: 9.849\t train_loss: 0.00929407 test_loss: 0.00572330\n",
            "  Epoch 163/200\t Time: 9.792\t train_loss: 0.00689355 test_loss: 0.00533080\n",
            "  Epoch 164/200\t Time: 9.812\t train_loss: 0.00946265 test_loss: 0.00351365\n",
            "  Epoch 165/200\t Time: 9.785\t train_loss: 0.00841254 test_loss: 0.00794630\n",
            "  Epoch 166/200\t Time: 9.830\t train_loss: 0.00605609 test_loss: 0.00249126\n",
            "  Epoch 167/200\t Time: 9.830\t train_loss: 0.00836184 test_loss: 0.00698534\n",
            "  Epoch 168/200\t Time: 9.804\t train_loss: 0.00802948 test_loss: 0.00296534\n",
            "  Epoch 169/200\t Time: 9.768\t train_loss: 0.00600242 test_loss: 0.00638949\n",
            "  Epoch 170/200\t Time: 9.839\t train_loss: 0.01263627 test_loss: 0.01294458\n",
            "  Epoch 171/200\t Time: 9.757\t train_loss: 0.00612787 test_loss: 0.00628722\n",
            "  Epoch 172/200\t Time: 9.802\t train_loss: 0.00610751 test_loss: 0.00859594\n",
            "  Epoch 173/200\t Time: 9.813\t train_loss: 0.00619794 test_loss: 0.00324153\n",
            "  Epoch 174/200\t Time: 9.857\t train_loss: 0.00630629 test_loss: 0.00639718\n",
            "  Epoch 175/200\t Time: 9.866\t train_loss: 0.00479386 test_loss: 0.00332918\n",
            "  Epoch 176/200\t Time: 9.832\t train_loss: 0.00836594 test_loss: 0.00377392\n",
            "  Epoch 177/200\t Time: 9.801\t train_loss: 0.00552894 test_loss: 0.00702870\n",
            "  Epoch 178/200\t Time: 9.745\t train_loss: 0.00694792 test_loss: 0.01340377\n",
            "  Epoch 179/200\t Time: 9.859\t train_loss: 0.00663093 test_loss: 0.01101121\n",
            "  Epoch 180/200\t Time: 9.834\t train_loss: 0.00505771 test_loss: 0.00370467\n",
            "  Epoch 181/200\t Time: 9.869\t train_loss: 0.00627171 test_loss: 0.00392497\n",
            "  Epoch 182/200\t Time: 9.820\t train_loss: 0.00490795 test_loss: 0.00480153\n",
            "  Epoch 183/200\t Time: 9.796\t train_loss: 0.00698683 test_loss: 0.00398219\n",
            "  Epoch 184/200\t Time: 9.778\t train_loss: 0.00575831 test_loss: 0.04649331\n",
            "  Epoch 185/200\t Time: 9.794\t train_loss: 0.00561931 test_loss: 0.00249416\n",
            "  Epoch 186/200\t Time: 9.878\t train_loss: 0.00666125 test_loss: 0.00790257\n",
            "  Epoch 187/200\t Time: 9.959\t train_loss: 0.00581465 test_loss: 0.00367769\n",
            "  Epoch 188/200\t Time: 9.859\t train_loss: 0.00848609 test_loss: 0.02950924\n",
            "  Epoch 189/200\t Time: 9.895\t train_loss: 0.00571862 test_loss: 0.00851327\n",
            "  Epoch 190/200\t Time: 9.802\t train_loss: 0.00527892 test_loss: 0.00460908\n",
            "  Epoch 191/200\t Time: 9.732\t train_loss: 0.00581738 test_loss: 0.00213849\n",
            "  Epoch 192/200\t Time: 9.827\t train_loss: 0.00563712 test_loss: 0.01187101\n",
            "  Epoch 193/200\t Time: 9.869\t train_loss: 0.00612480 test_loss: 0.00252670\n",
            "  Epoch 194/200\t Time: 9.842\t train_loss: 0.00540165 test_loss: 0.00375453\n",
            "  Epoch 195/200\t Time: 9.857\t train_loss: 0.00642706 test_loss: 0.00430459\n",
            "  Epoch 196/200\t Time: 9.832\t train_loss: 0.00568238 test_loss: 0.00541405\n",
            "  Epoch 197/200\t Time: 9.824\t train_loss: 0.00534583 test_loss: 0.00625088\n",
            "  Epoch 198/200\t Time: 9.831\t train_loss: 0.00703489 test_loss: 0.01266593\n",
            "  Epoch 199/200\t Time: 9.898\t train_loss: 0.00514903 test_loss: 0.00398788\n",
            "  Epoch 200/200\t Time: 9.863\t train_loss: 0.00867172 test_loss: 0.00375402\n",
            "Finished training.\n",
            "training net: 2\n",
            "Starting training...\n",
            "  Epoch 1/200\t Time: 4.423\t train_loss: 36864.06900024 test_loss: 6167.48583984\n",
            "  Epoch 2/200\t Time: 4.296\t train_loss: 1173.42000198 test_loss: 582.01019287\n",
            "  Epoch 3/200\t Time: 4.360\t train_loss: 165.34804869 test_loss: 83.05982971\n",
            "  Epoch 4/200\t Time: 4.432\t train_loss: 33.45047760 test_loss: 21.53999329\n",
            "  Epoch 5/200\t Time: 4.378\t train_loss: 14.17970020 test_loss: 17.56243134\n",
            "  Epoch 6/200\t Time: 4.382\t train_loss: 7.43193488 test_loss: 4.28752756\n",
            "  Epoch 7/200\t Time: 4.413\t train_loss: 4.89114326 test_loss: 3.74575806\n",
            "  Epoch 8/200\t Time: 4.355\t train_loss: 4.40702593 test_loss: 2.41298628\n",
            "  Epoch 9/200\t Time: 4.355\t train_loss: 3.07030334 test_loss: 2.47619319\n",
            "  Epoch 10/200\t Time: 4.404\t train_loss: 1.95093470 test_loss: 0.99411488\n",
            "  Epoch 11/200\t Time: 4.393\t train_loss: 2.19065714 test_loss: 9.32985592\n",
            "  Epoch 12/200\t Time: 4.407\t train_loss: 2.40490814 test_loss: 1.36062336\n",
            "  Epoch 13/200\t Time: 4.379\t train_loss: 1.82424445 test_loss: 4.56755447\n",
            "  Epoch 14/200\t Time: 4.427\t train_loss: 2.02767558 test_loss: 1.48266745\n",
            "  Epoch 15/200\t Time: 4.449\t train_loss: 1.81554847 test_loss: 0.75940335\n",
            "  Epoch 16/200\t Time: 4.345\t train_loss: 1.85273548 test_loss: 2.44921184\n",
            "  Epoch 17/200\t Time: 4.403\t train_loss: 1.79154165 test_loss: 3.92929769\n",
            "  Epoch 18/200\t Time: 4.370\t train_loss: 1.50645739 test_loss: 1.49308169\n",
            "  Epoch 19/200\t Time: 4.355\t train_loss: 2.00762712 test_loss: 2.62546349\n",
            "  Epoch 20/200\t Time: 4.392\t train_loss: 1.41621595 test_loss: 2.81243634\n",
            "  Epoch 21/200\t Time: 4.410\t train_loss: 1.16698110 test_loss: 1.55046928\n",
            "  Epoch 22/200\t Time: 4.332\t train_loss: 1.49328141 test_loss: 1.99838245\n",
            "  Epoch 23/200\t Time: 4.333\t train_loss: 1.23366221 test_loss: 1.09258389\n",
            "  Epoch 24/200\t Time: 4.419\t train_loss: 1.27875311 test_loss: 3.62341619\n",
            "  Epoch 25/200\t Time: 4.356\t train_loss: 2.47127810 test_loss: 23.80717850\n",
            "  Epoch 26/200\t Time: 4.435\t train_loss: 2.09936975 test_loss: 3.75565410\n",
            "  Epoch 27/200\t Time: 4.351\t train_loss: 2.00147828 test_loss: 1.73888052\n",
            "  Epoch 28/200\t Time: 4.356\t train_loss: 2.01413863 test_loss: 1.91281974\n",
            "  Epoch 29/200\t Time: 4.337\t train_loss: 1.24248468 test_loss: 1.18196476\n",
            "  Epoch 30/200\t Time: 4.356\t train_loss: 0.88524508 test_loss: 1.41435754\n",
            "  LR scheduler: new learning rate is 0.01\n",
            "  Epoch 31/200\t Time: 4.361\t train_loss: 1.60054748 test_loss: 11.92846203\n",
            "  Epoch 32/200\t Time: 4.410\t train_loss: 0.94570048 test_loss: 0.53329158\n",
            "  Epoch 33/200\t Time: 4.374\t train_loss: 1.62611927 test_loss: 0.67505914\n",
            "  Epoch 34/200\t Time: 4.333\t train_loss: 1.32371124 test_loss: 9.78118896\n",
            "  Epoch 35/200\t Time: 4.350\t train_loss: 0.62904063 test_loss: 0.38173231\n",
            "  Epoch 36/200\t Time: 4.355\t train_loss: 0.82437199 test_loss: 0.77275610\n",
            "  Epoch 37/200\t Time: 4.422\t train_loss: 0.96707945 test_loss: 0.48036119\n",
            "  Epoch 38/200\t Time: 4.401\t train_loss: 0.67493782 test_loss: 0.65639818\n",
            "  Epoch 39/200\t Time: 4.419\t train_loss: 0.97183752 test_loss: 1.02464378\n",
            "  Epoch 40/200\t Time: 4.359\t train_loss: 0.74611336 test_loss: 1.32328594\n",
            "  LR scheduler: new learning rate is 0.001\n",
            "  Epoch 41/200\t Time: 4.248\t train_loss: 0.92843157 test_loss: 0.45879516\n",
            "  Epoch 42/200\t Time: 4.380\t train_loss: 0.58209007 test_loss: 0.47378713\n",
            "  Epoch 43/200\t Time: 4.401\t train_loss: 0.53659347 test_loss: 1.94974411\n",
            "  Epoch 44/200\t Time: 4.327\t train_loss: 0.68624867 test_loss: 0.73526114\n",
            "  Epoch 45/200\t Time: 4.341\t train_loss: 0.85790110 test_loss: 1.73992980\n",
            "  Epoch 46/200\t Time: 4.347\t train_loss: 0.79122368 test_loss: 1.52598763\n",
            "  Epoch 47/200\t Time: 4.306\t train_loss: 0.78955088 test_loss: 1.48288357\n",
            "  Epoch 48/200\t Time: 4.306\t train_loss: 0.86205799 test_loss: 0.46142331\n",
            "  Epoch 49/200\t Time: 4.353\t train_loss: 0.58576041 test_loss: 0.65122300\n",
            "  Epoch 50/200\t Time: 4.393\t train_loss: 1.21862000 test_loss: 7.27224827\n",
            "  Epoch 51/200\t Time: 4.406\t train_loss: 0.97568374 test_loss: 0.43321696\n",
            "  Epoch 52/200\t Time: 4.345\t train_loss: 0.67683738 test_loss: 0.58534271\n",
            "  Epoch 53/200\t Time: 4.385\t train_loss: 0.70365016 test_loss: 1.17491949\n",
            "  Epoch 54/200\t Time: 4.320\t train_loss: 0.95025820 test_loss: 2.28638721\n",
            "  Epoch 55/200\t Time: 4.317\t train_loss: 0.61006751 test_loss: 0.40411168\n",
            "  Epoch 56/200\t Time: 4.375\t train_loss: 0.78212148 test_loss: 0.89327574\n",
            "  Epoch 57/200\t Time: 4.306\t train_loss: 0.77395259 test_loss: 1.71702230\n",
            "  Epoch 58/200\t Time: 4.337\t train_loss: 1.03171649 test_loss: 0.96260470\n",
            "  Epoch 59/200\t Time: 4.324\t train_loss: 0.69751957 test_loss: 0.81573576\n",
            "  Epoch 60/200\t Time: 4.308\t train_loss: 0.71410914 test_loss: 1.01834202\n",
            "  Epoch 61/200\t Time: 4.317\t train_loss: 0.55925816 test_loss: 0.38070568\n",
            "  Epoch 62/200\t Time: 4.388\t train_loss: 0.56200200 test_loss: 0.88920939\n",
            "  Epoch 63/200\t Time: 4.312\t train_loss: 0.74938705 test_loss: 0.52621192\n",
            "  Epoch 64/200\t Time: 4.258\t train_loss: 0.81879519 test_loss: 0.29910895\n",
            "  Epoch 65/200\t Time: 4.385\t train_loss: 0.76730723 test_loss: 1.38291502\n",
            "  Epoch 66/200\t Time: 4.267\t train_loss: 0.84191521 test_loss: 0.35677800\n",
            "  Epoch 67/200\t Time: 4.324\t train_loss: 0.82545220 test_loss: 0.52282739\n",
            "  Epoch 68/200\t Time: 4.343\t train_loss: 0.76895700 test_loss: 1.71921194\n",
            "  Epoch 69/200\t Time: 4.346\t train_loss: 0.65858343 test_loss: 0.69623959\n",
            "  Epoch 70/200\t Time: 4.292\t train_loss: 0.79723833 test_loss: 0.71571970\n",
            "  Epoch 71/200\t Time: 4.321\t train_loss: 0.85110060 test_loss: 3.66224289\n",
            "  Epoch 72/200\t Time: 4.317\t train_loss: 0.67926032 test_loss: 0.97022009\n",
            "  Epoch 73/200\t Time: 4.307\t train_loss: 0.84329342 test_loss: 1.08026624\n",
            "  Epoch 74/200\t Time: 4.301\t train_loss: 0.55954118 test_loss: 0.45218101\n",
            "  Epoch 75/200\t Time: 4.330\t train_loss: 0.87694811 test_loss: 0.95315611\n",
            "  Epoch 76/200\t Time: 4.384\t train_loss: 0.97451172 test_loss: 0.64517015\n",
            "  Epoch 77/200\t Time: 4.327\t train_loss: 0.65075344 test_loss: 0.46744043\n",
            "  Epoch 78/200\t Time: 4.340\t train_loss: 0.78073311 test_loss: 1.30363679\n",
            "  Epoch 79/200\t Time: 4.319\t train_loss: 0.71579079 test_loss: 0.46104771\n",
            "  Epoch 80/200\t Time: 4.345\t train_loss: 0.66251381 test_loss: 0.47406062\n",
            "  Epoch 81/200\t Time: 4.419\t train_loss: 0.68224477 test_loss: 0.58166564\n",
            "  Epoch 82/200\t Time: 4.392\t train_loss: 0.78358721 test_loss: 2.01196361\n",
            "  Epoch 83/200\t Time: 4.416\t train_loss: 0.75462261 test_loss: 0.65001124\n",
            "  Epoch 84/200\t Time: 4.410\t train_loss: 0.87308599 test_loss: 0.70868099\n",
            "  Epoch 85/200\t Time: 4.403\t train_loss: 0.86084416 test_loss: 0.84790164\n",
            "  Epoch 86/200\t Time: 4.341\t train_loss: 0.80952394 test_loss: 0.74957079\n",
            "  Epoch 87/200\t Time: 4.384\t train_loss: 0.76584717 test_loss: 0.87016118\n",
            "  Epoch 88/200\t Time: 4.303\t train_loss: 0.94973994 test_loss: 3.50331116\n",
            "  Epoch 89/200\t Time: 4.318\t train_loss: 0.58354654 test_loss: 1.19458294\n",
            "  Epoch 90/200\t Time: 4.358\t train_loss: 0.86495689 test_loss: 2.91883063\n",
            "  Epoch 91/200\t Time: 4.369\t train_loss: 0.85385625 test_loss: 0.97264206\n",
            "  Epoch 92/200\t Time: 4.379\t train_loss: 0.85128648 test_loss: 0.85938102\n",
            "  Epoch 93/200\t Time: 4.403\t train_loss: 0.63244785 test_loss: 0.61429530\n",
            "  Epoch 94/200\t Time: 4.419\t train_loss: 0.82599992 test_loss: 0.74202269\n",
            "  Epoch 95/200\t Time: 4.414\t train_loss: 0.72093575 test_loss: 2.01827812\n",
            "  Epoch 96/200\t Time: 4.398\t train_loss: 0.83254534 test_loss: 0.62600917\n",
            "  Epoch 97/200\t Time: 4.385\t train_loss: 0.58885579 test_loss: 0.96736568\n",
            "  Epoch 98/200\t Time: 4.382\t train_loss: 0.61309024 test_loss: 0.79711914\n",
            "  Epoch 99/200\t Time: 4.378\t train_loss: 0.73516105 test_loss: 0.52379245\n",
            "  Epoch 100/200\t Time: 4.305\t train_loss: 0.75655845 test_loss: 1.20387805\n",
            "  Epoch 101/200\t Time: 4.404\t train_loss: 0.91826770 test_loss: 0.97497863\n",
            "  Epoch 102/200\t Time: 4.299\t train_loss: 1.06313133 test_loss: 2.22816110\n",
            "  Epoch 103/200\t Time: 4.354\t train_loss: 0.74772067 test_loss: 0.37332058\n",
            "  Epoch 104/200\t Time: 4.303\t train_loss: 0.82873805 test_loss: 0.82451147\n",
            "  Epoch 105/200\t Time: 4.306\t train_loss: 0.71033851 test_loss: 0.61273098\n",
            "  Epoch 106/200\t Time: 4.386\t train_loss: 1.25226481 test_loss: 0.38052517\n",
            "  Epoch 107/200\t Time: 4.327\t train_loss: 0.85708821 test_loss: 1.46559644\n",
            "  Epoch 108/200\t Time: 4.350\t train_loss: 0.76177668 test_loss: 0.79488993\n",
            "  Epoch 109/200\t Time: 4.371\t train_loss: 0.67844246 test_loss: 0.72658944\n",
            "  Epoch 110/200\t Time: 4.307\t train_loss: 0.81832080 test_loss: 0.83449024\n",
            "  Epoch 111/200\t Time: 4.310\t train_loss: 0.79728519 test_loss: 0.38391080\n",
            "  Epoch 112/200\t Time: 4.299\t train_loss: 1.02204453 test_loss: 4.35253859\n",
            "  Epoch 113/200\t Time: 4.318\t train_loss: 0.71463214 test_loss: 0.36875004\n",
            "  Epoch 114/200\t Time: 4.344\t train_loss: 0.73085214 test_loss: 0.91070491\n",
            "  Epoch 115/200\t Time: 4.366\t train_loss: 0.73208424 test_loss: 2.14075065\n",
            "  Epoch 116/200\t Time: 4.350\t train_loss: 0.59799696 test_loss: 0.76715714\n",
            "  Epoch 117/200\t Time: 4.415\t train_loss: 0.68086160 test_loss: 0.39465782\n",
            "  Epoch 118/200\t Time: 4.351\t train_loss: 0.74719845 test_loss: 2.58316922\n",
            "  Epoch 119/200\t Time: 4.384\t train_loss: 0.81364005 test_loss: 0.64580178\n",
            "  Epoch 120/200\t Time: 4.396\t train_loss: 0.72312821 test_loss: 1.77723873\n",
            "  Epoch 121/200\t Time: 4.378\t train_loss: 0.66080338 test_loss: 1.96295774\n",
            "  Epoch 122/200\t Time: 4.413\t train_loss: 0.86601919 test_loss: 2.60119772\n",
            "  Epoch 123/200\t Time: 4.403\t train_loss: 1.16462228 test_loss: 0.50596994\n",
            "  Epoch 124/200\t Time: 4.315\t train_loss: 0.79356035 test_loss: 0.83629572\n",
            "  Epoch 125/200\t Time: 4.322\t train_loss: 0.63931140 test_loss: 0.23536961\n",
            "  Epoch 126/200\t Time: 4.309\t train_loss: 0.91678704 test_loss: 2.25428534\n",
            "  Epoch 127/200\t Time: 4.323\t train_loss: 0.74906557 test_loss: 0.92587852\n",
            "  Epoch 128/200\t Time: 4.334\t train_loss: 0.75820174 test_loss: 1.04866111\n",
            "  Epoch 129/200\t Time: 4.348\t train_loss: 0.87666984 test_loss: 1.24198663\n",
            "  Epoch 130/200\t Time: 4.295\t train_loss: 0.71950489 test_loss: 0.45247275\n",
            "  Epoch 131/200\t Time: 4.350\t train_loss: 0.84647657 test_loss: 2.74487519\n",
            "  Epoch 132/200\t Time: 4.331\t train_loss: 0.53250996 test_loss: 0.68614668\n",
            "  Epoch 133/200\t Time: 4.363\t train_loss: 0.87172736 test_loss: 0.90614951\n",
            "  Epoch 134/200\t Time: 4.281\t train_loss: 1.31151035 test_loss: 3.89911699\n",
            "  Epoch 135/200\t Time: 4.396\t train_loss: 0.82742819 test_loss: 0.80185473\n",
            "  Epoch 136/200\t Time: 4.349\t train_loss: 1.03856601 test_loss: 0.44060102\n",
            "  Epoch 137/200\t Time: 4.297\t train_loss: 0.82070456 test_loss: 2.48687649\n",
            "  Epoch 138/200\t Time: 4.384\t train_loss: 1.19861386 test_loss: 1.64169848\n",
            "  Epoch 139/200\t Time: 4.326\t train_loss: 0.90536422 test_loss: 4.34840822\n",
            "  Epoch 140/200\t Time: 4.348\t train_loss: 0.65540697 test_loss: 0.83879453\n",
            "  Epoch 141/200\t Time: 4.315\t train_loss: 0.80122906 test_loss: 0.63653797\n",
            "  Epoch 142/200\t Time: 4.369\t train_loss: 0.90489478 test_loss: 0.35151443\n",
            "  Epoch 143/200\t Time: 4.363\t train_loss: 0.76163909 test_loss: 0.55746168\n",
            "  Epoch 144/200\t Time: 4.385\t train_loss: 0.59535211 test_loss: 1.48992383\n",
            "  Epoch 145/200\t Time: 4.415\t train_loss: 0.53730528 test_loss: 0.46543533\n",
            "  Epoch 146/200\t Time: 4.329\t train_loss: 1.02972258 test_loss: 0.24774876\n",
            "  Epoch 147/200\t Time: 4.452\t train_loss: 0.57983065 test_loss: 0.50756741\n",
            "  Epoch 148/200\t Time: 4.315\t train_loss: 0.64956723 test_loss: 1.22622621\n",
            "  Epoch 149/200\t Time: 4.392\t train_loss: 0.56672957 test_loss: 1.24122167\n",
            "  Epoch 150/200\t Time: 4.371\t train_loss: 0.64408672 test_loss: 2.16358542\n",
            "  Epoch 151/200\t Time: 4.403\t train_loss: 0.68024526 test_loss: 2.10161829\n",
            "  Epoch 152/200\t Time: 4.345\t train_loss: 0.59997637 test_loss: 0.66165763\n",
            "  Epoch 153/200\t Time: 4.370\t train_loss: 0.68536822 test_loss: 0.48259091\n",
            "  Epoch 154/200\t Time: 4.350\t train_loss: 0.86355339 test_loss: 0.55559254\n",
            "  Epoch 155/200\t Time: 4.390\t train_loss: 0.62553729 test_loss: 1.04393804\n",
            "  Epoch 156/200\t Time: 4.378\t train_loss: 0.62794911 test_loss: 0.74185747\n",
            "  Epoch 157/200\t Time: 4.343\t train_loss: 0.99121035 test_loss: 7.41564274\n",
            "  Epoch 158/200\t Time: 4.368\t train_loss: 0.83948995 test_loss: 0.76809573\n",
            "  Epoch 159/200\t Time: 4.366\t train_loss: 0.90342547 test_loss: 1.92512500\n",
            "  Epoch 160/200\t Time: 4.397\t train_loss: 0.65435413 test_loss: 1.44403684\n",
            "  Epoch 161/200\t Time: 4.432\t train_loss: 0.61717775 test_loss: 1.24713421\n",
            "  Epoch 162/200\t Time: 4.344\t train_loss: 0.61245047 test_loss: 1.16008317\n",
            "  Epoch 163/200\t Time: 4.332\t train_loss: 1.99658476 test_loss: 19.63139343\n",
            "  Epoch 164/200\t Time: 4.307\t train_loss: 0.53274370 test_loss: 0.47298431\n",
            "  Epoch 165/200\t Time: 4.382\t train_loss: 0.75864101 test_loss: 0.40791330\n",
            "  Epoch 166/200\t Time: 4.433\t train_loss: 0.68453650 test_loss: 1.22897255\n",
            "  Epoch 167/200\t Time: 4.406\t train_loss: 0.54048732 test_loss: 0.44136474\n",
            "  Epoch 168/200\t Time: 4.354\t train_loss: 0.62767154 test_loss: 1.11433268\n",
            "  Epoch 169/200\t Time: 4.343\t train_loss: 0.62668379 test_loss: 0.68611664\n",
            "  Epoch 170/200\t Time: 4.333\t train_loss: 0.51597710 test_loss: 0.53619140\n",
            "  Epoch 171/200\t Time: 4.290\t train_loss: 0.57746716 test_loss: 0.82477552\n",
            "  Epoch 172/200\t Time: 4.384\t train_loss: 0.58332101 test_loss: 1.13373816\n",
            "  Epoch 173/200\t Time: 4.371\t train_loss: 0.97157204 test_loss: 0.73040473\n",
            "  Epoch 174/200\t Time: 4.408\t train_loss: 0.72690639 test_loss: 2.05631685\n",
            "  Epoch 175/200\t Time: 4.362\t train_loss: 0.72405004 test_loss: 0.81114548\n",
            "  Epoch 176/200\t Time: 4.331\t train_loss: 0.74106726 test_loss: 2.28294802\n",
            "  Epoch 177/200\t Time: 4.395\t train_loss: 0.74805077 test_loss: 1.08948541\n",
            "  Epoch 178/200\t Time: 4.307\t train_loss: 0.72737775 test_loss: 0.87983412\n",
            "  Epoch 179/200\t Time: 4.376\t train_loss: 0.63554336 test_loss: 0.63379031\n",
            "  Epoch 180/200\t Time: 4.342\t train_loss: 0.83495299 test_loss: 1.59778249\n",
            "  Epoch 181/200\t Time: 4.321\t train_loss: 0.63870669 test_loss: 1.13391399\n",
            "  Epoch 182/200\t Time: 4.329\t train_loss: 0.57910705 test_loss: 0.36701527\n",
            "  Epoch 183/200\t Time: 4.309\t train_loss: 0.71649233 test_loss: 0.89268821\n",
            "  Epoch 184/200\t Time: 4.283\t train_loss: 0.63228501 test_loss: 0.52767289\n",
            "  Epoch 185/200\t Time: 4.378\t train_loss: 0.68202547 test_loss: 0.26852933\n",
            "  Epoch 186/200\t Time: 4.320\t train_loss: 0.56844382 test_loss: 0.51526469\n",
            "  Epoch 187/200\t Time: 4.348\t train_loss: 0.55753120 test_loss: 1.83509052\n",
            "  Epoch 188/200\t Time: 4.350\t train_loss: 0.53630053 test_loss: 1.90995681\n",
            "  Epoch 189/200\t Time: 4.333\t train_loss: 0.75108982 test_loss: 0.58694124\n",
            "  Epoch 190/200\t Time: 4.328\t train_loss: 0.64408218 test_loss: 0.29995319\n",
            "  Epoch 191/200\t Time: 4.252\t train_loss: 0.65555513 test_loss: 1.45533264\n",
            "  Epoch 192/200\t Time: 4.242\t train_loss: 0.73924992 test_loss: 1.29348838\n",
            "  Epoch 193/200\t Time: 4.275\t train_loss: 0.54409996 test_loss: 0.89262670\n",
            "  Epoch 194/200\t Time: 4.282\t train_loss: 0.70433335 test_loss: 0.24580836\n",
            "  Epoch 195/200\t Time: 4.350\t train_loss: 0.61767579 test_loss: 3.00999331\n",
            "  Epoch 196/200\t Time: 4.327\t train_loss: 0.54805484 test_loss: 0.33766055\n",
            "  Epoch 197/200\t Time: 4.295\t train_loss: 0.66971437 test_loss: 1.15494382\n",
            "  Epoch 198/200\t Time: 4.304\t train_loss: 0.74860878 test_loss: 0.63913012\n",
            "  Epoch 199/200\t Time: 4.266\t train_loss: 0.90566517 test_loss: 0.28066131\n",
            "  Epoch 200/200\t Time: 4.280\t train_loss: 0.73333992 test_loss: 0.45608836\n",
            "Finished training.\n",
            "training net: 3\n",
            "Starting training...\n",
            "  Epoch 1/200\t Time: 7.311\t train_loss: 249.65470415 test_loss: 10.65486240\n",
            "  Epoch 2/200\t Time: 7.277\t train_loss: 1.21398029 test_loss: 8.51122570\n",
            "  Epoch 3/200\t Time: 7.269\t train_loss: 0.93796072 test_loss: 5.88157463\n",
            "  Epoch 4/200\t Time: 7.256\t train_loss: 0.68045993 test_loss: 7.88236189\n",
            "  Epoch 5/200\t Time: 7.221\t train_loss: 0.75959053 test_loss: 0.43303293\n",
            "  Epoch 6/200\t Time: 7.167\t train_loss: 0.57934667 test_loss: 10.58868027\n",
            "  Epoch 7/200\t Time: 7.206\t train_loss: 0.69027865 test_loss: 16.56600952\n",
            "  Epoch 8/200\t Time: 7.177\t train_loss: 0.56805813 test_loss: 0.58884794\n",
            "  Epoch 9/200\t Time: 7.205\t train_loss: 0.27107528 test_loss: 0.49705365\n",
            "  Epoch 10/200\t Time: 7.250\t train_loss: 0.29908901 test_loss: 2.31917810\n",
            "  Epoch 11/200\t Time: 7.228\t train_loss: 0.26528620 test_loss: 0.16545792\n",
            "  Epoch 12/200\t Time: 7.250\t train_loss: 0.22035731 test_loss: 2.40881848\n",
            "  Epoch 13/200\t Time: 7.217\t train_loss: 0.34632396 test_loss: 4.59864044\n",
            "  Epoch 14/200\t Time: 7.194\t train_loss: 0.44641577 test_loss: 0.25237963\n",
            "  Epoch 15/200\t Time: 7.199\t train_loss: 0.21004027 test_loss: 1.79537940\n",
            "  Epoch 16/200\t Time: 7.273\t train_loss: 0.27025499 test_loss: 1.29968035\n",
            "  Epoch 17/200\t Time: 7.203\t train_loss: 0.23122916 test_loss: 0.30650064\n",
            "  Epoch 18/200\t Time: 7.206\t train_loss: 0.20014622 test_loss: 0.10931624\n",
            "  Epoch 19/200\t Time: 7.271\t train_loss: 0.16545266 test_loss: 0.27892232\n",
            "  Epoch 20/200\t Time: 7.223\t train_loss: 0.21743625 test_loss: 2.85853672\n",
            "  Epoch 21/200\t Time: 7.195\t train_loss: 0.45697514 test_loss: 1.54075134\n",
            "  Epoch 22/200\t Time: 7.218\t train_loss: 0.23877097 test_loss: 0.62634993\n",
            "  Epoch 23/200\t Time: 7.222\t train_loss: 0.28781037 test_loss: 0.18145019\n",
            "  Epoch 24/200\t Time: 7.183\t train_loss: 0.16795895 test_loss: 0.21809188\n",
            "  Epoch 25/200\t Time: 7.285\t train_loss: 0.19817508 test_loss: 0.81447947\n",
            "  Epoch 26/200\t Time: 7.305\t train_loss: 0.21575784 test_loss: 0.31931856\n",
            "  Epoch 27/200\t Time: 7.263\t train_loss: 0.21383796 test_loss: 0.87989074\n",
            "  Epoch 28/200\t Time: 7.274\t train_loss: 0.48793670 test_loss: 2.95366096\n",
            "  Epoch 29/200\t Time: 7.222\t train_loss: 0.55718551 test_loss: 0.37105393\n",
            "  Epoch 30/200\t Time: 7.308\t train_loss: 0.08994580 test_loss: 0.69118273\n",
            "  LR scheduler: new learning rate is 0.01\n",
            "  Epoch 31/200\t Time: 7.271\t train_loss: 0.06048519 test_loss: 0.14344397\n",
            "  Epoch 32/200\t Time: 7.265\t train_loss: 0.06265589 test_loss: 0.69669199\n",
            "  Epoch 33/200\t Time: 7.260\t train_loss: 0.06314179 test_loss: 0.96279877\n",
            "  Epoch 34/200\t Time: 7.328\t train_loss: 0.06067148 test_loss: 1.01588917\n",
            "  Epoch 35/200\t Time: 7.304\t train_loss: 0.05887342 test_loss: 0.94984734\n",
            "  Epoch 36/200\t Time: 7.250\t train_loss: 0.05461435 test_loss: 0.52793217\n",
            "  Epoch 37/200\t Time: 7.313\t train_loss: 0.05595016 test_loss: 0.74929321\n",
            "  Epoch 38/200\t Time: 7.287\t train_loss: 0.05639737 test_loss: 1.71767092\n",
            "  Epoch 39/200\t Time: 7.195\t train_loss: 0.06276246 test_loss: 0.91093856\n",
            "  Epoch 40/200\t Time: 7.330\t train_loss: 0.05109784 test_loss: 1.50435221\n",
            "  LR scheduler: new learning rate is 0.001\n",
            "  Epoch 41/200\t Time: 7.256\t train_loss: 0.03738976 test_loss: 0.37045801\n",
            "  Epoch 42/200\t Time: 7.266\t train_loss: 0.04614919 test_loss: 0.62961638\n",
            "  Epoch 43/200\t Time: 7.329\t train_loss: 0.11223786 test_loss: 6.49681377\n",
            "  Epoch 44/200\t Time: 7.339\t train_loss: 0.04216222 test_loss: 0.03470917\n",
            "  Epoch 45/200\t Time: 7.296\t train_loss: 0.03009134 test_loss: 0.27793103\n",
            "  Epoch 46/200\t Time: 7.320\t train_loss: 0.03931888 test_loss: 1.20976508\n",
            "  Epoch 47/200\t Time: 7.272\t train_loss: 0.03150236 test_loss: 0.04406261\n",
            "  Epoch 48/200\t Time: 7.322\t train_loss: 0.04054561 test_loss: 0.64834976\n",
            "  Epoch 49/200\t Time: 7.216\t train_loss: 0.03398557 test_loss: 0.17478661\n",
            "  Epoch 50/200\t Time: 7.182\t train_loss: 0.07666221 test_loss: 3.73400974\n",
            "  Epoch 51/200\t Time: 7.359\t train_loss: 0.04068790 test_loss: 0.62188596\n",
            "  Epoch 52/200\t Time: 7.310\t train_loss: 0.05217241 test_loss: 1.55481601\n",
            "  Epoch 53/200\t Time: 7.218\t train_loss: 0.10459879 test_loss: 6.29550982\n",
            "  Epoch 54/200\t Time: 7.237\t train_loss: 0.08629132 test_loss: 4.05714417\n",
            "  Epoch 55/200\t Time: 7.270\t train_loss: 0.04211910 test_loss: 0.93673337\n",
            "  Epoch 56/200\t Time: 7.232\t train_loss: 0.09648210 test_loss: 5.22668695\n",
            "  Epoch 57/200\t Time: 7.251\t train_loss: 0.03661220 test_loss: 0.01778944\n",
            "  Epoch 58/200\t Time: 7.320\t train_loss: 0.03176945 test_loss: 0.32720166\n",
            "  Epoch 59/200\t Time: 7.282\t train_loss: 0.03936262 test_loss: 0.39677095\n",
            "  Epoch 60/200\t Time: 7.229\t train_loss: 0.03038094 test_loss: 0.26562306\n",
            "  Epoch 61/200\t Time: 7.197\t train_loss: 0.02692849 test_loss: 0.16253704\n",
            "  Epoch 62/200\t Time: 7.195\t train_loss: 0.03004796 test_loss: 0.54982960\n",
            "  Epoch 63/200\t Time: 7.273\t train_loss: 0.03218162 test_loss: 0.04346021\n",
            "  Epoch 64/200\t Time: 7.257\t train_loss: 0.02694841 test_loss: 0.07846394\n",
            "  Epoch 65/200\t Time: 7.333\t train_loss: 0.02507652 test_loss: 0.06087808\n",
            "  Epoch 66/200\t Time: 7.300\t train_loss: 0.44356220 test_loss: 32.61130524\n",
            "  Epoch 67/200\t Time: 7.296\t train_loss: 0.11061397 test_loss: 2.83166981\n",
            "  Epoch 68/200\t Time: 7.251\t train_loss: 0.03942831 test_loss: 0.68562865\n",
            "  Epoch 69/200\t Time: 7.241\t train_loss: 0.03692945 test_loss: 0.75268269\n",
            "  Epoch 70/200\t Time: 7.227\t train_loss: 0.02933394 test_loss: 0.02161863\n",
            "  Epoch 71/200\t Time: 7.261\t train_loss: 0.03245359 test_loss: 0.50730741\n",
            "  Epoch 72/200\t Time: 7.261\t train_loss: 0.03772691 test_loss: 0.85299307\n",
            "  Epoch 73/200\t Time: 7.229\t train_loss: 0.03211407 test_loss: 0.15390441\n",
            "  Epoch 74/200\t Time: 7.247\t train_loss: 0.03820609 test_loss: 0.64719164\n",
            "  Epoch 75/200\t Time: 7.287\t train_loss: 0.03758530 test_loss: 0.05690159\n",
            "  Epoch 76/200\t Time: 7.253\t train_loss: 0.02945407 test_loss: 0.03949276\n",
            "  Epoch 77/200\t Time: 7.258\t train_loss: 0.03083890 test_loss: 0.10578081\n",
            "  Epoch 78/200\t Time: 7.184\t train_loss: 0.02855322 test_loss: 0.13140455\n",
            "  Epoch 79/200\t Time: 7.218\t train_loss: 0.04966588 test_loss: 1.73548710\n",
            "  Epoch 80/200\t Time: 7.256\t train_loss: 0.05116656 test_loss: 1.46151996\n",
            "  Epoch 81/200\t Time: 7.208\t train_loss: 0.04304132 test_loss: 1.03285205\n",
            "  Epoch 82/200\t Time: 7.218\t train_loss: 0.03111165 test_loss: 0.02092059\n",
            "  Epoch 83/200\t Time: 7.224\t train_loss: 0.03616725 test_loss: 0.30092967\n",
            "  Epoch 84/200\t Time: 7.193\t train_loss: 0.03439772 test_loss: 0.69208252\n",
            "  Epoch 85/200\t Time: 7.240\t train_loss: 0.04551655 test_loss: 1.22020459\n",
            "  Epoch 86/200\t Time: 7.249\t train_loss: 0.02608117 test_loss: 0.09099412\n",
            "  Epoch 87/200\t Time: 7.228\t train_loss: 0.02777458 test_loss: 0.07741956\n",
            "  Epoch 88/200\t Time: 7.224\t train_loss: 0.03000774 test_loss: 0.54258633\n",
            "  Epoch 89/200\t Time: 7.310\t train_loss: 0.02847664 test_loss: 0.60594213\n",
            "  Epoch 90/200\t Time: 7.258\t train_loss: 0.04911152 test_loss: 1.83797979\n",
            "  Epoch 91/200\t Time: 7.294\t train_loss: 0.04386989 test_loss: 0.88326395\n",
            "  Epoch 92/200\t Time: 7.275\t train_loss: 0.04153801 test_loss: 0.84230673\n",
            "  Epoch 93/200\t Time: 7.261\t train_loss: 0.03662681 test_loss: 0.60700417\n",
            "  Epoch 94/200\t Time: 7.220\t train_loss: 0.03553384 test_loss: 0.46265203\n",
            "  Epoch 95/200\t Time: 7.266\t train_loss: 0.03019017 test_loss: 0.50819647\n",
            "  Epoch 96/200\t Time: 7.292\t train_loss: 0.02150359 test_loss: 0.15729067\n",
            "  Epoch 97/200\t Time: 7.250\t train_loss: 0.02808260 test_loss: 0.19778277\n",
            "  Epoch 98/200\t Time: 7.164\t train_loss: 0.03315358 test_loss: 0.47656694\n",
            "  Epoch 99/200\t Time: 7.257\t train_loss: 0.02455543 test_loss: 0.06980273\n",
            "  Epoch 100/200\t Time: 7.314\t train_loss: 0.02093134 test_loss: 0.06902476\n",
            "  Epoch 101/200\t Time: 7.349\t train_loss: 0.02912671 test_loss: 0.49018019\n",
            "  Epoch 102/200\t Time: 7.292\t train_loss: 0.02376676 test_loss: 0.07496680\n",
            "  Epoch 103/200\t Time: 7.322\t train_loss: 0.02376687 test_loss: 0.02088860\n",
            "  Epoch 104/200\t Time: 7.290\t train_loss: 0.03144460 test_loss: 0.34211165\n",
            "  Epoch 105/200\t Time: 7.215\t train_loss: 0.03233402 test_loss: 0.68410575\n",
            "  Epoch 106/200\t Time: 7.270\t train_loss: 0.06503560 test_loss: 3.26552153\n",
            "  Epoch 107/200\t Time: 7.292\t train_loss: 0.03202474 test_loss: 0.01779571\n",
            "  Epoch 108/200\t Time: 7.250\t train_loss: 0.02630618 test_loss: 0.02009901\n",
            "  Epoch 109/200\t Time: 7.131\t train_loss: 0.02578141 test_loss: 0.02933074\n",
            "  Epoch 110/200\t Time: 7.212\t train_loss: 0.04167333 test_loss: 0.31287372\n",
            "  Epoch 111/200\t Time: 7.297\t train_loss: 0.02942748 test_loss: 0.27983683\n",
            "  Epoch 112/200\t Time: 7.235\t train_loss: 0.03498766 test_loss: 0.98409903\n",
            "  Epoch 113/200\t Time: 7.238\t train_loss: 0.03325654 test_loss: 0.72893107\n",
            "  Epoch 114/200\t Time: 7.215\t train_loss: 0.08376673 test_loss: 4.95234966\n",
            "  Epoch 115/200\t Time: 7.233\t train_loss: 0.04089401 test_loss: 0.24494307\n",
            "  Epoch 116/200\t Time: 7.171\t train_loss: 0.03521663 test_loss: 0.45556056\n",
            "  Epoch 117/200\t Time: 7.349\t train_loss: 0.02095586 test_loss: 0.09867501\n",
            "  Epoch 118/200\t Time: 7.248\t train_loss: 0.02851457 test_loss: 0.08517455\n",
            "  Epoch 119/200\t Time: 7.232\t train_loss: 0.03717126 test_loss: 0.79677647\n",
            "  Epoch 120/200\t Time: 7.175\t train_loss: 0.03173486 test_loss: 0.74231994\n",
            "  Epoch 121/200\t Time: 7.236\t train_loss: 0.02413423 test_loss: 0.51467323\n",
            "  Epoch 122/200\t Time: 7.218\t train_loss: 0.02444555 test_loss: 0.51669401\n",
            "  Epoch 123/200\t Time: 7.222\t train_loss: 0.02169648 test_loss: 0.03766438\n",
            "  Epoch 124/200\t Time: 7.275\t train_loss: 0.01992546 test_loss: 0.27996850\n",
            "  Epoch 125/200\t Time: 7.279\t train_loss: 0.02590618 test_loss: 0.57330579\n",
            "  Epoch 126/200\t Time: 7.256\t train_loss: 0.02344487 test_loss: 0.07870812\n",
            "  Epoch 127/200\t Time: 7.240\t train_loss: 0.02017979 test_loss: 0.16048180\n",
            "  Epoch 128/200\t Time: 7.200\t train_loss: 0.02630966 test_loss: 0.42065769\n",
            "  Epoch 129/200\t Time: 7.212\t train_loss: 0.02236816 test_loss: 0.10938273\n",
            "  Epoch 130/200\t Time: 7.223\t train_loss: 0.05395361 test_loss: 2.42282772\n",
            "  Epoch 131/200\t Time: 7.249\t train_loss: 0.03561030 test_loss: 0.03932094\n",
            "  Epoch 132/200\t Time: 7.241\t train_loss: 0.01935016 test_loss: 0.02935706\n",
            "  Epoch 133/200\t Time: 7.248\t train_loss: 0.02877481 test_loss: 1.01167667\n",
            "  Epoch 134/200\t Time: 7.389\t train_loss: 0.03054062 test_loss: 0.42613140\n",
            "  Epoch 135/200\t Time: 7.329\t train_loss: 0.01973153 test_loss: 0.11842903\n",
            "  Epoch 136/200\t Time: 7.259\t train_loss: 0.01636626 test_loss: 0.01750048\n",
            "  Epoch 137/200\t Time: 7.297\t train_loss: 0.02517535 test_loss: 0.07478482\n",
            "  Epoch 138/200\t Time: 7.322\t train_loss: 0.02517947 test_loss: 0.39117500\n",
            "  Epoch 139/200\t Time: 7.319\t train_loss: 0.02045766 test_loss: 0.03961510\n",
            "  Epoch 140/200\t Time: 7.318\t train_loss: 0.03116558 test_loss: 0.90387249\n",
            "  Epoch 141/200\t Time: 7.231\t train_loss: 0.03624022 test_loss: 1.39725065\n",
            "  Epoch 142/200\t Time: 7.303\t train_loss: 0.02938268 test_loss: 0.33564848\n",
            "  Epoch 143/200\t Time: 7.367\t train_loss: 0.03585827 test_loss: 1.00562334\n",
            "  Epoch 144/200\t Time: 7.298\t train_loss: 0.03098891 test_loss: 0.49509487\n",
            "  Epoch 145/200\t Time: 7.243\t train_loss: 0.03510903 test_loss: 1.16771173\n",
            "  Epoch 146/200\t Time: 7.343\t train_loss: 0.02454464 test_loss: 0.03780774\n",
            "  Epoch 147/200\t Time: 7.250\t train_loss: 0.03219189 test_loss: 1.04606652\n",
            "  Epoch 148/200\t Time: 7.280\t train_loss: 0.02119608 test_loss: 0.02606761\n",
            "  Epoch 149/200\t Time: 7.259\t train_loss: 0.02270797 test_loss: 0.57475233\n",
            "  Epoch 150/200\t Time: 7.306\t train_loss: 0.02153544 test_loss: 0.39132875\n",
            "  Epoch 151/200\t Time: 7.257\t train_loss: 0.02617550 test_loss: 0.83278322\n",
            "  Epoch 152/200\t Time: 7.258\t train_loss: 0.04068940 test_loss: 1.56329012\n",
            "  Epoch 153/200\t Time: 7.228\t train_loss: 0.03163418 test_loss: 0.83880711\n",
            "  Epoch 154/200\t Time: 7.210\t train_loss: 0.36942669 test_loss: 27.65018463\n",
            "  Epoch 155/200\t Time: 7.282\t train_loss: 0.10587656 test_loss: 0.27649722\n",
            "  Epoch 156/200\t Time: 7.321\t train_loss: 0.02541741 test_loss: 0.19125751\n",
            "  Epoch 157/200\t Time: 7.238\t train_loss: 0.02413944 test_loss: 0.41451001\n",
            "  Epoch 158/200\t Time: 7.238\t train_loss: 0.02135054 test_loss: 0.01291959\n",
            "  Epoch 159/200\t Time: 7.287\t train_loss: 0.02088562 test_loss: 0.28846759\n",
            "  Epoch 160/200\t Time: 7.237\t train_loss: 0.02116423 test_loss: 0.43956071\n",
            "  Epoch 161/200\t Time: 7.130\t train_loss: 0.01897626 test_loss: 0.07174277\n",
            "  Epoch 162/200\t Time: 7.261\t train_loss: 0.02027010 test_loss: 0.42812887\n",
            "  Epoch 163/200\t Time: 7.287\t train_loss: 0.02176264 test_loss: 0.30882931\n",
            "  Epoch 164/200\t Time: 7.205\t train_loss: 0.02468964 test_loss: 0.20651942\n",
            "  Epoch 165/200\t Time: 7.193\t train_loss: 0.01708435 test_loss: 0.02001753\n",
            "  Epoch 166/200\t Time: 7.277\t train_loss: 0.01725583 test_loss: 0.17555347\n",
            "  Epoch 167/200\t Time: 7.286\t train_loss: 0.02441838 test_loss: 0.71436769\n",
            "  Epoch 168/200\t Time: 7.246\t train_loss: 0.02361183 test_loss: 0.26656887\n",
            "  Epoch 169/200\t Time: 7.261\t train_loss: 0.07604489 test_loss: 4.96906471\n",
            "  Epoch 170/200\t Time: 7.220\t train_loss: 0.03809534 test_loss: 0.35284343\n",
            "  Epoch 171/200\t Time: 7.198\t train_loss: 0.02157169 test_loss: 0.07539205\n",
            "  Epoch 172/200\t Time: 7.282\t train_loss: 0.02692936 test_loss: 0.97363913\n",
            "  Epoch 173/200\t Time: 7.363\t train_loss: 0.03221641 test_loss: 0.75894225\n",
            "  Epoch 174/200\t Time: 7.363\t train_loss: 0.02052059 test_loss: 0.01799250\n",
            "  Epoch 175/200\t Time: 7.334\t train_loss: 0.01910451 test_loss: 0.02447447\n",
            "  Epoch 176/200\t Time: 7.342\t train_loss: 0.04579119 test_loss: 2.44770575\n",
            "  Epoch 177/200\t Time: 7.313\t train_loss: 0.02562546 test_loss: 0.20339337\n",
            "  Epoch 178/200\t Time: 7.288\t train_loss: 0.01544398 test_loss: 0.20602314\n",
            "  Epoch 179/200\t Time: 7.269\t train_loss: 0.01814292 test_loss: 0.11563512\n",
            "  Epoch 180/200\t Time: 7.280\t train_loss: 0.05052778 test_loss: 3.00845528\n",
            "  Epoch 181/200\t Time: 7.283\t train_loss: 0.02514665 test_loss: 0.02513872\n",
            "  Epoch 182/200\t Time: 7.245\t train_loss: 0.02727810 test_loss: 1.20880485\n",
            "  Epoch 183/200\t Time: 7.221\t train_loss: 0.02950417 test_loss: 0.77606606\n",
            "  Epoch 184/200\t Time: 7.223\t train_loss: 0.02720004 test_loss: 0.88224649\n",
            "  Epoch 185/200\t Time: 7.241\t train_loss: 0.02616685 test_loss: 0.88694423\n",
            "  Epoch 186/200\t Time: 7.209\t train_loss: 0.02509414 test_loss: 0.47349852\n",
            "  Epoch 187/200\t Time: 7.220\t train_loss: 0.03013965 test_loss: 1.37120068\n",
            "  Epoch 188/200\t Time: 7.207\t train_loss: 0.02407955 test_loss: 0.43386686\n",
            "  Epoch 189/200\t Time: 7.191\t train_loss: 0.01623418 test_loss: 0.13677064\n",
            "  Epoch 190/200\t Time: 7.216\t train_loss: 0.01427236 test_loss: 0.08754083\n",
            "  Epoch 191/200\t Time: 7.226\t train_loss: 0.02121460 test_loss: 0.74919546\n",
            "  Epoch 192/200\t Time: 7.201\t train_loss: 0.01903900 test_loss: 0.18251109\n",
            "  Epoch 193/200\t Time: 7.180\t train_loss: 0.01756189 test_loss: 0.33057898\n",
            "  Epoch 194/200\t Time: 7.241\t train_loss: 0.02828448 test_loss: 1.07282245\n",
            "  Epoch 195/200\t Time: 7.260\t train_loss: 0.01704417 test_loss: 0.08674049\n",
            "  Epoch 196/200\t Time: 7.244\t train_loss: 0.01521310 test_loss: 0.25192720\n",
            "  Epoch 197/200\t Time: 7.165\t train_loss: 0.01320751 test_loss: 0.07371828\n",
            "  Epoch 198/200\t Time: 7.263\t train_loss: 0.02660847 test_loss: 0.82197344\n",
            "  Epoch 199/200\t Time: 7.343\t train_loss: 0.01636328 test_loss: 0.05812647\n",
            "  Epoch 200/200\t Time: 7.297\t train_loss: 0.02038049 test_loss: 0.38031256\n",
            "Finished training.\n",
            "training net: 4\n",
            "Starting training...\n",
            "  Epoch 1/200\t Time: 5.027\t train_loss: 536.95310263 test_loss: 8.66384697\n",
            "  Epoch 2/200\t Time: 5.044\t train_loss: 5.70045209 test_loss: 5.22157717\n",
            "  Epoch 3/200\t Time: 5.000\t train_loss: 1.65523612 test_loss: 9.73937702\n",
            "  Epoch 4/200\t Time: 5.024\t train_loss: 2.07454778 test_loss: 5.02217674\n",
            "  Epoch 5/200\t Time: 5.051\t train_loss: 1.37546875 test_loss: 10.88729095\n",
            "  Epoch 6/200\t Time: 5.038\t train_loss: 0.96616796 test_loss: 2.03874683\n",
            "  Epoch 7/200\t Time: 5.055\t train_loss: 0.99314135 test_loss: 1.06815732\n",
            "  Epoch 8/200\t Time: 5.062\t train_loss: 0.62823827 test_loss: 1.37854147\n",
            "  Epoch 9/200\t Time: 5.040\t train_loss: 0.59839593 test_loss: 0.60472274\n",
            "  Epoch 10/200\t Time: 5.021\t train_loss: 1.85579556 test_loss: 4.55604601\n",
            "  Epoch 11/200\t Time: 5.054\t train_loss: 1.24913911 test_loss: 0.30756414\n",
            "  Epoch 12/200\t Time: 5.034\t train_loss: 0.58404912 test_loss: 0.53442180\n",
            "  Epoch 13/200\t Time: 5.058\t train_loss: 0.53714782 test_loss: 1.15432739\n",
            "  Epoch 14/200\t Time: 5.020\t train_loss: 0.33473674 test_loss: 0.21565542\n",
            "  Epoch 15/200\t Time: 5.005\t train_loss: 0.50518897 test_loss: 4.21401358\n",
            "  Epoch 16/200\t Time: 5.045\t train_loss: 0.70399804 test_loss: 0.68040746\n",
            "  Epoch 17/200\t Time: 5.009\t train_loss: 0.51663371 test_loss: 0.78648961\n",
            "  Epoch 18/200\t Time: 5.075\t train_loss: 0.32239780 test_loss: 0.43976802\n",
            "  Epoch 19/200\t Time: 4.995\t train_loss: 0.42854252 test_loss: 1.78643632\n",
            "  Epoch 20/200\t Time: 5.054\t train_loss: 0.46180394 test_loss: 2.66001892\n",
            "  Epoch 21/200\t Time: 5.063\t train_loss: 0.42756060 test_loss: 0.24841926\n",
            "  Epoch 22/200\t Time: 5.031\t train_loss: 0.51057558 test_loss: 3.06449270\n",
            "  Epoch 23/200\t Time: 5.077\t train_loss: 0.45540300 test_loss: 0.27918589\n",
            "  Epoch 24/200\t Time: 5.042\t train_loss: 0.27737344 test_loss: 2.03500557\n",
            "  Epoch 25/200\t Time: 5.104\t train_loss: 0.40438494 test_loss: 0.55643731\n",
            "  Epoch 26/200\t Time: 5.118\t train_loss: 0.51778120 test_loss: 0.70006454\n",
            "  Epoch 27/200\t Time: 5.096\t train_loss: 0.25951096 test_loss: 2.02724433\n",
            "  Epoch 28/200\t Time: 5.092\t train_loss: 0.33714716 test_loss: 0.12763888\n",
            "  Epoch 29/200\t Time: 5.133\t train_loss: 0.27187341 test_loss: 0.22507212\n",
            "  Epoch 30/200\t Time: 5.173\t train_loss: 0.12964246 test_loss: 0.06805743\n",
            "  LR scheduler: new learning rate is 0.01\n",
            "  Epoch 31/200\t Time: 5.110\t train_loss: 0.08697992 test_loss: 0.27615872\n",
            "  Epoch 32/200\t Time: 5.052\t train_loss: 0.09322928 test_loss: 0.32586849\n",
            "  Epoch 33/200\t Time: 5.041\t train_loss: 0.08133160 test_loss: 0.49434465\n",
            "  Epoch 34/200\t Time: 5.077\t train_loss: 0.10263454 test_loss: 0.44213843\n",
            "  Epoch 35/200\t Time: 5.057\t train_loss: 0.11283444 test_loss: 0.39982179\n",
            "  Epoch 36/200\t Time: 5.053\t train_loss: 0.07843356 test_loss: 0.27098650\n",
            "  Epoch 37/200\t Time: 5.026\t train_loss: 0.06508536 test_loss: 0.06762290\n",
            "  Epoch 38/200\t Time: 5.078\t train_loss: 0.07573360 test_loss: 0.23044395\n",
            "  Epoch 39/200\t Time: 4.980\t train_loss: 0.07820157 test_loss: 0.09757992\n",
            "  Epoch 40/200\t Time: 5.033\t train_loss: 0.08356476 test_loss: 0.03389118\n",
            "  LR scheduler: new learning rate is 0.001\n",
            "  Epoch 41/200\t Time: 5.129\t train_loss: 0.07901919 test_loss: 0.16310143\n",
            "  Epoch 42/200\t Time: 5.077\t train_loss: 0.08084166 test_loss: 0.07192386\n",
            "  Epoch 43/200\t Time: 5.047\t train_loss: 0.07392678 test_loss: 0.12357932\n",
            "  Epoch 44/200\t Time: 5.087\t train_loss: 0.06685484 test_loss: 0.31971130\n",
            "  Epoch 45/200\t Time: 5.056\t train_loss: 0.12735261 test_loss: 1.89230692\n",
            "  Epoch 46/200\t Time: 5.103\t train_loss: 0.06108579 test_loss: 0.27321225\n",
            "  Epoch 47/200\t Time: 5.065\t train_loss: 0.10112285 test_loss: 0.18968472\n",
            "  Epoch 48/200\t Time: 5.117\t train_loss: 0.09639337 test_loss: 0.07129512\n",
            "  Epoch 49/200\t Time: 5.030\t train_loss: 0.07361333 test_loss: 0.08735997\n",
            "  Epoch 50/200\t Time: 5.084\t train_loss: 0.07884062 test_loss: 0.25798663\n",
            "  Epoch 51/200\t Time: 5.028\t train_loss: 0.07691802 test_loss: 0.09240552\n",
            "  Epoch 52/200\t Time: 5.041\t train_loss: 0.09943464 test_loss: 1.07452774\n",
            "  Epoch 53/200\t Time: 5.039\t train_loss: 0.07657921 test_loss: 0.23066212\n",
            "  Epoch 54/200\t Time: 5.089\t train_loss: 0.06616818 test_loss: 0.29764101\n",
            "  Epoch 55/200\t Time: 5.065\t train_loss: 0.08173847 test_loss: 0.22289814\n",
            "  Epoch 56/200\t Time: 5.080\t train_loss: 0.06815645 test_loss: 0.27076328\n",
            "  Epoch 57/200\t Time: 5.076\t train_loss: 0.07637526 test_loss: 0.04906048\n",
            "  Epoch 58/200\t Time: 5.083\t train_loss: 0.07538779 test_loss: 0.47862560\n",
            "  Epoch 59/200\t Time: 5.084\t train_loss: 0.08589301 test_loss: 0.04214413\n",
            "  Epoch 60/200\t Time: 5.112\t train_loss: 0.07993191 test_loss: 0.06987314\n",
            "  Epoch 61/200\t Time: 5.139\t train_loss: 0.09476386 test_loss: 0.81639493\n",
            "  Epoch 62/200\t Time: 5.070\t train_loss: 0.09619271 test_loss: 0.52499104\n",
            "  Epoch 63/200\t Time: 5.063\t train_loss: 0.11485645 test_loss: 1.37524199\n",
            "  Epoch 64/200\t Time: 5.114\t train_loss: 0.06755899 test_loss: 0.43158489\n",
            "  Epoch 65/200\t Time: 5.128\t train_loss: 0.07696694 test_loss: 0.29235321\n",
            "  Epoch 66/200\t Time: 5.099\t train_loss: 0.06662592 test_loss: 0.07086334\n",
            "  Epoch 67/200\t Time: 5.037\t train_loss: 0.05774021 test_loss: 0.18588662\n",
            "  Epoch 68/200\t Time: 5.055\t train_loss: 0.09609747 test_loss: 0.77395105\n",
            "  Epoch 69/200\t Time: 5.091\t train_loss: 0.07409246 test_loss: 0.19471487\n",
            "  Epoch 70/200\t Time: 5.086\t train_loss: 0.07258597 test_loss: 0.27995494\n",
            "  Epoch 71/200\t Time: 5.108\t train_loss: 0.12789813 test_loss: 2.16094184\n",
            "  Epoch 72/200\t Time: 5.114\t train_loss: 0.08022523 test_loss: 0.21661806\n",
            "  Epoch 73/200\t Time: 5.118\t train_loss: 0.11045667 test_loss: 1.35846782\n",
            "  Epoch 74/200\t Time: 5.076\t train_loss: 0.11153262 test_loss: 0.47696152\n",
            "  Epoch 75/200\t Time: 5.087\t train_loss: 0.06175312 test_loss: 0.09927329\n",
            "  Epoch 76/200\t Time: 5.048\t train_loss: 0.08848613 test_loss: 0.39679223\n",
            "  Epoch 77/200\t Time: 5.139\t train_loss: 0.06046818 test_loss: 0.13041760\n",
            "  Epoch 78/200\t Time: 5.035\t train_loss: 0.09247960 test_loss: 0.28531867\n",
            "  Epoch 79/200\t Time: 5.164\t train_loss: 0.07180696 test_loss: 0.21897671\n",
            "  Epoch 80/200\t Time: 5.048\t train_loss: 0.11914872 test_loss: 1.98681295\n",
            "  Epoch 81/200\t Time: 5.146\t train_loss: 0.08550269 test_loss: 0.07257206\n",
            "  Epoch 82/200\t Time: 5.093\t train_loss: 0.15368762 test_loss: 3.26982617\n",
            "  Epoch 83/200\t Time: 5.160\t train_loss: 0.08423408 test_loss: 0.54797328\n",
            "  Epoch 84/200\t Time: 5.102\t train_loss: 0.17455882 test_loss: 3.95137501\n",
            "  Epoch 85/200\t Time: 5.129\t train_loss: 0.07146511 test_loss: 0.20098457\n",
            "  Epoch 86/200\t Time: 5.084\t train_loss: 0.08986023 test_loss: 0.40476501\n",
            "  Epoch 87/200\t Time: 5.133\t train_loss: 0.07703787 test_loss: 0.04708165\n",
            "  Epoch 88/200\t Time: 5.070\t train_loss: 0.12085117 test_loss: 1.95310414\n",
            "  Epoch 89/200\t Time: 5.105\t train_loss: 0.07108856 test_loss: 0.07168901\n",
            "  Epoch 90/200\t Time: 5.033\t train_loss: 0.06487948 test_loss: 0.04881205\n",
            "  Epoch 91/200\t Time: 5.098\t train_loss: 0.07332021 test_loss: 0.25275716\n",
            "  Epoch 92/200\t Time: 5.071\t train_loss: 0.07913585 test_loss: 0.80302691\n",
            "  Epoch 93/200\t Time: 5.104\t train_loss: 0.06885352 test_loss: 0.25509411\n",
            "  Epoch 94/200\t Time: 5.031\t train_loss: 0.07429462 test_loss: 0.25146076\n",
            "  Epoch 95/200\t Time: 5.093\t train_loss: 0.13585485 test_loss: 2.13668108\n",
            "  Epoch 96/200\t Time: 5.078\t train_loss: 0.11090080 test_loss: 1.20190871\n",
            "  Epoch 97/200\t Time: 5.071\t train_loss: 0.29163103 test_loss: 6.95795727\n",
            "  Epoch 98/200\t Time: 5.111\t train_loss: 0.10503128 test_loss: 0.17117901\n",
            "  Epoch 99/200\t Time: 5.118\t train_loss: 0.07048993 test_loss: 0.31635165\n",
            "  Epoch 100/200\t Time: 5.154\t train_loss: 0.07918892 test_loss: 0.10802697\n",
            "  Epoch 101/200\t Time: 5.187\t train_loss: 0.07976709 test_loss: 0.07560235\n",
            "  Epoch 102/200\t Time: 5.162\t train_loss: 0.10790062 test_loss: 1.50729775\n",
            "  Epoch 103/200\t Time: 5.109\t train_loss: 0.10190543 test_loss: 0.70354676\n",
            "  Epoch 104/200\t Time: 5.146\t train_loss: 0.07636471 test_loss: 0.20265907\n",
            "  Epoch 105/200\t Time: 5.104\t train_loss: 0.07896256 test_loss: 0.23082423\n",
            "  Epoch 106/200\t Time: 5.113\t train_loss: 0.06916963 test_loss: 0.13117692\n",
            "  Epoch 107/200\t Time: 5.149\t train_loss: 0.08229442 test_loss: 0.13734254\n",
            "  Epoch 108/200\t Time: 5.108\t train_loss: 0.05578331 test_loss: 0.14814061\n",
            "  Epoch 109/200\t Time: 5.104\t train_loss: 0.06569355 test_loss: 0.27352387\n",
            "  Epoch 110/200\t Time: 5.122\t train_loss: 0.06830947 test_loss: 0.14610386\n",
            "  Epoch 111/200\t Time: 5.047\t train_loss: 0.06110950 test_loss: 0.04371718\n",
            "  Epoch 112/200\t Time: 5.078\t train_loss: 0.12332512 test_loss: 2.27158213\n",
            "  Epoch 113/200\t Time: 5.104\t train_loss: 0.08953684 test_loss: 0.33558202\n",
            "  Epoch 114/200\t Time: 5.139\t train_loss: 0.13126922 test_loss: 2.43978643\n",
            "  Epoch 115/200\t Time: 5.124\t train_loss: 0.09232813 test_loss: 1.03194892\n",
            "  Epoch 116/200\t Time: 5.135\t train_loss: 0.07947968 test_loss: 0.18587598\n",
            "  Epoch 117/200\t Time: 5.146\t train_loss: 0.07716370 test_loss: 0.26125035\n",
            "  Epoch 118/200\t Time: 5.140\t train_loss: 0.22463532 test_loss: 4.65596962\n",
            "  Epoch 119/200\t Time: 5.145\t train_loss: 0.08697765 test_loss: 0.48479533\n",
            "  Epoch 120/200\t Time: 5.133\t train_loss: 0.06766733 test_loss: 0.14459385\n",
            "  Epoch 121/200\t Time: 5.098\t train_loss: 0.06798780 test_loss: 0.14273173\n",
            "  Epoch 122/200\t Time: 5.182\t train_loss: 0.05528375 test_loss: 0.24905784\n",
            "  Epoch 123/200\t Time: 5.153\t train_loss: 0.07773273 test_loss: 0.18430483\n",
            "  Epoch 124/200\t Time: 5.105\t train_loss: 0.06446316 test_loss: 0.05642412\n",
            "  Epoch 125/200\t Time: 5.138\t train_loss: 0.06200208 test_loss: 0.31909776\n",
            "  Epoch 126/200\t Time: 5.123\t train_loss: 0.11071697 test_loss: 1.39750218\n",
            "  Epoch 127/200\t Time: 5.096\t train_loss: 0.06393066 test_loss: 0.08854139\n",
            "  Epoch 128/200\t Time: 5.158\t train_loss: 0.05899466 test_loss: 0.08420952\n",
            "  Epoch 129/200\t Time: 5.122\t train_loss: 0.13169627 test_loss: 1.96881151\n",
            "  Epoch 130/200\t Time: 5.159\t train_loss: 0.06220695 test_loss: 0.12896660\n",
            "  Epoch 131/200\t Time: 5.162\t train_loss: 0.10071427 test_loss: 1.01100552\n",
            "  Epoch 132/200\t Time: 5.084\t train_loss: 0.06087523 test_loss: 0.12291081\n",
            "  Epoch 133/200\t Time: 5.077\t train_loss: 0.05616949 test_loss: 0.14716288\n",
            "  Epoch 134/200\t Time: 5.086\t train_loss: 0.19068710 test_loss: 4.22034931\n",
            "  Epoch 135/200\t Time: 5.118\t train_loss: 0.14464424 test_loss: 2.84411049\n",
            "  Epoch 136/200\t Time: 5.107\t train_loss: 0.09931739 test_loss: 1.20479965\n",
            "  Epoch 137/200\t Time: 5.099\t train_loss: 0.11038099 test_loss: 1.25732970\n",
            "  Epoch 138/200\t Time: 5.098\t train_loss: 0.15224588 test_loss: 2.38304663\n",
            "  Epoch 139/200\t Time: 5.161\t train_loss: 0.10014754 test_loss: 1.04342675\n",
            "  Epoch 140/200\t Time: 5.114\t train_loss: 0.11402458 test_loss: 0.14378065\n",
            "  Epoch 141/200\t Time: 5.126\t train_loss: 0.08811025 test_loss: 0.87530184\n",
            "  Epoch 142/200\t Time: 5.127\t train_loss: 0.08413062 test_loss: 0.11140102\n",
            "  Epoch 143/200\t Time: 5.103\t train_loss: 0.06009688 test_loss: 0.05336699\n",
            "  Epoch 144/200\t Time: 5.081\t train_loss: 0.08556813 test_loss: 0.50511259\n",
            "  Epoch 145/200\t Time: 5.109\t train_loss: 0.08901076 test_loss: 0.75568247\n",
            "  Epoch 146/200\t Time: 5.108\t train_loss: 0.07793224 test_loss: 0.08472267\n",
            "  Epoch 147/200\t Time: 5.099\t train_loss: 0.08900964 test_loss: 0.80070794\n",
            "  Epoch 148/200\t Time: 5.245\t train_loss: 0.08074059 test_loss: 0.10397660\n",
            "  Epoch 149/200\t Time: 5.045\t train_loss: 0.14160037 test_loss: 2.81935787\n",
            "  Epoch 150/200\t Time: 5.070\t train_loss: 0.08195875 test_loss: 0.35421422\n",
            "  Epoch 151/200\t Time: 5.140\t train_loss: 0.06394585 test_loss: 0.09203979\n",
            "  Epoch 152/200\t Time: 5.102\t train_loss: 0.05798013 test_loss: 0.24122575\n",
            "  Epoch 153/200\t Time: 5.113\t train_loss: 0.07710045 test_loss: 0.19016637\n",
            "  Epoch 154/200\t Time: 5.133\t train_loss: 0.07306990 test_loss: 0.05310520\n",
            "  Epoch 155/200\t Time: 5.109\t train_loss: 0.10348145 test_loss: 1.23349595\n",
            "  Epoch 156/200\t Time: 5.162\t train_loss: 0.10638316 test_loss: 1.36980069\n",
            "  Epoch 157/200\t Time: 5.102\t train_loss: 0.07093877 test_loss: 0.25095356\n",
            "  Epoch 158/200\t Time: 5.087\t train_loss: 0.07332666 test_loss: 0.21141100\n",
            "  Epoch 159/200\t Time: 5.037\t train_loss: 0.07423809 test_loss: 0.18981245\n",
            "  Epoch 160/200\t Time: 5.123\t train_loss: 0.08894657 test_loss: 0.49188912\n",
            "  Epoch 161/200\t Time: 5.139\t train_loss: 0.08641320 test_loss: 0.12229706\n",
            "  Epoch 162/200\t Time: 5.149\t train_loss: 0.06103270 test_loss: 0.14511648\n",
            "  Epoch 163/200\t Time: 5.169\t train_loss: 0.07586954 test_loss: 0.79321867\n",
            "  Epoch 164/200\t Time: 5.123\t train_loss: 0.07018911 test_loss: 0.22344914\n",
            "  Epoch 165/200\t Time: 5.083\t train_loss: 0.06701265 test_loss: 0.04623827\n",
            "  Epoch 166/200\t Time: 5.095\t train_loss: 0.16701009 test_loss: 3.51560140\n",
            "  Epoch 167/200\t Time: 5.078\t train_loss: 0.07594255 test_loss: 0.07337477\n",
            "  Epoch 168/200\t Time: 5.133\t train_loss: 0.06841954 test_loss: 0.08270626\n",
            "  Epoch 169/200\t Time: 5.092\t train_loss: 0.10886455 test_loss: 1.68375874\n",
            "  Epoch 170/200\t Time: 5.068\t train_loss: 0.10076143 test_loss: 1.05886865\n",
            "  Epoch 171/200\t Time: 5.053\t train_loss: 0.06028174 test_loss: 0.06003520\n",
            "  Epoch 172/200\t Time: 5.111\t train_loss: 0.13017013 test_loss: 2.22832727\n",
            "  Epoch 173/200\t Time: 5.078\t train_loss: 0.12486982 test_loss: 2.09551048\n",
            "  Epoch 174/200\t Time: 5.135\t train_loss: 0.06370956 test_loss: 0.11994791\n",
            "  Epoch 175/200\t Time: 5.063\t train_loss: 0.14641754 test_loss: 2.49310255\n",
            "  Epoch 176/200\t Time: 5.092\t train_loss: 0.07327609 test_loss: 0.30624056\n",
            "  Epoch 177/200\t Time: 5.105\t train_loss: 0.05736050 test_loss: 0.07097213\n",
            "  Epoch 178/200\t Time: 5.071\t train_loss: 0.08205872 test_loss: 0.40852445\n",
            "  Epoch 179/200\t Time: 5.070\t train_loss: 0.06028066 test_loss: 0.16966528\n",
            "  Epoch 180/200\t Time: 5.130\t train_loss: 0.05314677 test_loss: 0.03594391\n",
            "  Epoch 181/200\t Time: 5.070\t train_loss: 0.12477611 test_loss: 1.74639189\n",
            "  Epoch 182/200\t Time: 5.059\t train_loss: 0.08482063 test_loss: 0.28611478\n",
            "  Epoch 183/200\t Time: 5.089\t train_loss: 0.07367311 test_loss: 0.31914040\n",
            "  Epoch 184/200\t Time: 5.060\t train_loss: 0.07699956 test_loss: 0.26257885\n",
            "  Epoch 185/200\t Time: 5.101\t train_loss: 0.06092310 test_loss: 0.15149918\n",
            "  Epoch 186/200\t Time: 5.074\t train_loss: 0.05866985 test_loss: 0.10105165\n",
            "  Epoch 187/200\t Time: 5.072\t train_loss: 0.06800014 test_loss: 0.21173617\n",
            "  Epoch 188/200\t Time: 5.091\t train_loss: 0.15998770 test_loss: 3.54738522\n",
            "  Epoch 189/200\t Time: 5.136\t train_loss: 0.08226699 test_loss: 0.35963076\n",
            "  Epoch 190/200\t Time: 5.106\t train_loss: 0.08486205 test_loss: 0.53506947\n",
            "  Epoch 191/200\t Time: 5.098\t train_loss: 0.05132592 test_loss: 0.19334888\n",
            "  Epoch 192/200\t Time: 5.075\t train_loss: 0.06411473 test_loss: 0.16595626\n",
            "  Epoch 193/200\t Time: 5.069\t train_loss: 0.06013907 test_loss: 0.15804130\n",
            "  Epoch 194/200\t Time: 5.082\t train_loss: 0.06857813 test_loss: 0.62865561\n",
            "  Epoch 195/200\t Time: 5.085\t train_loss: 0.10042440 test_loss: 1.65908003\n",
            "  Epoch 196/200\t Time: 5.116\t train_loss: 0.08347512 test_loss: 0.72612083\n",
            "  Epoch 197/200\t Time: 5.067\t train_loss: 0.05110857 test_loss: 0.22155890\n",
            "  Epoch 198/200\t Time: 5.149\t train_loss: 0.06084447 test_loss: 0.32599652\n",
            "  Epoch 199/200\t Time: 5.039\t train_loss: 0.05148760 test_loss: 0.03455701\n",
            "  Epoch 200/200\t Time: 5.045\t train_loss: 0.06441188 test_loss: 0.13273311\n",
            "Finished training.\n",
            "training net: 5\n",
            "Starting training...\n",
            "  Epoch 1/200\t Time: 4.735\t train_loss: 908.93028577 test_loss: 23.70213699\n",
            "  Epoch 2/200\t Time: 4.705\t train_loss: 11.45464123 test_loss: 24.79733658\n",
            "  Epoch 3/200\t Time: 4.700\t train_loss: 2.56695067 test_loss: 2.29011250\n",
            "  Epoch 4/200\t Time: 4.705\t train_loss: 1.37369533 test_loss: 0.94344628\n",
            "  Epoch 5/200\t Time: 4.661\t train_loss: 1.18649846 test_loss: 4.85826302\n",
            "  Epoch 6/200\t Time: 4.729\t train_loss: 1.15388529 test_loss: 1.45113766\n",
            "  Epoch 7/200\t Time: 4.719\t train_loss: 0.88035014 test_loss: 1.63941777\n",
            "  Epoch 8/200\t Time: 4.760\t train_loss: 0.89360290 test_loss: 0.61946452\n",
            "  Epoch 9/200\t Time: 4.686\t train_loss: 0.66031831 test_loss: 1.40284228\n",
            "  Epoch 10/200\t Time: 4.734\t train_loss: 0.60263147 test_loss: 1.92050910\n",
            "  Epoch 11/200\t Time: 4.723\t train_loss: 0.67873682 test_loss: 1.06193113\n",
            "  Epoch 12/200\t Time: 4.727\t train_loss: 0.68984192 test_loss: 1.36126208\n",
            "  Epoch 13/200\t Time: 4.683\t train_loss: 0.53298832 test_loss: 0.51139355\n",
            "  Epoch 14/200\t Time: 4.704\t train_loss: 0.31716556 test_loss: 0.23476920\n",
            "  Epoch 15/200\t Time: 4.675\t train_loss: 0.28240852 test_loss: 0.52364492\n",
            "  Epoch 16/200\t Time: 4.682\t train_loss: 0.37184417 test_loss: 0.38252798\n",
            "  Epoch 17/200\t Time: 4.641\t train_loss: 0.31580437 test_loss: 0.32076716\n",
            "  Epoch 18/200\t Time: 4.636\t train_loss: 0.55582728 test_loss: 5.64752054\n",
            "  Epoch 19/200\t Time: 4.706\t train_loss: 0.33734102 test_loss: 0.19281174\n",
            "  Epoch 20/200\t Time: 4.648\t train_loss: 0.21928760 test_loss: 0.39373013\n",
            "  Epoch 21/200\t Time: 4.744\t train_loss: 0.28822759 test_loss: 2.04373527\n",
            "  Epoch 22/200\t Time: 4.688\t train_loss: 0.35169441 test_loss: 0.33384210\n",
            "  Epoch 23/200\t Time: 4.736\t train_loss: 0.31390813 test_loss: 0.24860704\n",
            "  Epoch 24/200\t Time: 4.762\t train_loss: 0.30032526 test_loss: 0.45291179\n",
            "  Epoch 25/200\t Time: 4.779\t train_loss: 0.33398990 test_loss: 0.42216316\n",
            "  Epoch 26/200\t Time: 4.711\t train_loss: 0.25252634 test_loss: 0.30913553\n",
            "  Epoch 27/200\t Time: 4.731\t train_loss: 0.31803660 test_loss: 0.70019943\n",
            "  Epoch 28/200\t Time: 4.648\t train_loss: 0.20942196 test_loss: 0.45363691\n",
            "  Epoch 29/200\t Time: 4.709\t train_loss: 0.17581043 test_loss: 0.06338395\n",
            "  Epoch 30/200\t Time: 4.659\t train_loss: 0.22982432 test_loss: 0.27802336\n",
            "  LR scheduler: new learning rate is 0.01\n",
            "  Epoch 31/200\t Time: 4.687\t train_loss: 0.17666178 test_loss: 0.14840579\n",
            "  Epoch 32/200\t Time: 4.633\t train_loss: 0.12888174 test_loss: 0.23710413\n",
            "  Epoch 33/200\t Time: 4.702\t train_loss: 0.13574242 test_loss: 0.41165656\n",
            "  Epoch 34/200\t Time: 4.695\t train_loss: 0.10607507 test_loss: 0.22036700\n",
            "  Epoch 35/200\t Time: 4.677\t train_loss: 0.14701321 test_loss: 1.07698369\n",
            "  Epoch 36/200\t Time: 4.750\t train_loss: 0.20459224 test_loss: 2.42268348\n",
            "  Epoch 37/200\t Time: 4.719\t train_loss: 0.14318500 test_loss: 0.09643898\n",
            "  Epoch 38/200\t Time: 4.682\t train_loss: 0.11531661 test_loss: 0.41501325\n",
            "  Epoch 39/200\t Time: 4.697\t train_loss: 0.15766673 test_loss: 0.20251261\n",
            "  Epoch 40/200\t Time: 4.703\t train_loss: 0.12614014 test_loss: 0.12729147\n",
            "  LR scheduler: new learning rate is 0.001\n",
            "  Epoch 41/200\t Time: 4.710\t train_loss: 0.14271601 test_loss: 0.19777243\n",
            "  Epoch 42/200\t Time: 4.654\t train_loss: 0.20066933 test_loss: 2.35706329\n",
            "  Epoch 43/200\t Time: 4.754\t train_loss: 0.16564674 test_loss: 0.17238383\n",
            "  Epoch 44/200\t Time: 4.658\t train_loss: 0.14567588 test_loss: 0.24321243\n",
            "  Epoch 45/200\t Time: 4.699\t train_loss: 0.13204012 test_loss: 0.18280357\n",
            "  Epoch 46/200\t Time: 4.718\t train_loss: 0.11159383 test_loss: 0.06746364\n",
            "  Epoch 47/200\t Time: 4.730\t train_loss: 0.12072195 test_loss: 0.17939883\n",
            "  Epoch 48/200\t Time: 4.697\t train_loss: 0.10310253 test_loss: 0.41283262\n",
            "  Epoch 49/200\t Time: 4.735\t train_loss: 0.09868900 test_loss: 0.08474538\n",
            "  Epoch 50/200\t Time: 4.727\t train_loss: 0.12268224 test_loss: 0.08768144\n",
            "  Epoch 51/200\t Time: 4.673\t train_loss: 0.16255640 test_loss: 0.17606892\n",
            "  Epoch 52/200\t Time: 4.706\t train_loss: 0.12535078 test_loss: 0.22196344\n",
            "  Epoch 53/200\t Time: 4.655\t train_loss: 0.11510089 test_loss: 0.15484218\n",
            "  Epoch 54/200\t Time: 4.684\t train_loss: 0.07500130 test_loss: 0.07105438\n",
            "  Epoch 55/200\t Time: 4.653\t train_loss: 0.12886468 test_loss: 0.10069163\n",
            "  Epoch 56/200\t Time: 4.690\t train_loss: 0.11723887 test_loss: 0.04458965\n",
            "  Epoch 57/200\t Time: 4.688\t train_loss: 0.14651769 test_loss: 0.16642030\n",
            "  Epoch 58/200\t Time: 4.689\t train_loss: 0.14906171 test_loss: 0.12746325\n",
            "  Epoch 59/200\t Time: 4.711\t train_loss: 0.13296012 test_loss: 0.06800506\n",
            "  Epoch 60/200\t Time: 4.670\t train_loss: 0.13629496 test_loss: 0.05179114\n",
            "  Epoch 61/200\t Time: 4.697\t train_loss: 0.12793274 test_loss: 0.13551079\n",
            "  Epoch 62/200\t Time: 4.665\t train_loss: 0.18386265 test_loss: 0.85726470\n",
            "  Epoch 63/200\t Time: 4.745\t train_loss: 0.11958384 test_loss: 0.15982121\n",
            "  Epoch 64/200\t Time: 4.670\t train_loss: 0.11706003 test_loss: 0.24003793\n",
            "  Epoch 65/200\t Time: 4.662\t train_loss: 0.16357525 test_loss: 0.10938726\n",
            "  Epoch 66/200\t Time: 4.640\t train_loss: 0.73543615 test_loss: 14.69226360\n",
            "  Epoch 67/200\t Time: 4.684\t train_loss: 0.14493549 test_loss: 0.48205763\n",
            "  Epoch 68/200\t Time: 4.704\t train_loss: 0.10843754 test_loss: 0.16900168\n",
            "  Epoch 69/200\t Time: 4.694\t train_loss: 0.19627285 test_loss: 2.52663851\n",
            "  Epoch 70/200\t Time: 4.682\t train_loss: 0.11838271 test_loss: 0.05976965\n",
            "  Epoch 71/200\t Time: 4.667\t train_loss: 0.27542519 test_loss: 3.66647148\n",
            "  Epoch 72/200\t Time: 4.712\t train_loss: 0.11823584 test_loss: 0.20706116\n",
            "  Epoch 73/200\t Time: 4.671\t train_loss: 0.08333164 test_loss: 0.14725614\n",
            "  Epoch 74/200\t Time: 4.769\t train_loss: 0.19538349 test_loss: 0.09407748\n",
            "  Epoch 75/200\t Time: 4.604\t train_loss: 0.20560978 test_loss: 2.57033157\n",
            "  Epoch 76/200\t Time: 4.694\t train_loss: 0.12011916 test_loss: 0.15051816\n",
            "  Epoch 77/200\t Time: 4.659\t train_loss: 0.09041981 test_loss: 0.19228914\n",
            "  Epoch 78/200\t Time: 4.651\t train_loss: 0.11220242 test_loss: 0.47157049\n",
            "  Epoch 79/200\t Time: 4.680\t train_loss: 0.25946450 test_loss: 3.87430501\n",
            "  Epoch 80/200\t Time: 4.659\t train_loss: 0.14178885 test_loss: 0.16494426\n",
            "  Epoch 81/200\t Time: 4.672\t train_loss: 0.12222201 test_loss: 0.82527333\n",
            "  Epoch 82/200\t Time: 4.698\t train_loss: 0.12987736 test_loss: 0.14543182\n",
            "  Epoch 83/200\t Time: 4.621\t train_loss: 0.09539916 test_loss: 0.13119081\n",
            "  Epoch 84/200\t Time: 4.677\t train_loss: 0.22407893 test_loss: 3.09094477\n",
            "  Epoch 85/200\t Time: 4.713\t train_loss: 0.22191505 test_loss: 0.05509483\n",
            "  Epoch 86/200\t Time: 4.660\t train_loss: 0.12326610 test_loss: 0.07485109\n",
            "  Epoch 87/200\t Time: 4.729\t train_loss: 0.13664647 test_loss: 0.46518052\n",
            "  Epoch 88/200\t Time: 4.715\t train_loss: 0.10256531 test_loss: 0.40633431\n",
            "  Epoch 89/200\t Time: 4.735\t train_loss: 0.11572429 test_loss: 0.06453326\n",
            "  Epoch 90/200\t Time: 4.741\t train_loss: 0.18003148 test_loss: 0.57251358\n",
            "  Epoch 91/200\t Time: 4.746\t train_loss: 0.08828185 test_loss: 0.15749808\n",
            "  Epoch 92/200\t Time: 4.745\t train_loss: 0.12966431 test_loss: 0.07762589\n",
            "  Epoch 93/200\t Time: 4.681\t train_loss: 0.18772747 test_loss: 0.06706840\n",
            "  Epoch 94/200\t Time: 4.650\t train_loss: 0.14496233 test_loss: 0.07571362\n",
            "  Epoch 95/200\t Time: 4.667\t train_loss: 0.11353587 test_loss: 0.15821832\n",
            "  Epoch 96/200\t Time: 4.687\t train_loss: 0.11870921 test_loss: 0.09772113\n",
            "  Epoch 97/200\t Time: 4.727\t train_loss: 0.09441438 test_loss: 0.05789681\n",
            "  Epoch 98/200\t Time: 4.722\t train_loss: 0.09239297 test_loss: 0.09868726\n",
            "  Epoch 99/200\t Time: 4.683\t train_loss: 0.19690166 test_loss: 2.28829813\n",
            "  Epoch 100/200\t Time: 4.681\t train_loss: 0.11121391 test_loss: 0.13504855\n",
            "  Epoch 101/200\t Time: 4.681\t train_loss: 0.09144587 test_loss: 0.05615117\n",
            "  Epoch 102/200\t Time: 4.662\t train_loss: 0.11223871 test_loss: 0.10995357\n",
            "  Epoch 103/200\t Time: 4.639\t train_loss: 0.24582725 test_loss: 2.57423711\n",
            "  Epoch 104/200\t Time: 4.691\t train_loss: 0.09139061 test_loss: 0.14786814\n",
            "  Epoch 105/200\t Time: 4.721\t train_loss: 0.10830627 test_loss: 0.44888866\n",
            "  Epoch 106/200\t Time: 4.656\t train_loss: 0.10967119 test_loss: 0.16624096\n",
            "  Epoch 107/200\t Time: 4.704\t train_loss: 0.13401602 test_loss: 0.21345374\n",
            "  Epoch 108/200\t Time: 4.697\t train_loss: 0.11226960 test_loss: 0.04078080\n",
            "  Epoch 109/200\t Time: 4.660\t train_loss: 0.13087295 test_loss: 0.11544981\n",
            "  Epoch 110/200\t Time: 4.749\t train_loss: 0.11412829 test_loss: 0.50392854\n",
            "  Epoch 111/200\t Time: 4.662\t train_loss: 0.11215514 test_loss: 0.42189047\n",
            "  Epoch 112/200\t Time: 4.738\t train_loss: 0.15226609 test_loss: 0.54428256\n",
            "  Epoch 113/200\t Time: 4.682\t train_loss: 0.09542877 test_loss: 0.10565968\n",
            "  Epoch 114/200\t Time: 4.768\t train_loss: 0.10849306 test_loss: 0.14590928\n",
            "  Epoch 115/200\t Time: 4.709\t train_loss: 0.11978097 test_loss: 0.25018877\n",
            "  Epoch 116/200\t Time: 4.697\t train_loss: 0.13086287 test_loss: 0.17913404\n",
            "  Epoch 117/200\t Time: 4.638\t train_loss: 0.10170504 test_loss: 0.43240079\n",
            "  Epoch 118/200\t Time: 4.725\t train_loss: 0.09980665 test_loss: 0.11980302\n",
            "  Epoch 119/200\t Time: 4.658\t train_loss: 0.08956824 test_loss: 0.23988168\n",
            "  Epoch 120/200\t Time: 4.684\t train_loss: 0.11015316 test_loss: 0.19781177\n",
            "  Epoch 121/200\t Time: 4.705\t train_loss: 0.10431767 test_loss: 0.17168190\n",
            "  Epoch 122/200\t Time: 4.664\t train_loss: 0.17247585 test_loss: 2.00634122\n",
            "  Epoch 123/200\t Time: 4.742\t train_loss: 0.14964204 test_loss: 0.30676985\n",
            "  Epoch 124/200\t Time: 4.638\t train_loss: 0.09719215 test_loss: 0.07040726\n",
            "  Epoch 125/200\t Time: 4.784\t train_loss: 0.17387582 test_loss: 0.09052619\n",
            "  Epoch 126/200\t Time: 4.654\t train_loss: 0.12659926 test_loss: 0.24373028\n",
            "  Epoch 127/200\t Time: 4.750\t train_loss: 0.07747892 test_loss: 0.06618601\n",
            "  Epoch 128/200\t Time: 4.700\t train_loss: 0.07941519 test_loss: 0.09019359\n",
            "  Epoch 129/200\t Time: 4.669\t train_loss: 0.10937426 test_loss: 0.24720040\n",
            "  Epoch 130/200\t Time: 4.648\t train_loss: 0.09469141 test_loss: 0.05119004\n",
            "  Epoch 131/200\t Time: 4.719\t train_loss: 0.10003128 test_loss: 0.22762522\n",
            "  Epoch 132/200\t Time: 4.732\t train_loss: 0.09221762 test_loss: 0.08893066\n",
            "  Epoch 133/200\t Time: 4.704\t train_loss: 0.11646831 test_loss: 0.07607625\n",
            "  Epoch 134/200\t Time: 4.674\t train_loss: 0.09174502 test_loss: 0.20335929\n",
            "  Epoch 135/200\t Time: 4.742\t train_loss: 0.08526997 test_loss: 0.12036054\n",
            "  Epoch 136/200\t Time: 4.659\t train_loss: 0.10299548 test_loss: 0.14543426\n",
            "  Epoch 137/200\t Time: 4.621\t train_loss: 0.08198360 test_loss: 0.17575493\n",
            "  Epoch 138/200\t Time: 4.725\t train_loss: 0.12082558 test_loss: 0.06703447\n",
            "  Epoch 139/200\t Time: 4.592\t train_loss: 0.15641563 test_loss: 0.17133649\n",
            "  Epoch 140/200\t Time: 4.643\t train_loss: 0.18601465 test_loss: 0.18903579\n",
            "  Epoch 141/200\t Time: 4.717\t train_loss: 0.11670227 test_loss: 0.19530179\n",
            "  Epoch 142/200\t Time: 4.648\t train_loss: 0.11836562 test_loss: 0.12169612\n",
            "  Epoch 143/200\t Time: 4.640\t train_loss: 0.09528297 test_loss: 0.05729457\n",
            "  Epoch 144/200\t Time: 4.724\t train_loss: 0.10286378 test_loss: 0.11135887\n",
            "  Epoch 145/200\t Time: 4.684\t train_loss: 0.10351694 test_loss: 0.16968623\n",
            "  Epoch 146/200\t Time: 4.638\t train_loss: 0.12668381 test_loss: 0.14802501\n",
            "  Epoch 147/200\t Time: 4.702\t train_loss: 0.11778389 test_loss: 0.13029984\n",
            "  Epoch 148/200\t Time: 4.721\t train_loss: 0.13267440 test_loss: 0.68754876\n",
            "  Epoch 149/200\t Time: 4.667\t train_loss: 0.08979933 test_loss: 0.06535963\n",
            "  Epoch 150/200\t Time: 4.725\t train_loss: 0.24742721 test_loss: 4.25911045\n",
            "  Epoch 151/200\t Time: 4.671\t train_loss: 0.13150761 test_loss: 0.03830614\n",
            "  Epoch 152/200\t Time: 4.730\t train_loss: 0.12924684 test_loss: 0.53828186\n",
            "  Epoch 153/200\t Time: 4.631\t train_loss: 0.11518250 test_loss: 0.56539565\n",
            "  Epoch 154/200\t Time: 4.696\t train_loss: 0.07988453 test_loss: 0.17666911\n",
            "  Epoch 155/200\t Time: 4.679\t train_loss: 0.10579606 test_loss: 0.14287739\n",
            "  Epoch 156/200\t Time: 4.681\t train_loss: 0.12591065 test_loss: 0.08478059\n",
            "  Epoch 157/200\t Time: 4.747\t train_loss: 0.11847809 test_loss: 0.27729255\n",
            "  Epoch 158/200\t Time: 4.723\t train_loss: 0.10945060 test_loss: 0.18212585\n",
            "  Epoch 159/200\t Time: 4.684\t train_loss: 0.33774272 test_loss: 4.18748808\n",
            "  Epoch 160/200\t Time: 4.695\t train_loss: 0.12570202 test_loss: 0.28926039\n",
            "  Epoch 161/200\t Time: 4.731\t train_loss: 0.08428627 test_loss: 0.09612376\n",
            "  Epoch 162/200\t Time: 4.636\t train_loss: 0.09458442 test_loss: 0.07540025\n",
            "  Epoch 163/200\t Time: 4.718\t train_loss: 0.09037950 test_loss: 0.09807804\n",
            "  Epoch 164/200\t Time: 4.729\t train_loss: 0.09761130 test_loss: 0.13647068\n",
            "  Epoch 165/200\t Time: 4.704\t train_loss: 0.12216978 test_loss: 0.45257762\n",
            "  Epoch 166/200\t Time: 4.671\t train_loss: 0.10695145 test_loss: 0.06736194\n",
            "  Epoch 167/200\t Time: 4.702\t train_loss: 0.18037164 test_loss: 0.15361150\n",
            "  Epoch 168/200\t Time: 4.668\t train_loss: 0.14621294 test_loss: 0.08523820\n",
            "  Epoch 169/200\t Time: 4.715\t train_loss: 0.09035521 test_loss: 0.15957066\n",
            "  Epoch 170/200\t Time: 4.721\t train_loss: 0.16376003 test_loss: 2.22233486\n",
            "  Epoch 171/200\t Time: 4.675\t train_loss: 0.21467734 test_loss: 0.80013019\n",
            "  Epoch 172/200\t Time: 4.694\t train_loss: 0.08891477 test_loss: 0.05777808\n",
            "  Epoch 173/200\t Time: 4.728\t train_loss: 0.10974818 test_loss: 0.56089419\n",
            "  Epoch 174/200\t Time: 4.676\t train_loss: 0.07577552 test_loss: 0.11236620\n",
            "  Epoch 175/200\t Time: 4.699\t train_loss: 0.09594419 test_loss: 0.08128449\n",
            "  Epoch 176/200\t Time: 4.706\t train_loss: 0.12557653 test_loss: 0.85556805\n",
            "  Epoch 177/200\t Time: 4.696\t train_loss: 0.09538590 test_loss: 0.22930494\n",
            "  Epoch 178/200\t Time: 4.682\t train_loss: 0.26690981 test_loss: 3.05984831\n",
            "  Epoch 179/200\t Time: 4.691\t train_loss: 0.12301268 test_loss: 0.80620331\n",
            "  Epoch 180/200\t Time: 4.698\t train_loss: 0.12177536 test_loss: 0.14612630\n",
            "  Epoch 181/200\t Time: 4.665\t train_loss: 0.16420847 test_loss: 0.65091342\n",
            "  Epoch 182/200\t Time: 4.738\t train_loss: 0.07562399 test_loss: 0.12017717\n",
            "  Epoch 183/200\t Time: 4.747\t train_loss: 0.10958005 test_loss: 0.16532274\n",
            "  Epoch 184/200\t Time: 4.675\t train_loss: 0.07743733 test_loss: 0.11531702\n",
            "  Epoch 185/200\t Time: 4.666\t train_loss: 0.10848947 test_loss: 0.22889179\n",
            "  Epoch 186/200\t Time: 4.621\t train_loss: 0.08826588 test_loss: 0.05318544\n",
            "  Epoch 187/200\t Time: 4.659\t train_loss: 0.11200099 test_loss: 0.05092929\n",
            "  Epoch 188/200\t Time: 4.727\t train_loss: 0.10423703 test_loss: 0.25277552\n",
            "  Epoch 189/200\t Time: 4.835\t train_loss: 0.08454124 test_loss: 0.13151787\n",
            "  Epoch 190/200\t Time: 4.679\t train_loss: 0.10836150 test_loss: 0.14799534\n",
            "  Epoch 191/200\t Time: 4.704\t train_loss: 0.10493392 test_loss: 0.16441186\n",
            "  Epoch 192/200\t Time: 4.746\t train_loss: 0.09549621 test_loss: 0.09865076\n",
            "  Epoch 193/200\t Time: 4.768\t train_loss: 0.15980953 test_loss: 0.09065569\n",
            "  Epoch 194/200\t Time: 4.687\t train_loss: 0.09655806 test_loss: 0.07396017\n",
            "  Epoch 195/200\t Time: 4.702\t train_loss: 0.10729517 test_loss: 0.04923285\n",
            "  Epoch 196/200\t Time: 4.702\t train_loss: 0.09136020 test_loss: 0.11518908\n",
            "  Epoch 197/200\t Time: 4.665\t train_loss: 0.09597136 test_loss: 0.21757558\n",
            "  Epoch 198/200\t Time: 4.667\t train_loss: 0.08510479 test_loss: 0.04989361\n",
            "  Epoch 199/200\t Time: 4.733\t train_loss: 0.07669791 test_loss: 0.16494885\n",
            "  Epoch 200/200\t Time: 4.707\t train_loss: 0.09685055 test_loss: 0.42411777\n",
            "Finished training.\n",
            "[]\n",
            "Starting testing...\n",
            "inup (17000, 8)\n",
            "g (17000, 8)\n",
            "[[ 4.49716460e-04  3.00498553e-04  2.37242577e-04  7.13062583e-06\n",
            "  -1.71575015e-05  4.30873930e-05 -2.79047876e-06 -2.63891634e-04]\n",
            " [ 3.00498553e-04  5.31146405e-04  3.72187327e-04 -3.84917930e-05\n",
            "  -4.29067280e-06  2.17269933e-04 -7.91487579e-05 -4.53809398e-04]\n",
            " [ 2.37242577e-04  3.72187327e-04  3.71724807e-04 -4.09464724e-05\n",
            "  -1.00165214e-05  1.89504205e-04 -8.05292257e-05 -3.75448627e-04]\n",
            " [ 7.13062583e-06 -3.84917930e-05 -4.09464724e-05  1.65741558e-04\n",
            "  -1.58493102e-04  8.51430091e-05  9.64239273e-05 -3.84622941e-05]\n",
            " [-1.71575015e-05 -4.29067280e-06 -1.00165214e-05 -1.58493102e-04\n",
            "   2.86634176e-04 -9.10759765e-05 -1.38583377e-04  1.37129382e-04]\n",
            " [ 4.30873930e-05  2.17269933e-04  1.89504205e-04  8.51430091e-05\n",
            "  -9.10759765e-05  2.80635207e-04 -3.06353323e-05 -2.63115218e-04]\n",
            " [-2.79047876e-06 -7.91487579e-05 -8.05292257e-05  9.64239273e-05\n",
            "  -1.38583377e-04 -3.06353323e-05  1.80412322e-04  4.45699868e-05]\n",
            " [-2.63891634e-04 -4.53809398e-04 -3.75448627e-04 -3.84622941e-05\n",
            "   1.37129382e-04 -2.63115218e-04  4.45699868e-05  5.90580220e-04]]\n",
            "[[  6836.78970825  -3816.8730085   -5193.79852798  -8145.34895588\n",
            "   -1857.43768039   7243.29454043    287.72066581    -73.73761633]\n",
            " [ -3816.8730085   11243.75404418   -879.0993246    6153.07475103\n",
            "   -2155.54767533  -5789.69683243  -2656.15227145   4897.72818061]\n",
            " [ -5193.79852798   -879.0993246   16951.1022605   11740.21771914\n",
            "    1630.28947936  -9212.26595382   -499.16640848   4099.4941543 ]\n",
            " [ -8145.34895588   6153.07475103  11740.21771914  31733.85099716\n",
            "    8424.73196622 -17934.75612965  -5998.72995903   1125.02380617]\n",
            " [ -1857.43768039  -2155.54767533   1630.28947936   8424.73196622\n",
            "   13014.33168839  -1576.19324774   6238.85224253  -5096.13892246]\n",
            " [  7243.29454043  -5789.69683243  -9212.26595382 -17934.75612965\n",
            "   -1576.19324774  18712.83572076   4990.2348847      89.47314492]\n",
            " [   287.72066581  -2656.15227145   -499.16640848  -5998.72995903\n",
            "    6238.85224253   4990.2348847   13716.80904426  -2881.02649156]\n",
            " [   -73.73761633   4897.72818061   4099.4941543    1125.02380617\n",
            "   -5096.13892246     89.47314492  -2881.02649156   9543.79445284]]\n",
            "md [      0.         9400178.69550645 9400148.79843922 ... 9416487.15969406\n",
            " 9387543.20214063 9400178.69550645]\n",
            "mu 9398294.386217896\n",
            "s 72343.97048821159\n",
            "Starting testing...\n",
            "inup (2000, 8)\n",
            "g (2000, 8)\n",
            "[[ 1.82058290e-02 -9.70976512e-04  6.50587622e-05 -2.11505348e-03\n",
            "  -1.24286185e-03 -2.19206295e-03  6.22311065e-03 -2.17399614e-03]\n",
            " [-9.70976512e-04  1.98144387e-02  1.44985926e-02 -2.20764937e-03\n",
            "  -6.33782624e-03  1.13295998e-02  5.15106454e-03 -9.64611782e-03]\n",
            " [ 6.50587622e-05  1.44985926e-02  2.40799569e-02  2.99246778e-04\n",
            "  -1.15442878e-02  1.29720591e-02  1.18105583e-02 -1.85657085e-02]\n",
            " [-2.11505348e-03 -2.20764937e-03  2.99246778e-04  8.50677495e-03\n",
            "  -3.50871757e-04  5.15386047e-03 -4.76568117e-03  5.87315518e-04]\n",
            " [-1.24286185e-03 -6.33782624e-03 -1.15442878e-02 -3.50871757e-04\n",
            "   2.21737532e-02 -1.11733139e-02 -5.39422859e-03  8.83052649e-03]\n",
            " [-2.19206295e-03  1.13295998e-02  1.29720591e-02  5.15386047e-03\n",
            "  -1.11733139e-02  2.42573304e-02 -6.37966652e-03 -8.94416225e-03]\n",
            " [ 6.22311065e-03  5.15106454e-03  1.18105583e-02 -4.76568117e-03\n",
            "  -5.39422859e-03 -6.37966652e-03  2.81191794e-02 -3.22708276e-03]\n",
            " [-2.17399614e-03 -9.64611782e-03 -1.85657085e-02  5.87315518e-04\n",
            "   8.83052649e-03 -8.94416225e-03 -3.22708276e-03  2.65479140e-02]]\n",
            "[[  70.98277906    5.04432144   58.12369722    7.75650504   -2.57505164\n",
            "   -28.50609236  -42.77115014   34.17505489]\n",
            " [   5.04432144  122.4940268   -71.67484019   64.60751954  -16.92600404\n",
            "   -45.28610134    2.14117924  -15.99944875]\n",
            " [  58.12369722  -71.67484019  400.19927778  -47.81893617   -4.9882214\n",
            "  -147.47806971 -188.69158811  188.68157253]\n",
            " [   7.75650504   64.60751954  -47.81893617  182.30900614  -15.91745823\n",
            "   -53.6040943    19.48894625  -23.76016134]\n",
            " [  -2.57505164  -16.92600404   -4.9882214   -15.91745823   74.74217551\n",
            "    50.05026393   27.1320203   -14.19800744]\n",
            " [ -28.50609236  -45.28610134 -147.47806971  -53.6040943    50.05026393\n",
            "   178.57421406  110.25482759  -63.82164455]\n",
            " [ -42.77115014    2.14117924 -188.69158811   19.48894625   27.1320203\n",
            "   110.25482759  147.18713334  -89.10068546]\n",
            " [  34.17505489  -15.99944875  188.68157253  -23.76016134  -14.19800744\n",
            "   -63.82164455  -89.10068546  139.51887578]]\n",
            "md [      0.         3621589.0546638  3637236.16846758 ... 3621653.88639209\n",
            " 3622967.34314738 3622967.34314738]\n",
            "mu 3620080.193759667\n",
            "s 81056.96644216267\n",
            "Starting testing...\n",
            "inup (10000, 8)\n",
            "g (10000, 8)\n",
            "[[ 4.30288014e-04 -1.66097568e-04 -2.45720725e-04 -2.94816454e-05\n",
            "  -1.37212322e-04 -3.03774314e-05 -6.38830343e-05  3.34830776e-04]\n",
            " [-1.66097568e-04  9.74111760e-04  3.25833038e-04 -2.74902901e-04\n",
            "   4.48548444e-04  1.12750108e-04  1.01278060e-04 -7.10145983e-04]\n",
            " [-2.45720725e-04  3.25833038e-04  5.00850876e-04 -1.38674973e-04\n",
            "   2.68044190e-04  1.39695049e-04  7.70647537e-05 -5.71208398e-04]\n",
            " [-2.94816454e-05 -2.74902901e-04 -1.38674973e-04  5.20833947e-04\n",
            "  -2.71350127e-04  1.63214097e-05  8.18597510e-05  2.68228039e-04]\n",
            " [-1.37212322e-04  4.48548444e-04  2.68044190e-04 -2.71350127e-04\n",
            "   4.49410383e-04  9.62313756e-05  4.83581420e-05 -5.96521120e-04]\n",
            " [-3.03774314e-05  1.12750108e-04  1.39695049e-04  1.63214097e-05\n",
            "   9.62313756e-05  4.16542826e-04  8.22142816e-06 -2.15796512e-04]\n",
            " [-6.38830343e-05  1.01278060e-04  7.70647537e-05  8.18597510e-05\n",
            "   4.83581420e-05  8.22142816e-06  3.64430376e-04  4.78845741e-05]\n",
            " [ 3.34830776e-04 -7.10145983e-04 -5.71208398e-04  2.68228039e-04\n",
            "  -5.96521120e-04 -2.15796512e-04  4.78845741e-05  1.56699407e-03]]\n",
            "[[ 3.70407781e+03  5.61853165e+00  1.74324046e+03  1.14429420e+03\n",
            "   6.80225128e+02 -6.07015269e+02 -3.17591018e+01 -1.73025429e+02]\n",
            " [ 5.61853165e+00  2.04564402e+03 -3.18058032e+01  2.99741849e+02\n",
            "  -1.24703003e+03 -5.04859926e+01 -5.13667834e+02  3.96989397e+02]\n",
            " [ 1.74324046e+03 -3.18058032e+01  4.83106967e+03  1.08065479e+03\n",
            "   2.18077539e+02 -9.46040125e+02 -1.11202866e+03  1.17587749e+03]\n",
            " [ 1.14429420e+03  2.99741849e+02  1.08065479e+03  3.90055138e+03\n",
            "   3.02297676e+03 -8.19732992e+02 -1.46202871e+03  7.00155796e+02]\n",
            " [ 6.80225128e+02 -1.24703003e+03  2.18077539e+02  3.02297676e+03\n",
            "   8.30184976e+03 -6.70861367e+02 -1.60448602e+03  1.96853089e+03]\n",
            " [-6.07015269e+02 -5.04859926e+01 -9.46040125e+02 -8.19732992e+02\n",
            "  -6.70861367e+02  2.88642522e+03  3.11128492e+02  3.48970954e+01]\n",
            " [-3.17591018e+01 -5.13667834e+02 -1.11202866e+03 -1.46202871e+03\n",
            "  -1.60448602e+03  3.11128492e+02  3.79056631e+03 -1.06488421e+03]\n",
            " [-1.73025429e+02  3.96989397e+02  1.17587749e+03  7.00155796e+02\n",
            "   1.96853089e+03  3.48970954e+01 -1.06488421e+03  1.95056054e+03]]\n",
            "md [      0.         6524037.9788144  6529131.04137551 ... 6529130.82204376\n",
            " 6529130.82204376 6524043.40261459]\n",
            "mu 6523602.3398477575\n",
            "s 65438.795641284116\n",
            "Starting testing...\n",
            "inup (4000, 8)\n",
            "g (4000, 8)\n",
            "[[ 2.68452042e-03 -7.12988911e-04 -1.98033026e-04  6.36273270e-04\n",
            "   2.03217262e-05 -1.86433355e-04  7.14387319e-05 -2.44345904e-04]\n",
            " [-7.12988911e-04  1.62957042e-03  6.83848816e-04 -7.67932413e-05\n",
            "  -8.52259627e-05  4.02211613e-04  9.50084790e-05 -4.84837293e-04]\n",
            " [-1.98033026e-04  6.83848816e-04  1.12458699e-03  2.88555401e-04\n",
            "  -3.73908816e-04  2.29411113e-04  4.14349160e-04 -3.43293920e-04]\n",
            " [ 6.36273270e-04 -7.67932413e-05  2.88555401e-04  8.04462519e-04\n",
            "  -3.49934816e-04  3.51571375e-04  8.00491621e-05 -6.79040879e-04]\n",
            " [ 2.03217262e-05 -8.52259627e-05 -3.73908816e-04 -3.49934816e-04\n",
            "   1.06616691e-03  7.08753790e-05 -5.78874068e-05 -2.64850149e-05]\n",
            " [-1.86433355e-04  4.02211613e-04  2.29411113e-04  3.51571375e-04\n",
            "   7.08753790e-05  1.23375768e-03 -4.60573830e-06 -7.88612702e-04]\n",
            " [ 7.14387319e-05  9.50084790e-05  4.14349160e-04  8.00491621e-05\n",
            "  -5.78874068e-05 -4.60573830e-06  1.02215764e-03  1.89786849e-04]\n",
            " [-2.44345904e-04 -4.84837293e-04 -3.43293920e-04 -6.79040879e-04\n",
            "  -2.64850149e-05 -7.88612702e-04  1.89786849e-04  1.87691804e-03]]\n",
            "[[ 595.69078448   99.79879146  114.20204676 -731.10561313 -222.00489228\n",
            "   230.12107349  -43.62548771  -42.31874082]\n",
            " [  99.79879146 1134.85868033 -691.59263259  691.99138705   99.02763979\n",
            "  -214.41850713   56.01965246  335.64381296]\n",
            " [ 114.20204676 -691.59263259 1833.65536381 -574.18234776  358.66934195\n",
            "    86.87208626 -634.248186     69.56303363]\n",
            " [-731.10561313  691.99138705 -574.18234776 3910.66016403 1212.44899067\n",
            "  -680.15880885 -234.35446182 1148.39997177]\n",
            " [-222.00489228   99.02763979  358.66934195 1212.44899067 1491.48319317\n",
            "  -295.46236939 -229.0645545   420.99214779]\n",
            " [ 230.12107349 -214.41850713   86.87208626 -680.15880885 -295.46236939\n",
            "  1293.87480826  -42.50797244  288.15659708]\n",
            " [ -43.62548771   56.01965246 -634.248186   -234.35446182 -229.0645545\n",
            "   -42.50797244 1302.47621875 -344.79452361]\n",
            " [ -42.31874082  335.64381296   69.56303363 1148.39997177  420.99214779\n",
            "   288.15659708 -344.79452361 1204.0561576 ]]\n",
            "md [      0.         6357517.98814746 6353467.43874197 ... 6353469.14166236\n",
            " 6357517.01814698 6351926.43866214]\n",
            "mu 6355612.5775226345\n",
            "s 100630.05314242144\n",
            "Starting testing...\n",
            "inup (3000, 8)\n",
            "g (3000, 8)\n",
            "[[ 3.58616785e-03  2.13143662e-03  1.73398478e-03  1.20451999e-03\n",
            "  -9.10917103e-05  2.06527642e-04  6.36797235e-04 -4.82372030e-04]\n",
            " [ 2.13143662e-03  4.11043445e-03  1.61342205e-03 -4.48734005e-04\n",
            "   5.40721581e-04  7.18013832e-04 -1.98100688e-04 -2.14373997e-03]\n",
            " [ 1.73398478e-03  1.61342205e-03  3.40395040e-03  9.95194525e-04\n",
            "  -1.40654394e-04  6.23674963e-04  7.42267504e-05 -5.46572773e-04]\n",
            " [ 1.20451999e-03 -4.48734005e-04  9.95194525e-04  2.49024774e-03\n",
            "  -1.14867732e-03  1.97142432e-04  9.82345788e-04  1.38269727e-03]\n",
            " [-9.10917103e-05  5.40721581e-04 -1.40654394e-04 -1.14867732e-03\n",
            "   2.82052126e-03  2.83993039e-05 -6.00793338e-04 -1.67894294e-03]\n",
            " [ 2.06527642e-04  7.18013832e-04  6.23674963e-04  1.97142432e-04\n",
            "   2.83993039e-05  2.38587056e-03  6.81110524e-04 -9.40071637e-04]\n",
            " [ 6.36797235e-04 -1.98100688e-04  7.42267504e-05  9.82345788e-04\n",
            "  -6.00793338e-04  6.81110524e-04  2.13653924e-03  5.26755255e-04]\n",
            " [-4.82372030e-04 -2.14373997e-03 -5.46572773e-04  1.38269727e-03\n",
            "  -1.67894294e-03 -9.40071637e-04  5.26755255e-04  3.08163335e-03]]\n",
            "[[ 684.53715259 -366.52784753  -90.2663072  -370.88242549  -74.05655159\n",
            "   162.29540138 -145.95809997   36.68800033]\n",
            " [-366.52784753  693.32259244 -126.40103339  197.51200133  174.50764247\n",
            "   -19.27185938   44.83105535  395.43002659]\n",
            " [ -90.2663072  -126.40103339  491.69066708 -245.89948358    4.65421536\n",
            "   -76.66097915  123.72435025   53.48179548]\n",
            " [-370.88242549  197.51200133 -245.89948358 1068.94255186   90.4528282\n",
            "  -167.04225772 -172.82736727 -416.02732388]\n",
            " [ -74.05655159  174.50764247    4.65421536   90.4528282   618.62730911\n",
            "    98.5725093    32.63208327  431.5786756 ]\n",
            " [ 162.29540138  -19.27185938  -76.66097915 -167.04225772   98.5725093\n",
            "   659.00011307 -244.23935528  369.83642961]\n",
            " [-145.95809997   44.83105535  123.72435025 -172.82736727   32.63208327\n",
            "  -244.23935528  694.58149474  -67.6255751 ]\n",
            " [  36.68800033  395.43002659   53.48179548 -416.02732388  431.5786756\n",
            "   369.83642961  -67.6255751  1160.99431357]]\n",
            "md [     0.         869842.08029493 870399.87172196 ... 869841.91149989\n",
            " 869841.91149989 869841.91149989]\n",
            "mu 869460.9487126908\n",
            "s 15986.030674773714\n",
            "testing net: 1\n",
            "Starting testing...\n",
            "Testing time: 14.768\n",
            "Test set AUC: 79.63%\n",
            "Finished testing.\n",
            "testing net: 2\n",
            "Starting testing...\n",
            "Testing time: 14.813\n",
            "Test set AUC: 76.98%\n",
            "Finished testing.\n",
            "testing net: 3\n",
            "Starting testing...\n",
            "Testing time: 14.714\n",
            "Test set AUC: 82.19%\n",
            "Finished testing.\n",
            "testing net: 4\n",
            "Starting testing...\n",
            "Testing time: 14.715\n",
            "Test set AUC: 74.04%\n",
            "Finished testing.\n",
            "testing net: 5\n",
            "Starting testing...\n",
            "Testing time: 14.553\n",
            "Test set AUC: 87.89%\n",
            "Finished testing.\n",
            "tensor([0.0068, 0.0055, 0.0041,  ..., 0.0044, 0.0219, 0.0045],\n",
            "       dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# def main(dataset_name, net_name, xp_path, data_path, load_config, load_model, objective, nu, device, seed,\n",
        "#          optimizer_name, lr, n_epochs, lr_milestone, batch_size, weight_decay, pretrain, ae_optimizer_name, ae_lr,\n",
        "#          ae_n_epochs, ae_lr_milestone, ae_batch_size, ae_weight_decay, n_jobs_dataloader, normal_class):\n",
        "from scipy.interpolate import make_interp_spline\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Deep SVDD, a fully deep method for anomaly detection.\n",
        "    :arg DATASET_NAME: Name of the dataset to load.\n",
        "    :arg NET_NAME: Name of the neural network to use.\n",
        "    :arg XP_PATH: Export path for logging the experiment.\n",
        "    :arg DATA_PATH: Root path of data.\n",
        "    \"\"\"\n",
        "\n",
        "    device = 'cuda'\n",
        "    if not torch.cuda.is_available():\n",
        "        device = 'cpu'\n",
        "    # logger.info('Computation device: %s' % device)\n",
        "    # logger.info('Number of dataloader workers: %d' % n_jobs_dataloader)\n",
        "\n",
        "    # Load data\n",
        "    dataset_p=My_Dataset_preTrain(root=\"ll\",anomalInTrain=True)\n",
        "    dataset_vali =My_Dataset_cross(root=\"ll\",anomal=True)\n",
        "    # dataset =My_Dataset(root=\"ll\")\n",
        "\n",
        "    # Initialize DeepSVDD model and set neural network \\phi\n",
        "    deep_SVDD = MSVDD('one-class',0.1)\n",
        "    # deep_SVDD.set_network(\"lll\")\n",
        "    # If specified, load Deep SVDD model (radius R, center c, network weights, and possibly autoencoder weights)\n",
        "    # if load_model:\n",
        "    #     deep_SVDD.load_model(model_path=load_model, load_ae=True)\n",
        "        # logger.info('Loading model from %s.' % load_model)\n",
        "\n",
        "    # logger.info('Pretraining: %s' % pretrain)\n",
        "    pretrain=True;\n",
        "    if pretrain:\n",
        "        # Log pretraining details\n",
        "        # logger.info('Pretraining optimizer: %s' % cfg.settings['ae_optimizer_name'])\n",
        "        # logger.info('Pretraining learning rate: %g' % cfg.settings['ae_lr'])\n",
        "        # logger.info('Pretraining epochs: %d' % cfg.settings['ae_n_epochs'])\n",
        "        # logger.info('Pretraining learning rate scheduler milestones: %s' % (cfg.settings['ae_lr_milestone'],))\n",
        "        # logger.info('Pretraining batch size: %d' % cfg.settings['ae_batch_size'])\n",
        "        # logger.info('Pretraining weight decay: %g' % cfg.settings['ae_weight_decay'])\n",
        "\n",
        "        # Pretrain model on dataset (via autoencoder)\n",
        "        # valid_pret=None\n",
        "        valid_pret = deep_SVDD.pretrain(dataset_p, dataset_vali,device=device,load=True,n_epochs=400,lr_milestones=[40,60] )\n",
        "       \n",
        "        # deep_SVDD.load()\n",
        "        # ,\n",
        "        #                    optimizer_name=cfg.settings['ae_optimizer_name'],\n",
        "        #                    lr=cfg.settings['ae_lr'],\n",
        "        #                    n_epochs=cfg.settings['ae_n_epochs'],\n",
        "        #                    lr_milestones=cfg.settings['ae_lr_milestone'],\n",
        "        #                    batch_size=cfg.settings['ae_batch_size'],\n",
        "        #                    weight_decay=cfg.settings['ae_weight_decay'],\n",
        "        #                    device=device,\n",
        "        #                    n_jobs_dataloader=n_jobs_dataloader)\n",
        "\n",
        "    # Train model on dataset,n_epoch\n",
        "    # dataset_vali =My_Dataset_cross(root=\"ll\",anomal=False)\n",
        "    # dataset_p=My_Dataset_preTrain(root=\"ll\")\n",
        "\n",
        "    print(device)\n",
        "    valid_t=deep_SVDD.train(dataset_p,dataset_vali,5,device=device,lr_milestones=[30, 40],load=False, resume=False,n_epochs=[30, 60,30,50,10], lr=0.1)\n",
        "\n",
        "    profile = deep_SVDD.update_profile(dataset_p)\n",
        "\n",
        "\n",
        "    # print(valid_t)\n",
        "    # import numpy as np\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # from scipy.stats import norm\n",
        "    # import statistics\n",
        "    # from matplotlib.pyplot import figure\n",
        "    # from sklearn import preprocessing\n",
        "    # figure(figsize=(10, 5), dpi=150)\n",
        "    # t_loss=list()\n",
        "    # t_valid=list()\n",
        "    # xxx=list()\n",
        "    # for i in range(0, 151):\n",
        "    #   if i%1==0:\n",
        "    #     xxx.append(i)\n",
        "    #     t=valid_t['loss'][i]\n",
        "    #     v=valid_t['valid'][i]\n",
        "    #     if t>10:\n",
        "    #       t=10\n",
        "    #     if v >10:\n",
        "    #       v=10\n",
        "    #     t_loss.append(t)\n",
        "    #     t_valid.append(v)\n",
        "    # print(len(t_loss))\n",
        "    # print(len(xxx))\n",
        "    # t_loss=np.array(t_loss)\n",
        "    # t_valid=np.array(t_valid)\n",
        "\n",
        "    # xxx=np.array(xxx)\n",
        "    # print(xxx.shape)\n",
        "    # print(t_loss.shape)\n",
        "    # cubic_interploation_model_t = interp1d(xxx, t_loss, kind = \"cubic\")\n",
        "    # X_t = np.linspace(xxx.min(),xxx.max() , 150)\n",
        "\n",
        "    # Y_t=cubic_interploation_model_t(X_t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # cubic_interploation_model_l = interp1d(xxx, t_valid, kind = \"cubic\")\n",
        "    # X_l = np.linspace(xxx.min(),xxx.max() , 150)\n",
        "\n",
        "    # Y_l=cubic_interploation_model_l(X_l)\n",
        "\n",
        "    # plt.plot(X_t,Y_t, label='train loss')\n",
        "    # plt.plot(X_l,Y_l, label='validation_loss')\n",
        "    # plt.legend(loc=\"upper right\")\n",
        "    # # plt.yticks(np.arange(0, 18, 3))\n",
        "    # plt.yscale('linear')     \n",
        "    # plt.xlabel(\"Epoch\")\n",
        "    # plt.ylabel(\"Error\")   \n",
        "    # # plt.ylim(-0, 02.)\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "    # import numpy as np\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # from scipy.stats import norm\n",
        "    # import statistics\n",
        "    # from matplotlib.pyplot import figure\n",
        "    # from sklearn import preprocessing\n",
        "    # figure(figsize=(30, 20), dpi=150)\n",
        "    # clid_t['loss'], label='train loss')\n",
        "    # plt.plot(range(0, len(valid_t['valid']) ),valid_t['valid'], label='validation_losss')\n",
        "    # plt.legend(loc=\"upper left\")\n",
        "    # # plt.ylim(-0, 02.)\n",
        "    # plt.show()\n",
        "    # deep_SVDD.load_clf(5,device=device,lr_milestones=[40],n_epochs=100, lr=0.1,batch_size=128,weight_decay=1e-6,n_jobs_dataloader=0)\n",
        "   \n",
        "    # ,\n",
        "    #                 optimizer_name='adam',\n",
        "    #                 lr=cfg.settings['lr'],\n",
        "    #                 n_epochs=cfg.settings['n_epochs'],\n",
        "    #                 lr_milestones=cfg.settings['lr_milestone'],\n",
        "    #                 batch_size=cfg.settings['batch_size'],\n",
        "    #                 weight_decay=cfg.settings['weight_decay'],\n",
        "    #                 device=device,\n",
        "    #                 n_jobs_dataloader=n_jobs_dataloader)\n",
        "    test_dataset=My_Dataset_Test(root='ll')\n",
        "    # Test model\n",
        "    indices, labels, scores,inp,lbl = deep_SVDD.test(test_dataset)\n",
        "    print(scores)\n",
        "    # Plot most anomalous and most normal (within-class) test samples\n",
        "    # inindicesdices, labels, scores, outputs= zip(*deep_SVDD.results['test_scores'])\n",
        "    # , labels, scores = np.array(indices), np.array(labels), np.array(scores)\n",
        "    idx_sorted = indices[labels == 0][np.argsort(scores[labels == 0])]  # sorted from lowest to highest anomaly score\n",
        "    return deep_SVDD, dataset_p, indices, labels, scores ,inp,lbl,profile\n",
        "    # if di.\n",
        "\n",
        "svdd, dataset, indices, labels, scores,inp,lbl,profile= main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(3)\n",
        "b = torch.randn(3)\n",
        "\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(a.add(b))"
      ],
      "metadata": {
        "id": "XPTXkMcqkeoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def md(t, cov_inv, m_t, mask):\n",
        "  tmp=(t - m_t)[:,mask]\n",
        "  print(tmp.shape)\n",
        "  print(t.shape)\n",
        "  print(cov_inv.shape)\n",
        "  print(m_t.shape)\n",
        "  a=np.matmul(tmp,cov_inv)\n",
        "  print(a.shape)\n",
        "  b=np.sum(np.multiply(a,tmp),axis=1)\n",
        "  print(b.shape)\n",
        "  return np.sqrt(np.absolute(b))"
      ],
      "metadata": {
        "id": "Eww21tpiu61G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddd=My_Dataset(root=\"ll\")\n",
        "\n",
        "indices_v, labels_v, scores_v, inp_V, lbl_V = svdd.test(ddd)\n",
        "\n",
        "indices_v, labels_v, scores_v = np.array(indices_v), np.array(labels_v), np.array(scores_v)"
      ],
      "metadata": {
        "id": "q5BK0ph5UbKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oMXJNwDQUcJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(profile)\n",
        "\n",
        "def check( observe, profile):\n",
        "  l = len(observe[0])\n",
        "  mask = np.zeros(shape=(l,l))\n",
        "  for i in range(0,l):\n",
        "    for j in range(0,l):\n",
        "      if(j<i):\n",
        "        mask[i][j]=1\n",
        "      else:\n",
        "        continue\n",
        "  return md(observe, profile['cov_mat'], profile['m_tam'], mask==1)\n",
        "\n",
        "def detect(alpha ,md ,profile):\n",
        "  mu = profile['mu']\n",
        "  s= profile['s']\n",
        "  t1= md <mu+s*alpha\n",
        "  t2= md > mu-s*alpha\n",
        "  print(\"t1\", any(t1))\n",
        "  print(\"t2\", any(t2))\n",
        "  print(t2)\n",
        "  res = t1 & t2\n",
        "  print(res)\n",
        "  return res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "FPVogzC3uz91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = list()\n",
        "for i in inp_V:\n",
        "  arr = np.array(i[0])\n",
        "  for j in range(1,len(i)):\n",
        "    arr= np.append(arr,i[j],axis=0)\n",
        "  data.append(arr)\n",
        "\n",
        "\n",
        "label = np.array(lbl_V[0])\n",
        "for i in range(1,len(lbl_V)):\n",
        "  label= np.append(label,np.array(lbl_V[i]))\n",
        "\n",
        "label=label.reshape(-1,1)\n",
        "result = list()\n",
        "for i in range(0,len(data)):\n",
        "  print(data[i].shape)\n",
        "  result.append(detect(8,check(genTAM(data[i]),profile[i]),profile[i]))\n",
        "for i in result:\n",
        "  print(any(i))"
      ],
      "metadata": {
        "id": "DWz9YWpgUseO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D68lCmunWfc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in inp:\n",
        "  arr = np.array(i[0])\n",
        "  for j in range(1,len(i)):\n",
        "    arr= np.append(arr,i[j],axis=0)\n",
        "  data.append(arr)\n",
        "\n",
        "\n",
        "label = np.array(lbl[0])\n",
        "for i in range(1,len(lbl)):\n",
        "  label= np.append(label,np.array(lbl[i]))\n",
        "\n",
        "label=label.reshape(-1,1)\n",
        "result = list()\n",
        "for i in range(0,len(data)):\n",
        "  # print(data[i].shape)\n",
        "  result.append(detect(0.1,check(genTAM(data[i]),profile[i]),profile[i]))"
      ],
      "metadata": {
        "id": "IgfWTQa0UtF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in result:\n",
        "#   print(i)\n",
        "\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Nigod5jXCl9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "m = np.array([1,2,3])\n",
        "a = np.array([1,2,3])\n",
        "\n",
        "print(np.sum(a))\n",
        "# b = np.array([1, 0, 0])\n",
        "\n",
        "# c=np.dot(b,m.T)\n",
        "# print(c)\n",
        "# np.dot(c,b.T)\n",
        "# c = np.array(\n",
        "#               [[3,4,5,6,7,8],\n",
        "#                [3,4,5,6,7,8],\n",
        "#                [3,4,5,6,7,8],\n",
        "#                [3,4,5,6,7,8],\n",
        "#                [3,4,5,6,7,8],\n",
        "#                [3,4,5,6,7,8]]\n",
        "#     )\n",
        "\n",
        "# # m_t= m.transpose()\n",
        "\n",
        "# w = m>1\n",
        "# e = m<5\n",
        "\n",
        "# print(w)\n",
        "# print(e)\n",
        "# print(e & w)\n",
        "# r=(a-b)[: ,b==1]\n",
        "# print(r)\n",
        "# p=np.matmul(r,c)\n",
        "# print(p)\n",
        "# print(r)\n",
        "# print(np.diag(np.matmul(p,r.T)))\n",
        "# print(np.sum(np.multiply(p,r),axis=1))\n",
        "# print(b)\n",
        "# cov_inv= np.linalg.inr(b)\n",
        "# print(cov_inv)\n"
      ],
      "metadata": {
        "id": "d64wNFpZWNG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d_Eug7jRILrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genTAM(vector):\n",
        "  l=len(vector)\n",
        "\n",
        "\n",
        "  d = np.absolute(np.array([vector[0]]))\n",
        "  # print(d)\n",
        "  d_t= d.transpose()\n",
        "  f_d=np.dot(d_t,d)/2\n",
        "  np.fill_diagonal(f_d,0)\n",
        "  ttt=np.array([f_d])\n",
        "  # print(\"kk\", ttt.shape)\n",
        "\n",
        "  for i in range(1,l):\n",
        "  \n",
        "    d = np.absolute(np.array([vector[i]]))\n",
        "    d_t= d.transpose()\n",
        "    f_d=np.dot(d_t,d)/2\n",
        "    np.fill_diagonal(f_d,0)\n",
        "    ttt=np.append(ttt,[f_d],axis=0)\n",
        "  # print(\"fin\")\n",
        "  # print(ttt.shape)\n",
        "  return ttt;\n",
        "\n",
        "def calcCov(self, tam, m_tam):\n",
        "  d = len(m_tam)-1 \n",
        "  cov_d =int((d*(d+1))/2)\n",
        "  \n",
        "  cov_mat=np.zeros(shape=(cov_d,cov_d))\n",
        "  cc=0\n",
        "  for j in range(0,d):\n",
        "    for k in range(0,d):\n",
        "      if k>=j:\n",
        "          break\n",
        "      c=0\n",
        "      for l in range(0,d):\n",
        "        for v in range(0,d):\n",
        "          if v>=l:\n",
        "            break\n",
        "          cov_mat[cc][c]=self.sigma(tam, m_tam, j+1, k, l+1, v, len(tam))\n",
        "          c+=1\n",
        "      cc+=1\n",
        "  return cov_mat\n",
        "\n",
        "def sigma(self, tam, m_tam, j, k , l, v, g,cov=0):\n",
        "  # print(j,k,l,v)\n",
        "  # print(tam.shape)\n",
        "  # print(m_tam.shape)\n",
        "  \n",
        "  for i in range(0, g):\n",
        "    cov+=(tam[i][j][k] - m_tam[j][k])*(tam[i][l][v] - m_tam[l][v])/(g-1)\n",
        "  return cov\n",
        "\n",
        "\n",
        "def md(self, t, cov_inv, m_t, mask):\n",
        "  tmp=(t - m_t)[:,mask]\n",
        "  return np.diag(np.sqrt(np.matmul(np.matmul(tmp,cov_inv),tmp.T)))\n",
        "  \n",
        "  \n",
        "\n",
        "def profileGenerator(self, tam):\n",
        "    # print(tam.shape)\n",
        "    g=len(tam)\n",
        "    l= len(tam[0])\n",
        "    mask = np.zeros(shape=(l,l))\n",
        "    for i in range(0,l):\n",
        "      for j in range(0,l):\n",
        "        if(j<i):\n",
        "          mask[i][j]=1\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "  \n",
        "    md= np.zeros(g)\n",
        "    # m_tam=m_tsm/8\n",
        "    m_tam = np.sum(tam, axis=0)/g\n",
        "    cov_mat=self.calcCov(tam, m_tam)\n",
        "    # cov_mat = np.cov(tam[:,mask==1].T)\n",
        "    # print(\"=========\")\n",
        "    # print(tam[0])\n",
        "    # print(tam[1])\n",
        "    # print(tam[2])\n",
        "    # print(\"=========\")\n",
        "    # print(cov_mat)\n",
        "    # print(cov_mat.shape)\n",
        "    try:\n",
        "      cov_inv= np.linalg.inv(cov_mat)\n",
        "    except:\n",
        "      cov_inv= np.linalg.pinv(cov_mat)\n",
        "    print(cov_inv)\n",
        "    for i in range(0,g):\n",
        "      md[i]=self.md(tam[i], cov_inv, m_tam, mask==1)\n",
        "    mu=np.average(md);\n",
        "    s= np.sqrt(np.sum(np.power(md-mu,2)/g-1))\n",
        "    return mu, s, m_tam, cov_mat\n"
      ],
      "metadata": {
        "id": "V0pcYANjPKb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ij8lfvDC8UMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r= np.array([[[1,2,3],[4,5,6],[7,8,9]],[[1,2,3],[4,5,6],[7,8,9]]])\n",
        "print(np.append(r,[[[1,1,1],[2,2,2,],[3,3,3]]],axis=0))\n",
        "# mask=np.array([[True,False,False],[True,True,False],[True,True,True]])\n",
        "# print(mask)\n",
        "# r[mask]\n",
        "# np.concatenate((,np.diag(r),r.T[mask.T]))"
      ],
      "metadata": {
        "id": "TeENpWLPEGMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "a=np.array([[1,2],[2,1]])\n",
        "c = np.array([4,5])\n",
        "b= np.matmul(c,a)\n",
        "# b=np.sum(a,axis=0)\n",
        "print(b)\n",
        "# print(a-b)"
      ],
      "metadata": {
        "id": "I0j0KNjsfUbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a =np.array([[1,2,3],[5,6,7]])\n",
        "a= np.flip(a,axis=0)\n",
        "a"
      ],
      "metadata": {
        "id": "_tPrQDkeQFij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAGxCCfdJocx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# print(len(inp[1]))\n",
        "\n",
        "# a= list()\n",
        "# a.append([[1,8,3],[4,5,6]]) \n",
        "# a.append([[7,8,9],[10,11,12]])\n",
        "# arr =  np.array(a[0])\n",
        "# print(a)\n",
        "# print(arr)\n",
        "# arr= np.append(arr,np.array(a[1]),axis=0)\n",
        "# print(arr)\n",
        "# # a. append(b)\n",
        "\n",
        "my_array= np.array(inp_V[0][0])\n",
        "for i in range(1,len(inp_V[0])):\n",
        "  my_array= np.append(my_array,np.array(inp_V[0][i]),axis=0)\n",
        "\n",
        "l = np.array(lbl_V[0])\n",
        "for i in range(1,len(lbl_V)):\n",
        "  l= np.append(l,np.array(lbl_V[i]))\n",
        "\n",
        "l=l.reshape(-1,1)\n",
        "# print(len(inp_V))\n",
        "# print(len(inp_V[0]))\n",
        "# print(len(inp_V[0][0]))\n",
        "# print(len(inp_V[0][0][0]))\n",
        "# print(\"===\")\n",
        "# print(my_array)\n",
        "# print(np.reshape(np.array(lbl[1]),(len(my_array))))\n",
        "# my_array = my_array / np.array([ np.linalg.norm(my_array,axis=1)]).transpose()\n",
        "\n",
        "my_array=np.append(my_array,l,axis=1)\n",
        "my_array = np.flip(my_array,axis=0)\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# x = np.random.rand(1000)*10\n",
        "print(my_array.shape)\n",
        "# norm2 = normalize(arr[:,np.newaxis], axis=11).ravel()\n",
        "# print(norm1)\n",
        "\n",
        "\n",
        "\n",
        "# min = np.min(my_array)\n",
        "# max = np.max(my_array)\n",
        "# r= max - min\n",
        "# my_array= (my_array-min)/r\n",
        "# my_array\n",
        "df = pd.DataFrame(my_array, columns = ['c1','c2','c3','c4','c5','c6','c7','c8','lbl'])\n",
        "# print(df)  # import numpy as np\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # from scipy.stats import norm\n",
        "    # import statistics\n",
        "    # from matplotlib.pyplot import figure\n",
        "    # from sklearn import preprocessing\n",
        "    # figure(figsize=(30, 20), dpi=150)\n",
        "    # print(valid_t)\n",
        "    # plt.plot(range(0, len(valid_t['loss']) ),valid_t['loss'], label='train loss')\n",
        "    # plt.plot(range(0, len(valid_t['valid']) ),valid_t['valid'], label='validation_losss')\n",
        "    # plt.legend(loc=\"upper left\")\n",
        "    # # plt.ylim(-0, 02.)\n",
        "    # plt.show()\n",
        "# # df['lbl'] = lbl[0]\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"ticks\")\n",
        "\n",
        "sns.pairplot(df, hue=\"lbl\", hue_order=[1, 0], plot_kws=dict(alpha=0.25))\n",
        "# print(df)\n",
        "# print(type(df))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# print(len(inp[1]))\n",
        "\n",
        "# a= list()\n",
        "# a.append([[1,8,3],[4,5,6]]) \n",
        "# a.append([[7,8,9],[10,11,12]])\n",
        "# arr =  np.array(a[0])\n",
        "# print(a)\n",
        "# print(arr)\n",
        "# arr= np.append(arr,np.array(a[1]),axis=0)\n",
        "# print(arr)\n",
        "# # a. append(b)\n",
        "\n",
        "my_array= np.array(inp_V[1][0])\n",
        "for i in range(1,len(inp_V[1])):\n",
        "  my_array= np.append(my_array,np.array(inp_V[1][i]),axis=0)\n",
        "\n",
        "l = np.array(lbl_V[0])\n",
        "for i in range(1,len(lbl_V)):\n",
        "  l= np.append(l,np.array(lbl_V[i]))\n",
        "\n",
        "l=l.reshape(-1,1)\n",
        "# print(len(inp_V))\n",
        "# print(len(inp_V[0]))\n",
        "# print(len(inp_V[0][0]))\n",
        "# print(len(inp_V[0][0][0]))\n",
        "# print(\"===\")\n",
        "# print(my_array)\n",
        "# print(np.reshape(np.array(lbl[1]),(len(my_array))))\n",
        "# my_array = my_array / np.array([ np.linalg.norm(my_array,axis=1)]).transpose()\n",
        "\n",
        "my_array=np.append(my_array,l,axis=1)\n",
        "my_array = np.flip(my_array,axis=0)\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# x = np.random.rand(1000)*10\n",
        "print(my_array.shape)\n",
        "# norm2 = normalize(arr[:,np.newaxis], axis=11).ravel()\n",
        "# print(norm1)\n",
        "\n",
        "\n",
        "\n",
        "# min = np.min(my_array)\n",
        "# max = np.max(my_array)\n",
        "# r= max - min\n",
        "# my_array= (my_array-min)/r\n",
        "# my_array\n",
        "df = pd.DataFrame(my_array, columns = ['c1','c2','c3','c4','c5','c6','c7','c8','lbl'])\n",
        "# print(df)  # import numpy as np\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # from scipy.stats import norm\n",
        "    # import statistics\n",
        "    # from matplotlib.pyplot import figure\n",
        "    # from sklearn import preprocessing\n",
        "    # figure(figsize=(30, 20), dpi=150)\n",
        "    # print(valid_t)\n",
        "    # plt.plot(range(0, len(valid_t['loss']) ),valid_t['loss'], label='train loss')\n",
        "    # plt.plot(range(0, len(valid_t['valid']) ),valid_t['valid'], label='validation_losss')\n",
        "    # plt.legend(loc=\"upper left\")\n",
        "    # # plt.ylim(-0, 02.)\n",
        "    # plt.show()\n",
        "# # df['lbl'] = lbl[0]\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"ticks\")\n",
        "\n",
        "sns.pairplot(df, hue=\"lbl\", hue_order=[1, 0], plot_kws=dict(alpha=0.25))\n",
        "# print(df)\n",
        "# print(type(df))"
      ],
      "metadata": {
        "id": "nuKjK9BlBtXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Dxa4bhhPQs"
      },
      "outputs": [],
      "source": [
        "# v_pt000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJcNZ8-UwSqM"
      },
      "outputs": [],
      "source": [
        "# v_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiE9ngv_efqd"
      },
      "outputs": [],
      "source": [
        "# indices, labels, scores\n",
        "# indices# print(np.argsort(scores[labels == 0]))\n",
        "\n",
        "\n",
        "# idx_sorted = indices[labels == 0][np.argsort(scores[labels == 0])]  # sorted from lowest to highest anomaly score\n",
        "# print(len(idx_sorted))\n",
        "\n",
        "# print(dataset.test_set.data)\n",
        "# X_normals = vertical_concat.iloc[idx_sorted[:20]]\n",
        "# X_outliers = vertical_concat.iloc[idx_sorted[-20:]]\n",
        "# xp_path=\"/content/gdrive/MyDrive/ArshadPeoject\"\n",
        "# plot_images_grid(torch.tensor(X_normals.values), export_img=xp_path + '/normals', title='Most normal examples', padding=2)\n",
        "# plot_images_grid(torch.tensor(X_outliers.values), export_img=xp_path + '/outliers', title='Most anomalous examples', padding=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74lyQ02o8ivW"
      },
      "outputs": [],
      "source": [
        "# scores[labels == 0][np.argsort(scores[labels == 0])] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J-wOLAsDsx3"
      },
      "outputs": [],
      "source": [
        "ddd=My_Dataset(root=\"ll\")\n",
        "\n",
        "indices_v, labels_v, scores_v, _,_= svdd.test(ddd)\n",
        "indices_v, labels_v, scores_v = np.array(indices_v), np.array(labels_v), np.array(scores_v)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8_2xdBdwx63"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import statistics\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# figure(figsize=(15, 10), dpi=100)\n",
        "# xxx=np.arange(0,100, 10)\n",
        "\n",
        "# plt.plot(xxx,v_t[4]['loss'])\n",
        "# plt.plot(xxx,v_t[4]['valid'])\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tElYV55p8YT6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "figure(figsize=(15, 10), dpi=100)\n",
        "\n",
        "# Plot between -10 an0.1d 10 with .001 steps.\n",
        "xxx=np.arange(-55, -40, 0.01)\n",
        "ww=scores_v[labels_v == 0][np.argsort(scores_v[labels_v == 0])]\n",
        "\n",
        "x_axis =ww\n",
        "qq=scores_v[labels_v == 1][np.argsort(scores_v[labels_v == 1])]\n",
        "x2_axis = qq\n",
        "print(x_axis)\n",
        "print(x2_axis)\n",
        "# Calculating mean and standard deviation\n",
        "mean = statistics.mean(x_axis)\n",
        "sd = statistics.stdev(x_axis)\n",
        "mean2 = statistics.mean(x2_axis)\n",
        "sd2 = statistics.stdev(x2_axis)\n",
        "print(mean)\n",
        "print(sd)\n",
        "print(mean2)\n",
        "print(sd2)\n",
        "plt.plot(xxx, (norm.pdf(xxx, mean, sd)))\n",
        "plt.plot(xxx, norm.pdf(xxx, mean2, sd2))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA3_t3Hl4tsB"
      },
      "outputs": [],
      "source": [
        "def solve(m1,m2,std1,std2):\n",
        "  a = 1/(2*std1**2) - 1/(2*std2**2)\n",
        "  b = m2/(std2**2) - m1/(std1**2)\n",
        "  c = m1**2 /(2*std1**2) - m2**2 / (2*std2**2) - np.log(std2/std1)\n",
        "  return np.roots([a,b,c])\n",
        "\n",
        "\n",
        "result = solve(mean,mean2,sd,sd2)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co2ZQjfZZtat"
      },
      "outputs": [],
      "source": [
        "tp=0\n",
        "tn=0\n",
        "fp=0\n",
        "fn=0\n",
        "alpha=1.4\n",
        "print(scores_v.shape)\n",
        "print(indices_v.shape)\n",
        "for i in indices_v:\n",
        "  if scores_v[i]<mean +alpha*sd and scores_v[i]>mean-alpha*sd:\n",
        "    if labels_v[i]==1:\n",
        "      fn+=1\n",
        "    else:\n",
        "      tn+=1\n",
        "  else:\n",
        "    if labels_v[i]==1:\n",
        "      tp+=1\n",
        "    else:\n",
        "      fp+=1\n",
        "  # if labels_v[i]==0:\n",
        "  #   if normal_min>=scores[i]:\n",
        "  #     normal_min=scores[i]\n",
        "    \n",
        "  #   if normal_max<=scores[i]:\n",
        "  #       normal_max=scores[i]\n",
        "  # else:\n",
        "     \n",
        "  #   if anomal_min>=scores[i]:\n",
        "  #     anomal_min=scores[i]\n",
        "    \n",
        "  #   if anomal_max<=scores[i]:\n",
        "  #     anomal_max=scores[i]\n",
        "precision = tp/(tp+fp)\n",
        "recall= tp/(tp+fn)\n",
        "print(precision, recall)\n",
        "f1_score = 2*precision*recall/(precision+recall)\n",
        "print(f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0iYhEY-5L8ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ_gzHLfoGpE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_celNYZmz5pp"
      },
      "outputs": [],
      "source": [
        "normal_min=100\n",
        "anomal_min=100\n",
        "normal_max=-100\n",
        "anomal_max=-100\n",
        "tp=0\n",
        "tn=0\n",
        "fp=0\n",
        "fn=0\n",
        "for i in indices:\n",
        "  if  scores[i]<=mean + 0.2*sd and scores[i]>=mean-0.2*sd:\n",
        "    if labels[i]==1:\n",
        "      fn+=1\n",
        "    else:\n",
        "      tn+=1\n",
        "  else:\n",
        "    if labels[i]==1:\n",
        "      tp+=1\n",
        "    else:\n",
        "      fp+=1\n",
        "\n",
        "print(tp, tn ,fp, fn)\n",
        "precision = tp/(tp+fp)\n",
        "recall= tp/(tp+fn)\n",
        "print(precision, recall)\n",
        "f1_score = 2*precision*recall/(precision+recall)\n",
        "print(f1_score)\n",
        "\n",
        "\n",
        "\n",
        "tpr = tp /(tp+fn)\n",
        "fpr = fp /(fp+tn)\n",
        "speci = 1-fpr\n",
        "acc =  (tp+tn)/(tp+tn+fp+fn)\n",
        "print(\"========\")\n",
        "print(tpr)\n",
        "print(fpr)\n",
        "print(speci)\n",
        "print(acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NASZxCJ9T7_s"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from matplotlib import pyplot\n",
        "\n",
        "fpr, tpr, thresh= roc_curve(labels_v, scores_v)\n",
        "print(thresh)\n",
        "pyplot.plot(fpr, tpr, linestyle='--', label='No Skill')\n",
        "roc_auc_score(labels_v, scores_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMCRsouuT74i"
      },
      "outputs": [],
      "source": [
        "# torch.save(\n",
        "#     {\n",
        "#       'test_1': {\n",
        "#           'desc': '520*520*1, K=5, ',\n",
        "#           'lables': labels_v,\n",
        "#           'scores': scores_v\n",
        "#           }\n",
        "#     }\n",
        "# ,'/content/gdrive/MyDrive/ArshadPeoject/result/res.dict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y33ia_Z71svs"
      },
      "outputs": [],
      "source": [
        "print(\"normal_min =\",normal_min)\n",
        "print(\"anomal_min =\",anomal_min)\n",
        "print(\"normal_max =\",normal_max)\n",
        "print(\"anomal_max =\",anomal_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQi80lc4AW2D"
      },
      "outputs": [],
      "source": [
        "plt.plot(xxx, norm.pdf(xxx, mean2, sd2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_uzFTyq4vom"
      },
      "outputs": [],
      "source": [
        "\n",
        "net_parameters = filter(lambda p: p.requires_grad, svdd.net.parameters())\n",
        "params = sum([np.prod(p.size()) for p in net_parameters])\n",
        "print('Trainable parameters: {}'.format(params))\n",
        "print(svdd.net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRT3ZoPV4Xd5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('classic')\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "dataset =My_Dataset(root=\"ll\")\n",
        "vertical_concat = pd.concat([dataset.test_set.data, dataset.test_set.anomal_data], axis=0,ignore_index=True)\n",
        "\n",
        "cols=[\"col\"]*200\n",
        "for i in range(0,200):\n",
        "  cols[i]=\"col-\"+str(i)\n",
        "\n",
        "# vv= vertical_concat.iloc[:,0:20]\n",
        "# vv=pd.concat([vv,lbldf], axis=1)\n",
        "# vv= vv.drop_duplicates()\n",
        "# vv = pd.concat([vv, lbldf], axis=1)\n",
        "print(vertical_concat)\n",
        "\n",
        "\n",
        "\n",
        "pp = sns.pairplot(vertical_concat, vars=cols,hue='label',palette=['red','blue'])\n",
        "\n",
        "# fig = pp.fig \n",
        "# fig.subplots_adjust(top=0.93, wspace=0.3)\n",
        "# t = fig.suptitle('Wine Attributes Pairwise Plots', fontsize=14)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DrivationTree_(1)_ final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}